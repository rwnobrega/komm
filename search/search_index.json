{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Komm","text":"<p>A Python library for communication systems.</p> <p>Komm is an open-source library for Python 3 providing tools for analysis and simulation of analog and digital communication systems. This project is inspired by many other communication systems libraries, such as MATLAB\u00ae Communications System Toolbox\u2122, GNU Radio, CommPy, and SageMath. Komm is licensed under the GNU General Public License v3.0.</p>"},{"location":"#quick-installation","title":"Quick Installation","text":"<pre><code>pip install komm\n</code></pre> <p>For more installation options and source code, please visit the project's development page.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Check out our library reference for information on the classes and functions available.</p>"},{"location":"nav/","title":"Nav","text":"<ul> <li>Home</li> <li>Library reference</li> <li>Algebra<ul> <li>BinaryPolynomial</li> <li>BinaryPolynomialFraction</li> <li>FiniteBifield</li> </ul> </li> <li>Channels<ul> <li>AWGNChannel</li> <li>DiscreteMemorylessChannel</li> <li>BinarySymmetricChannel</li> <li>BinaryErasureChannel</li> <li>ZChannel</li> </ul> </li> <li>Constellations<ul> <li>Constellation</li> <li>PAMConstellation</li> <li>QAMConstellation</li> <li>ASKConstellation</li> <li>PSKConstellation</li> <li>APSKConstellation</li> </ul> </li> <li>Error control<ul> <li>Block codes</li> <li>BlockCode</li> <li>SystematicBlockCode</li> <li>CyclicCode</li> <li>HammingCode</li> <li>SimplexCode</li> <li>GolayCode</li> <li>RepetitionCode</li> <li>SingleParityCheckCode</li> <li>CordaroWagnerCode</li> <li>ReedMullerCode</li> <li>BCHCode</li> <li>Lexicode</li> <li>PolarCode</li> <li>SlepianArray</li> <li>Convolutional codes</li> <li>ConvolutionalCode</li> <li>LowRateConvolutionalCode</li> <li>HighRateConvolutionalCode</li> <li>TerminatedConvolutionalCode</li> <li>ViterbiStreamDecoder</li> <li>Decoders</li> <li>BCJRDecoder</li> <li>BerlekampDecoder</li> <li>ExhaustiveSearchDecoder</li> <li>ReedDecoder</li> <li>SCDecoder</li> <li>SyndromeTableDecoder</li> <li>ViterbiDecoder</li> <li>WagnerDecoder</li> <li>Checksum</li> <li>CyclicRedundancyCheck</li> </ul> </li> <li>Finite-state machines<ul> <li>MooreMachine</li> <li>MealyMachine</li> </ul> </li> <li>Labelings<ul> <li>Labeling</li> <li>NaturalLabeling</li> <li>ReflectedLabeling</li> <li>ReflectedRectangularLabeling</li> <li>ProductLabeling</li> </ul> </li> <li>Pulse formatting<ul> <li>Pulse</li> <li>RectangularPulse</li> <li>ManchesterPulse</li> <li>SincPulse</li> <li>RaisedCosinePulse</li> <li>RootRaisedCosinePulse</li> <li>GaussianPulse</li> </ul> </li> <li>Sequences<ul> <li>Binary sequences</li> <li>BinarySequence</li> <li>BarkerSequence</li> <li>WalshHadamardSequence</li> <li>LFSRSequence</li> <li>GoldSequence</li> <li>KasamiSequence</li> <li>Complex sequences</li> <li>ComplexSequence</li> <li>ZadoffChuSequence</li> </ul> </li> <li>Source coding<ul> <li>Lossless coding</li> <li>FixedToVariableCode</li> <li>ShannonCode</li> <li>FanoCode</li> <li>HuffmanCode</li> <li>VariableToFixedCode</li> <li>TunstallCode</li> <li>LempelZiv77Code</li> <li>LempelZiv78Code</li> <li>LempelZivWelchCode</li> <li>Integer coding</li> <li>UnaryCode</li> <li>FibonacciCode</li> <li>Quantization</li> <li>ScalarQuantizer</li> <li>UniformQuantizer</li> <li>LloydMaxQuantizer</li> </ul> </li> <li>Sources<ul> <li>DiscreteMemorylessSource</li> <li>MarkovChain</li> </ul> </li> <li>Utilities<ul> <li>bits_to_int</li> <li>int_to_bits</li> <li>sampling_rate_compress</li> <li>sampling_rate_expand</li> <li>fourier_transform</li> <li>boxplus</li> <li>gaussian_q</li> <li>gaussian_q_inv</li> <li>marcum_q</li> <li>autocorrelation</li> <li>cyclic_autocorrelation</li> <li>entropy</li> <li>binary_entropy</li> <li>binary_entropy_inv</li> <li>relative_entropy</li> </ul> </li> </ul>"},{"location":"ref/","title":"Library reference","text":""},{"location":"ref/#algebra","title":"Algebra","text":"<ul> <li><code>BinaryPolynomial</code> \u2013 Binary polynomial.</li> <li><code>BinaryPolynomialFraction</code> \u2013 Binary polynomial fraction.</li> <li><code>FiniteBifield</code> \u2013 Finite field with binary characteristic.</li> </ul>"},{"location":"ref/#channels","title":"Channels","text":"<ul> <li><code>AWGNChannel</code> \u2013 Additive white Gaussian noise (AWGN) channel.</li> <li><code>DiscreteMemorylessChannel</code> \u2013 General discrete memoryless channel (DMC).</li> <li><code>BinarySymmetricChannel</code> \u2013 Binary symmetric channel (BSC).</li> <li><code>BinaryErasureChannel</code> \u2013 Binary erasure channel (BEC).</li> <li><code>ZChannel</code> \u2013 Z-channel.</li> </ul>"},{"location":"ref/#constellations","title":"Constellations","text":"<ul> <li><code>Constellation</code> \u2013 General real or complex constellation.</li> <li><code>PAMConstellation</code> \u2013 Pulse-amplitude modulation (PAM) constellation.</li> <li><code>QAMConstellation</code> \u2013 Quadrature amplitude modulation (QAM) constellation.</li> <li><code>ASKConstellation</code> \u2013 Amplitude-shift keying (ASK) constellation.</li> <li><code>PSKConstellation</code> \u2013 Phase-shift keying (PSK) constellation.</li> <li><code>APSKConstellation</code> \u2013 Amplitude- and phase-shift keying (APSK) constellation.</li> </ul>"},{"location":"ref/#error-control","title":"Error control","text":""},{"location":"ref/#block-codes","title":"Block codes","text":"<ul> <li><code>BlockCode</code> \u2013 General binary linear block code.</li> <li><code>SystematicBlockCode</code> \u2013 Systematic linear block code.</li> <li><code>CyclicCode</code> \u2013 General binary cyclic code.</li> <li><code>HammingCode</code> \u2013 Hamming code.</li> <li><code>SimplexCode</code> \u2013 Simplex (maximum-length) code.</li> <li><code>GolayCode</code> \u2013 Binary Golay code.</li> <li><code>RepetitionCode</code> \u2013 Repetition code.</li> <li><code>SingleParityCheckCode</code> \u2013 Single parity-check code.</li> <li><code>CordaroWagnerCode</code> \u2013 Cordaro\u2013Wagner code.</li> <li><code>ReedMullerCode</code> \u2013 Reed\u2013Muller code.</li> <li><code>BCHCode</code> \u2013 Bose\u2013Ray-Chaudhuri\u2013Hocquenghem (BCH) code.</li> <li><code>Lexicode</code> \u2013 Lexicographic code (lexicode).</li> <li><code>PolarCode</code> \u2013 Polar (Ar\u0131kan) code.</li> <li><code>SlepianArray</code> \u2013 Slepian array (standard array) for a linear block code.</li> </ul>"},{"location":"ref/#convolutional-codes","title":"Convolutional codes","text":"<ul> <li><code>ConvolutionalCode</code> \u2013 Binary convolutional encoder.</li> <li><code>LowRateConvolutionalCode</code> \u2013 Low-rate convolutional encoder.</li> <li><code>HighRateConvolutionalCode</code> \u2013 High-rate convolutional encoder.</li> <li><code>TerminatedConvolutionalCode</code> \u2013 Terminated convolutional code.</li> <li><code>ViterbiStreamDecoder</code> \u2013 Convolutional stream decoder using Viterbi algorithm.</li> </ul>"},{"location":"ref/#decoders","title":"Decoders","text":"<ul> <li><code>BCJRDecoder</code> \u2013 Bahl\u2013Cocke\u2013Jelinek\u2013Raviv (BCJR) decoder for terminated convolutional codes.</li> <li><code>BerlekampDecoder</code> \u2013 Berlekamp decoder for BCH codes.</li> <li><code>ExhaustiveSearchDecoder</code> \u2013 Exhaustive search decoder for general block codes.</li> <li><code>ReedDecoder</code> \u2013 Reed decoder for Reed-Muller codes.</li> <li><code>SCDecoder</code> \u2013 Successive cancellation decoder for Polar codes.</li> <li><code>SyndromeTableDecoder</code> \u2013 Syndrome table decoder for general block codes.</li> <li><code>ViterbiDecoder</code> \u2013 Viterbi decoder for terminated convolutional codes.</li> <li><code>WagnerDecoder</code> \u2013 Wagner decoder for single parity-check codes.</li> </ul>"},{"location":"ref/#checksum","title":"Checksum","text":"<ul> <li><code>CyclicRedundancyCheck</code> \u2013 Cyclic redundancy check (CRC) [Not implemented yet].</li> </ul>"},{"location":"ref/#finite-state-machines","title":"Finite-state machines","text":"<ul> <li><code>MooreMachine</code> \u2013 Finite-state Moore machine.</li> <li><code>MealyMachine</code> \u2013 Finite-state Mealy machine.</li> </ul>"},{"location":"ref/#labelings","title":"Labelings","text":"<ul> <li><code>Labeling</code> \u2013 General binary labeling.</li> <li><code>NaturalLabeling</code> \u2013 Natural binary labeling.</li> <li><code>ReflectedLabeling</code> \u2013 Reflected (Gray) binary labeling.</li> <li><code>ReflectedRectangularLabeling</code> \u2013 Reflected rectangular binary labeling.</li> <li><code>ProductLabeling</code> \u2013 Cartesian product of labelings.</li> </ul>"},{"location":"ref/#pulse-formatting","title":"Pulse formatting","text":"<ul> <li><code>Pulse</code> \u2013 General pulse [Not implemented yet].</li> <li><code>RectangularPulse</code> \u2013 Rectangular pulse.</li> <li><code>ManchesterPulse</code> \u2013 Manchester pulse.</li> <li><code>SincPulse</code> \u2013 Sinc pulse.</li> <li><code>RaisedCosinePulse</code> \u2013 Raised-cosine pulse.</li> <li><code>RootRaisedCosinePulse</code> \u2013 Root-raised-cosine pulse.</li> <li><code>GaussianPulse</code> \u2013 Gaussian pulse.</li> </ul>"},{"location":"ref/#sequences","title":"Sequences","text":""},{"location":"ref/#binary-sequences","title":"Binary sequences","text":"<ul> <li><code>BinarySequence</code> \u2013 General binary sequence.</li> <li><code>BarkerSequence</code> \u2013 Barker sequence.</li> <li><code>WalshHadamardSequence</code> \u2013 Walsh\u2013Hadamard sequence.</li> <li><code>LFSRSequence</code> \u2013 Linear-feedback shift register (LFSR) sequence.</li> <li><code>GoldSequence</code> \u2013 Gold sequence [Not implemented yet].</li> <li><code>KasamiSequence</code> \u2013 Kasami sequence [Not implemented yet].</li> </ul>"},{"location":"ref/#complex-sequences","title":"Complex sequences","text":"<ul> <li><code>ComplexSequence</code> \u2013 General complex sequence.</li> <li><code>ZadoffChuSequence</code> \u2013 Zadoff\u2013Chu sequence.</li> </ul>"},{"location":"ref/#source-coding","title":"Source coding","text":""},{"location":"ref/#lossless-coding","title":"Lossless coding","text":"<ul> <li><code>FixedToVariableCode</code> \u2013 General fixed-to-variable length code.</li> <li><code>ShannonCode</code> \u2013 Binary Shannon code.</li> <li><code>FanoCode</code> \u2013 Binary Fano code.</li> <li><code>HuffmanCode</code> \u2013 Binary Huffman code.</li> <li><code>VariableToFixedCode</code> \u2013 General variable-to-fixed length code.</li> <li><code>TunstallCode</code> \u2013 Binary Tunstall code.</li> <li><code>LempelZiv77Code</code> \u2013 Lempel\u2013Ziv 77 (LZ77 or LZ1) code.</li> <li><code>LempelZiv78Code</code> \u2013 Lempel\u2013Ziv 78 (LZ78 or LZ2) code.</li> <li><code>LempelZivWelchCode</code> \u2013 Lempel\u2013Ziv\u2013Welch (LZW) code.</li> </ul>"},{"location":"ref/#integer-coding","title":"Integer coding","text":"<ul> <li><code>UnaryCode</code> \u2013 Unary code.</li> <li><code>FibonacciCode</code> \u2013 Fibonacci code.</li> </ul>"},{"location":"ref/#quantization","title":"Quantization","text":"<ul> <li><code>ScalarQuantizer</code> \u2013 General scalar quantizer.</li> <li><code>UniformQuantizer</code> \u2013 Uniform scalar quantizer.</li> <li><code>LloydMaxQuantizer</code> \u2013 Lloyd\u2013Max scalar quantizer.</li> </ul>"},{"location":"ref/#sources","title":"Sources","text":"<ul> <li><code>DiscreteMemorylessSource</code> \u2013 Discrete memoryless source.</li> <li><code>MarkovChain</code> \u2013 Finite-state homogeneous discrete-time Markov chain.</li> </ul>"},{"location":"ref/#utilities","title":"Utilities","text":"<ul> <li><code>bits_to_int</code> \u2013 Converts a bit array to its integer representation.</li> <li><code>int_to_bits</code> \u2013 Converts an integer, or array of integers, to their bit representations.</li> <li><code>sampling_rate_compress</code> \u2013 Performs sampling rate compression (downsampling).</li> <li><code>sampling_rate_expand</code> \u2013 Performs sampling rate expansion (upsampling).</li> <li><code>fourier_transform</code> \u2013 Computes the Fourier transform.</li> <li><code>boxplus</code> \u2013 Computes the box-plus operation.</li> <li><code>gaussian_q</code> \u2013 Computes the Gaussian Q-function.</li> <li><code>gaussian_q_inv</code> \u2013 Computes the inverse Gaussian Q-function.</li> <li><code>marcum_q</code> \u2013 Computes the Marcum Q-function.</li> <li><code>autocorrelation</code> \u2013 Computes the autocorrelation $R[\\ell]$ of a real or complex sequence $x[n]$.</li> <li><code>cyclic_autocorrelation</code> \u2013 Computes the cyclic autocorrelation $\\tilde{R}[\\ell]$ of a real or complex sequence $x[n]$.</li> <li><code>entropy</code> \u2013 Computes the entropy of a random variable with a given pmf.</li> <li><code>binary_entropy</code> \u2013 Computes the binary entropy function.</li> <li><code>binary_entropy_inv</code> \u2013 Computes the inverse of the binary entropy function.</li> <li><code>relative_entropy</code> \u2013 Computes the relative entropy (Kullback\u2013Leibler divergence) between two pmfs.</li> </ul>"},{"location":"ref/APSKConstellation/","title":"komm.APSKConstellation","text":"<p>Amplitude- and phase-shift keying (APSK) constellation. It is a complex one-dimensional constellation obtained by the union of $K$ component PSK constellations, called rings.</p> <p>Parameters:</p> <ul> <li> <code>orders</code> (<code>tuple[int, ...]</code>)         \u2013          <p>A $K$-tuple with the orders $M_k$ of each ring, for $k \\in [0 : K)$.</p> </li> <li> <code>amplitudes</code> (<code>tuple[float, ...]</code>)         \u2013          <p>A $K$-tuple with the amplitudes $A_k$ of each ring, for $k \\in [0 : K)$.</p> </li> <li> <code>phase_offsets</code> (<code>float | tuple[float, ...]</code>)         \u2013          <p>A $K$-tuple with the phase offsets $\\phi_k$ of each ring, for $k \\in [0 : K)$. If specified as a single float $\\phi$, then it is assumed that $\\phi_k = \\phi$ for all $k \\in [0 : K)$. The default value is <code>0.0</code>.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>The $8$-APSK constellation with $(M_0, M_1) = (4, 4)$, $(A_0, A_1) = (1, 2)$, and $(\\phi_0, \\phi_1) = (0, 0)$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n</code></pre> </li> <li> <p>The $16$-APSK constellation with $(M_0, M_1) = (8, 8)$, $(A_0, A_1) = (1, 2)$, and $(\\phi_0, \\phi_1) = (0, 1/16)$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(\n...     orders=(8, 8),\n...     amplitudes=(1.0, 2.0),\n...     phase_offsets=(0.0, 1 / 16)\n... )\n</code></pre> </li> <li> <p>The $16$-APSK constellation with $(M_0, M_1) = (4, 12)$, $(A_0, A_1) = (\\sqrt{2}, 3)$, and $(\\phi_0, \\phi_1) = (1/8, 0)$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(\n...     orders=(4, 12),\n...     amplitudes=(np.sqrt(2), 3.0),\n...     phase_offsets=(1 / 8, 0.0)\n... )\n</code></pre> </li> </ol>"},{"location":"ref/APSKConstellation/#matrix","title":"<code>matrix</code><code>  Array2D[complexfloating] </code>  <code>cached</code> <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.matrix\narray([[ 1.+0.j],\n       [ 0.+1.j],\n       [-1.+0.j],\n       [ 0.-1.j],\n       [ 2.+0.j],\n       [ 0.+2.j],\n       [-2.+0.j],\n       [ 0.-2.j]])\n</code></pre>"},{"location":"ref/APSKConstellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>For the APSK constellation, it is given by $$     M = \\sum_{k \\in [0:K)} M_k. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.order\n8\n</code></pre>"},{"location":"ref/APSKConstellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>For the APSK constellation, it is given by $N = 1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.dimension\n1\n</code></pre>"},{"location":"ref/APSKConstellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[complexfloating]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean of the APSK constellation is given by $$     \\mathbf{m} = 0. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.mean()\narray([0.+0.j])\n</code></pre>"},{"location":"ref/APSKConstellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean energy of the APSK constellation is given by $$     E = \\frac{1}{M} \\sum_{k \\in [0:K)} A^2 M_k. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(2.5)\n</code></pre>"},{"location":"ref/APSKConstellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(1.0)\n</code></pre>"},{"location":"ref/APSKConstellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([0.-1.j, 1.+0.j])\n</code></pre>"},{"location":"ref/APSKConstellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.closest_indices([0.1 - 1.1j, 1.2 + 0.1j])\narray([3, 0])\n</code></pre>"},{"location":"ref/APSKConstellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.closest_symbols([0.1 - 1.1j, 1.2 + 0.1j])\narray([0.-1.j, 1.+0.j])\n</code></pre>"},{"location":"ref/APSKConstellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.APSKConstellation(orders=(4, 4), amplitudes=(1.0, 2.0))\n&gt;&gt;&gt; const.posteriors([0.1 - 1.1j], snr=2.0).round(3)\narray([0.104, 0.015, 0.076, 0.516, 0.011, 0.   , 0.006, 0.272])\n</code></pre>"},{"location":"ref/ASKConstellation/","title":"komm.ASKConstellation","text":"<p>Amplitude-shift keying (ASK) constellation. It is a complex one-dimensional constellation in which the symbols are uniformly arranged in a ray. More precisely, the $i$-th symbol is given by $$     x_i = iA \\exp(\\mathrm{j} 2 \\pi \\phi), \\quad i \\in [0 : M), $$ where $M$ is the order, $A$ is the base amplitude, and $\\phi$ is the phase offset of the constellation.</p> <p>Parameters:</p> <ul> <li> <code>order</code> (<code>int</code>)         \u2013          <p>The order $M$ of the constellation.</p> </li> <li> <code>base_amplitude</code> (<code>float</code>)         \u2013          <p>The base amplitude $A$ of the constellation. The default value is <code>1.0</code>.</p> </li> <li> <code>phase_offset</code> (<code>float</code>)         \u2013          <p>The phase offset $\\phi$ of the constellation (in turns, not radians). The default value is <code>0.0</code>.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>The $4$-ASK constellation with base amplitude $A = 1$ and phase offset $\\phi = 0$ is depicted below      </p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n</code></pre> </li> <li> <p>The $4$-ASK constellation with base amplitude $A = 2\\sqrt{2}$ and phase offset $\\phi = 1/8$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(\n...     order=4,\n...     base_amplitude=2 * np.sqrt(2),\n...     phase_offset=1 / 8,\n... )\n</code></pre> </li> </ol>"},{"location":"ref/ASKConstellation/#matrix","title":"<code>matrix</code><code>  Array2D[complexfloating] </code>  <code>cached</code> <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.matrix\narray([[0.+0.j],\n       [1.+0.j],\n       [2.+0.j],\n       [3.+0.j]])\n</code></pre>"},{"location":"ref/ASKConstellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.order\n4\n</code></pre>"},{"location":"ref/ASKConstellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>For the ASK constellation, it is given by $N = 1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.dimension\n1\n</code></pre>"},{"location":"ref/ASKConstellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[complexfloating]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean of the ASK constellation is given by $$     \\mathbf{m} = \\frac{A}{2} (M-1) \\exp(\\mathrm{j}\\phi). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.mean()\narray([1.5+0.j])\n</code></pre>"},{"location":"ref/ASKConstellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean energy of the ASK constellation is given by $$     E = \\frac{A^2}{6} (M - 1) (2M - 1). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(3.5)\n</code></pre>"},{"location":"ref/ASKConstellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>For the ASK constellation, the minimum distance is given by $$     d_{\\min} = A $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(1.0)\n</code></pre>"},{"location":"ref/ASKConstellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([3.+0.j, 0.+0.j])\n</code></pre>"},{"location":"ref/ASKConstellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.closest_indices([3.1 + 0.2j, 0.1 - 0.2j])\narray([3, 0])\n</code></pre>"},{"location":"ref/ASKConstellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.closest_symbols([3.1 + 0.2j, 0.1 - 0.2j])\narray([3.+0.j, 0.+0.j])\n</code></pre>"},{"location":"ref/ASKConstellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.ASKConstellation(4)\n&gt;&gt;&gt; const.posteriors([3.1 + 0.2j, 0.1 - 0.2j], snr=5.0).round(3)\narray([0.   , 0.002, 0.152, 0.846, 0.755, 0.241, 0.004, 0.   ])\n</code></pre>"},{"location":"ref/AWGNChannel/","title":"komm.AWGNChannel","text":"<p>Additive white Gaussian noise (AWGN) channel. It is defined by $$     Y_n = X_n + Z_n, $$ where $X_n$ is the channel input signal, $Y_n$ is the channel output signal, and $Z_n$ is the noise, which is iid according to a Gaussian distribution with zero mean. The channel signal-to-noise ratio is calculated by $$     \\snr = \\frac{P}{N}, $$ where $P = \\mathrm{E}[X^2_n]$ is the average power of the input signal, and $N = \\mathrm{E}[Z^2_n]$ is the average power (and variance) of the noise. For more details, see CT06, Ch. 9.</p> <p>Parameters:</p> <ul> <li> <code>signal_power</code> (<code>float | Literal['measured']</code>)         \u2013          <p>The input signal power $P$. If equal to the string <code>'measured'</code>, then every time the channel is invoked the input signal power will be computed from the input itself (i.e., its squared Euclidean norm).</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The channel signal-to-noise ratio $\\snr$ (linear, not decibel). The default value is <code>np.inf</code>, which corresponds to a noiseless channel.</p> </li> </ul>"},{"location":"ref/AWGNChannel/#noise_power","title":"<code>noise_power</code><code>  float </code>  <code>cached</code> <code>property</code>","text":"<p>The noise power $N$.</p>"},{"location":"ref/AWGNChannel/#capacity","title":"<code>capacity()</code>","text":"<p>Returns the channel capacity $C$. It is given by $C = \\frac{1}{2}\\log_2(1 + \\snr)$, in bits per dimension.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; awgn = komm.AWGNChannel(signal_power=1.0, snr=63.0)\n&gt;&gt;&gt; awgn.capacity()\nnp.float64(3.0)\n</code></pre>"},{"location":"ref/AWGNChannel/#transmit","title":"<code>transmit()</code>","text":"<p>Transmits the input signal through the channel and returns the output signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $X_n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The output signal $Y_n$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; awgn = komm.AWGNChannel(signal_power=5.0, snr=200.0)\n&gt;&gt;&gt; x = [1.0, 3.0, -3.0, -1.0, -1.0, 1.0, 3.0, 1.0, -1.0, 3.0]\n&gt;&gt;&gt; awgn.transmit(x).round(2)\narray([ 1.05,  2.84, -2.88, -0.85, -1.31,  0.79,  3.02,  0.95, -1.  ,  2.87])\n</code></pre>"},{"location":"ref/BCHCode/","title":"komm.BCHCode","text":"<p>Bose\u2013Ray-Chaudhuri\u2013Hocquenghem (BCH) code. For given parameters $\\mu \\geq 2$ and $\\delta$ satisfying $2 \\leq \\delta \\leq 2^{\\mu} - 1$, a binary BCH code is a cyclic code with generator polynomial given by $$     g(X) = \\mathrm{lcm} \\left\\{ \\phi_1(X), \\phi_2(X), \\ldots, \\phi_{\\delta - 1}(X) \\right\\}, $$ where $\\phi_i(X)$ is the minimal polynomial of $\\alpha^i$, and $\\alpha$ is a primitive element of $\\mathrm{GF}(2^\\mu)$. The parameter $\\delta$ must be a Bose distance. The resulting code is denoted by $\\bch(\\mu, \\delta)$, and has the following parameters, where $\\delta = 2 \\tau + 1$:</p> <ul> <li>Length: $n = 2^{\\mu} - 1$</li> <li>Dimension: $k \\geq n - \\mu \\tau$</li> <li>Redundancy: $m \\leq \\mu \\tau$</li> <li>Minimum distance: $d \\geq \\delta$</li> </ul> <p>Only narrow-sense and primitive BCH codes are implemented. For more details, see LC04, Ch. 6 and HP03, Sec. 5.1.</p> <p>The table below lists the possible values of $\\delta$ for $2 \\leq \\mu \\leq 10$.</p> $\\mu$ $n$ Bose distances $\\delta$ $2$ $3$ $3$ $3$ $7$ $3$, $7$ $4$ $15$ $3$, $5$, $7$, $15$ $5$ $31$ $3$, $5$, $7$, $11$, $15$, $31$ $6$ $63$ $3$, $5$, $7$, $9$, $11$, $13$, $15$, $21$, $23$, $27$, $31$, $63$ $7$ $127$ $3$, $5$, $7$, $9$, $11$, $13$, $15$, $19$, $21$, $23$, $27$, $29$, $31$, $43$, $47$, $55$, $63$, $127$ $8$ $255$ $3$, $5$, $7$, $9$, $11$, $13$, $15$, $17$, $19$, $21$, $23$, $25$, $27$, $29$, $31$, $37$, $39$, $43$, $45$, $47$, $51$, $53$, $55$, $59$, $61$, $63$, $85$, $87$, $91$, $95$, $111$, $119$, $127$, $255$ $9$ $511$ $3$, $5$, $7$, $9$, $11$, $13$, $15$, $17$, $19$, $21$, $23$, $25$, $27$, $29$, $31$, $35$, $37$, $39$, $41$, $43$, $45$, $47$, $51$, $53$, $55$, $57$, $59$, $61$, $63$, $73$, $75$, $77$, $79$, $83$, $85$, $87$, $91$, $93$, $95$, $103$, $107$, $109$, $111$, $117$, $119$, $123$, $125$, $127$, $171$, $175$, $183$, $187$, $191$, $219$, $223$, $239$, $255$, $511$ $10$ $1023$ $3$, $5$, $7$, $9$, $11$, $13$, $15$, $17$, $19$, $21$, $23$, $25$, $27$, $29$, $31$, $33$, $35$, $37$, $39$, $41$, $43$, $45$, $47$, $49$, $51$, $53$, $55$, $57$, $59$, $61$, $63$, $69$, $71$, $73$, $75$, $77$, $79$, $83$, $85$, $87$, $89$, $91$, $93$, $95$, $99$, $101$, $103$, $105$, $107$, $109$, $111$, $115$, $117$, $119$, $121$, $123$, $125$, $127$, $147$, $149$, $151$, $155$, $157$, $159$, $165$, $167$, $171$, $173$, $175$, $179$, $181$, $183$, $187$, $189$, $191$, $205$, $207$, $213$, $215$, $219$, $221$, $223$, $231$, $235$, $237$, $239$, $245$, $247$, $251$, $253$, $255$, $341$, $343$, $347$, $351$, $363$, $367$, $375$, $379$, $383$, $439$, $447$, $479$, $495$, $511$, $1023$ Notes <ul> <li>For $\\delta = 3$ it reduces to the Hamming code.</li> <li>For $\\delta = 2^{\\mu} - 1$ it reduces to the repetition code.</li> </ul> <p>Parameters:</p> <ul> <li> <code>mu</code> (<code>int</code>)         \u2013          <p>The parameter $\\mu$ of the BCH code.</p> </li> <li> <code>delta</code> (<code>int</code>)         \u2013          <p>The Bose distance $\\delta$ of the BCH code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BCHCode(mu=5, delta=7)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(31, 16, 15)\n&gt;&gt;&gt; code.generator_polynomial\nBinaryPolynomial(0b1000111110101111)\n&gt;&gt;&gt; code.minimum_distance()\n7\n</code></pre> <pre><code>&gt;&gt;&gt; komm.BCHCode(mu=7, delta=31)\nBCHCode(mu=7, delta=31)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.BCHCode(mu=7, delta=32)\nTraceback (most recent call last):\n...\nValueError: 'delta' must be a Bose distance (next one is 43)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.BCHCode(mu=7, delta=43)\nBCHCode(mu=7, delta=43)\n</code></pre>"},{"location":"ref/BCJRDecoder/","title":"komm.BCJRDecoder","text":"<p>Bahl\u2013Cocke\u2013Jelinek\u2013Raviv (BCJR) decoder for terminated convolutional codes. For more details, see LC04, Sec. 12.6.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>TerminatedConvolutionalCode</code>)         \u2013          <p>The terminated convolutional code to be used for decoding.</p> </li> <li> <code>output_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the output. Either <code>'hard'</code> or <code>'soft'</code>. Default is <code>'soft'</code>.</p> </li> </ul> Notes <ul> <li>Input type: <code>soft</code> (L-values).</li> <li>Output type: <code>hard</code> (bits) or <code>soft</code> (L-values).</li> </ul>"},{"location":"ref/BCJRDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.TerminatedConvolutionalCode(\n...     convolutional_code=komm.ConvolutionalCode([[0b11, 0b1]], [0b11]),\n...     num_blocks=3,\n...     mode=\"zero-termination\",\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.BCJRDecoder(code)\n&gt;&gt;&gt; decoder.decode([-0.8, -0.1, -1.0, +0.5, +1.8, -1.1, -1.6, +1.6])\narray([-0.47774884, -0.61545527,  1.03018771])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.BCJRDecoder(code, output_type=\"hard\")\n&gt;&gt;&gt; decoder.decode([-0.8, -0.1, -1.0, +0.5, +1.8, -1.1, -1.6, +1.6])\narray([1, 1, 0])\n</code></pre>"},{"location":"ref/BarkerSequence/","title":"komm.BarkerSequence","text":"<p>Barker sequence. A Barker sequence is a binary sequence with autocorrelation $R[\\ell]$ satisfying $|R[\\ell]| \\leq 1$, for $\\ell \\neq 0$. The only known Barker sequences (up to negation and reversion) are shown in the table below. For more details, see Wikipedia: Barker code.</p> Length $L$ Barker sequence $b[n]$ $2$ $01$ and $00$ $3$ $001$ $4$ $0010$ and $0001$ $5$ $00010$ $7$ $0001101$ $11$ $00011101101$ $13$ $0000011001010$ <p>Parameters:</p> <ul> <li> <code>length</code> (<code>int</code>)         \u2013          <p>Length of the Barker sequence. Must be in the set $\\{ 2, 3, 4, 5, 7, 11, 13 \\}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; barker = komm.BarkerSequence(length=13)\n&gt;&gt;&gt; barker.polar_sequence\narray([ 1,  1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1])\n&gt;&gt;&gt; barker.autocorrelation()\narray([13,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1,  0,  1])\n</code></pre>"},{"location":"ref/BerlekampDecoder/","title":"komm.BerlekampDecoder","text":"<p>Berlekamp decoder for BCH codes. For more details, see LC04, Sec. 6.3.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>BCHCode</code>)         \u2013          <p>The BCH code to be used for decoding.</p> </li> </ul> Notes <ul> <li>Input type: <code>hard</code> (bits).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/BerlekampDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BCHCode(4, 7)\n&gt;&gt;&gt; decoder = komm.BerlekampDecoder(code)\n&gt;&gt;&gt; decoder.decode([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0])\narray([0, 0, 0, 0, 0])\n</code></pre>"},{"location":"ref/BinaryErasureChannel/","title":"komm.BinaryErasureChannel","text":"<p>Binary erasure channel (BEC). It is a discrete memoryless channel with input alphabet $\\mathcal{X} = \\{ 0, 1 \\}$ and output alphabet $\\mathcal{Y} = \\{ 0, 1, 2 \\}$. The channel is characterized by a parameter $\\epsilon$, called the erasure probability. With probability $1 - \\epsilon$, the output symbol is identical to the input symbol, and with probability $\\epsilon$, the output symbol is replaced by an erasure symbol (denoted by $2$). For more details, see CT06, Sec. 7.1.5.</p> <p>Parameters:</p> <ul> <li> <code>erasure_probability</code> (<code>float</code>)         \u2013          <p>The channel erasure probability $\\epsilon$. Must satisfy $0 \\leq \\epsilon \\leq 1$. Default value is <code>0.0</code>, which corresponds to a noiseless channel.</p> </li> </ul>"},{"location":"ref/BinaryErasureChannel/#input_cardinality","title":"<code>input_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel input cardinality $|\\mathcal{X}|$.</p> <p>For the BEC, it is given by $|\\mathcal{X}| = 2$.</p>"},{"location":"ref/BinaryErasureChannel/#output_cardinality","title":"<code>output_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel output cardinality $|\\mathcal{Y}|$.</p> <p>For the BEC, it is given by $|\\mathcal{Y}| = 3$.</p>"},{"location":"ref/BinaryErasureChannel/#transition_matrix","title":"<code>transition_matrix</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The channel transition probability matrix $p_{Y \\mid X}$.</p> <p>For the BEC, it is given by $$     p_{Y \\mid X} =     \\begin{bmatrix}         1 - \\epsilon &amp; 0 &amp; \\epsilon \\\\         0 &amp; 1 - \\epsilon &amp; \\epsilon     \\end{bmatrix}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bec = komm.BinaryErasureChannel(0.2)\n&gt;&gt;&gt; bec.transition_matrix\narray([[0.8, 0. , 0.2],\n       [0. , 0.8, 0.2]])\n</code></pre>"},{"location":"ref/BinaryErasureChannel/#mutual_information","title":"<code>mutual_information()</code>","text":"<p>Returns the mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$ of the channel.</p> <p>Parameters:</p> <ul> <li> <code>input_pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p_X$ of the channel input $X$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$.</p> </li> </ul> <p>For the BEC, it is given by $$     \\mathrm{I}(X ; Y) = (1 - \\epsilon) \\, \\Hb(\\pi), $$ in bits, where $\\pi = \\Pr[X = 1]$, and $\\Hb$ is the binary entropy function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bec = komm.BinaryErasureChannel(0.2)\n&gt;&gt;&gt; bec.mutual_information([0.45, 0.55])\nnp.float64(0.7942195631902467)\n</code></pre>"},{"location":"ref/BinaryErasureChannel/#capacity","title":"<code>capacity()</code>","text":"<p>Returns the channel capacity $C$.</p> <p>Parameters:</p> <ul> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The channel capacity $C$.</p> </li> </ul> <p>For the BEC, it is given by $$     C = 1 - \\epsilon, $$ in bits.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bec = komm.BinaryErasureChannel(0.2)\n&gt;&gt;&gt; bec.capacity()\nnp.float64(0.8)\n</code></pre>"},{"location":"ref/BinaryErasureChannel/#transmit","title":"<code>transmit()</code>","text":"<p>Transmits the input sequence through the channel and returns the output sequence.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bec = komm.BinaryErasureChannel(0.2)\n&gt;&gt;&gt; bec.transmit([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\narray([1, 1, 1, 0, 2, 0, 1, 0, 2, 0])\n</code></pre>"},{"location":"ref/BinaryPolynomial/","title":"komm.BinaryPolynomial","text":"<p>Binary polynomial. A binary polynomial is a polynomial whose coefficients are elements in the finite field $\\mathbb{F}_2 = \\{ 0, 1 \\}$.</p> <p>The default constructor of the class expects the following:</p> <p>Parameters:</p> <ul> <li> <code>value</code> (<code>SupportsInt</code>)         \u2013          <p>An integer whose binary digits represent the coefficients of the polynomial\u2014the leftmost bit standing for the highest degree term. For example, the binary polynomial $X^4 + X^3 + X$ is represented by the integer <code>0b11010</code> = <code>0o32</code> = <code>26</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\nBinaryPolynomial(0b11010)\n</code></pre> <p>See also the class methods <code>from_coefficients</code> and <code>from_exponents</code> for alternative ways to construct a binary polynomial.</p> Algebraic structure <p>The binary polynomials form an Euclidean domain. The following operations are supported: addition (<code>+</code>), subtraction (<code>-</code>), multiplication (<code>*</code>), euclidean division (<code>//</code>), modulo (<code>%</code>), and exponentiation (<code>**</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly1 = komm.BinaryPolynomial(0b10111)  # X^4 + X^2 + X + 1\n&gt;&gt;&gt; poly2 = komm.BinaryPolynomial(0b101)  # X^2 + 1\n&gt;&gt;&gt; poly1 + poly2  # X^4 + X\nBinaryPolynomial(0b10010)\n&gt;&gt;&gt; poly1 - poly2  # X^4 + X\nBinaryPolynomial(0b10010)\n&gt;&gt;&gt; poly1 * poly2  # X^6 + X^3 + X + 1\nBinaryPolynomial(0b1001011)\n&gt;&gt;&gt; poly1 // poly2  # X^2\nBinaryPolynomial(0b100)\n&gt;&gt;&gt; poly1 % poly2  # X + 1\nBinaryPolynomial(0b11)\n&gt;&gt;&gt; poly1 ** 2  # X^8 + X^4 + X^2 + 1\nBinaryPolynomial(0b100010101)\n</code></pre>"},{"location":"ref/BinaryPolynomial/#from_coefficients","title":"<code>from_coefficients()</code>  <code>classmethod</code>","text":"<p>Constructs a binary polynomial from its coefficients.</p> <p>Parameters:</p> <ul> <li> <code>coefficients</code> (<code>ArrayLike</code>)         \u2013          <p>The coefficients of the binary polynomial\u2014the $i$-th element of the array standing for the coefficient of $X^i$. For example, <code>[0, 1, 0, 1, 1]</code> represents the binary polynomial $X^4 + X^3 + X$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomial.from_coefficients([0, 1, 0, 1, 1])  # X^4 + X^3 + X\nBinaryPolynomial(0b11010)\n</code></pre>"},{"location":"ref/BinaryPolynomial/#from_exponents","title":"<code>from_exponents()</code>  <code>classmethod</code>","text":"<p>Constructs a binary polynomial from its exponents.</p> <p>Parameters:</p> <ul> <li> <code>exponents</code> (<code>ArrayLike</code>)         \u2013          <p>The exponents of the nonzero terms of the binary polynomial. For example, <code>[1, 3, 4]</code> represents the binary polynomial $X^4 + X^3 + X$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomial.from_exponents([1, 3, 4])  # X^4 + X^3 + X\nBinaryPolynomial(0b11010)\n</code></pre>"},{"location":"ref/BinaryPolynomial/#degree","title":"<code>degree</code><code>  int </code>  <code>property</code>","text":"<p>The degree of the binary polynomial.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\n&gt;&gt;&gt; poly.degree\n4\n</code></pre>"},{"location":"ref/BinaryPolynomial/#coefficients","title":"<code>coefficients()</code>","text":"<p>Returns the coefficients of the binary polynomial.</p> <p>Parameters:</p> <ul> <li> <code>width</code> (<code>int | None</code>)         \u2013          <p>If this parameter is specified, the output will be filled with zeros on the right so that the its length will be the specified value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>coefficients</code>  (<code>NDArray[integer]</code>)          \u2013          <p>Coefficients of the binary polynomial. The $i$-th element of the array stands for the coefficient of $X^i$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\n&gt;&gt;&gt; poly.coefficients()\narray([0, 1, 0, 1, 1])\n&gt;&gt;&gt; poly.coefficients(width=8)\narray([0, 1, 0, 1, 1, 0, 0, 0])\n</code></pre>"},{"location":"ref/BinaryPolynomial/#exponents","title":"<code>exponents()</code>","text":"<p>Returns the exponents of the binary polynomial.</p> <p>Returns:</p> <ul> <li> <code>exponents</code>  (<code>NDArray[integer]</code>)          \u2013          <p>Exponents of the nonzero terms of the binary polynomial. The exponents are returned in ascending order.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\n&gt;&gt;&gt; poly.exponents()\narray([1, 3, 4])\n</code></pre>"},{"location":"ref/BinaryPolynomial/#reciprocal","title":"<code>reciprocal()</code>","text":"<p>Returns the reciprocal (reflexion) of the binary polynomial. The reciprocal of a binary polynomial is the polynomial with the coefficients in reversed order, that is, the coefficient of $X^i$ becomes the coefficient of $X^{n-i}$, where $n$ is the degree of the polynomial.</p> <p>Returns:</p> <ul> <li> <code>result</code>  (<code>BinaryPolynomial</code>)          \u2013          <p>The binary polynomial with the reversed coefficients.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\n&gt;&gt;&gt; poly.reciprocal()  # X^3 + X + 1\nBinaryPolynomial(0b1011)\n</code></pre>"},{"location":"ref/BinaryPolynomial/#evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluates the binary polynomial at a given point. Uses Horner's method.</p> <p>Parameters:</p> <ul> <li> <code>point</code> (<code>RingElement</code>)         \u2013          <p>The point at which the polynomial is evaluated. It must be an element of a ring in which multiplication by integers is defined.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code>  (<code>RingElement</code>)          \u2013          <p>The result of evaluating the binary polynomial at <code>point</code>. It has the same type as <code>point</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = komm.BinaryPolynomial(0b11010)  # X^4 + X^3 + X\n&gt;&gt;&gt; poly.evaluate(komm.Integer(7))  # same as 7**4 + 7**3 + 7\nInteger(2751)\n</code></pre>"},{"location":"ref/BinaryPolynomial/#is_irreducible","title":"<code>is_irreducible()</code>","text":"<p>Checks whether the binary polynomial is irreducible. A binary polynomial is irreducible if it is not divisible by any other nonconstant binary polynomial of smaller degree.</p> <p>This method checks the irreducibility using Rabin's irreducibility test.</p> <p>Returns:</p> <ul> <li> <code>result</code>  (<code>bool</code>)          \u2013          <p><code>True</code>, if the binary polynomial is irreducible; <code>False</code>, otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomial(0b11011).is_irreducible()\nFalse\n&gt;&gt;&gt; komm.BinaryPolynomial(0b11111).is_irreducible()\nTrue\n&gt;&gt;&gt; komm.BinaryPolynomial(0b10011).is_irreducible()\nTrue\n</code></pre>"},{"location":"ref/BinaryPolynomial/#is_primitive","title":"<code>is_primitive()</code>","text":"<p>Checks whether the binary polynomial is primitive. A binary polynomial of degree $m$ is primitive if it is irreducible and if the smallest positive integer $n$ such that the polynomial divides $X^n + 1$ is $n = 2^m - 1$.</p> <p>Returns:</p> <ul> <li> <code>result</code>  (<code>bool</code>)          \u2013          <p><code>True</code>, if the binary polynomial is primitive; <code>False</code>, otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomial(0b11011).is_primitive()\nFalse\n&gt;&gt;&gt; komm.BinaryPolynomial(0b11111).is_primitive()\nFalse\n&gt;&gt;&gt; komm.BinaryPolynomial(0b10011).is_primitive()\nTrue\n</code></pre>"},{"location":"ref/BinaryPolynomial/#xgcd","title":"<code>xgcd()</code>  <code>classmethod</code>","text":"<p>Performs the extended Euclidean algorithm on two given binary polynomials.</p>"},{"location":"ref/BinaryPolynomial/#gcd","title":"<code>gcd()</code>  <code>classmethod</code>","text":"<p>Computes the greatest common divisor (gcd) of the arguments.</p>"},{"location":"ref/BinaryPolynomial/#lcm","title":"<code>lcm()</code>  <code>classmethod</code>","text":"<p>Computes the least common multiple (lcm) of the arguments.</p>"},{"location":"ref/BinaryPolynomialFraction/","title":"komm.BinaryPolynomialFraction","text":"<p>Binary polynomial fraction. A binary polynomial fraction is a ratio of two binary polynomials.</p> <p>Parameters:</p> <ul> <li> <code>numerator</code> (<code>SupportsInt</code>)         \u2013          <p>The numerator of the fraction.</p> </li> <li> <code>denominator</code> (<code>SupportsInt</code>)         \u2013          <p>The denominator of the fraction.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.BinaryPolynomialFraction(0b11010, 0b101)  # (X^4 + X^3 + X) / (X^2 + 1)\nBinaryPolynomialFraction(0b11010, 0b101)\n</code></pre> Algebraic structure <p>The binary polynomial fractions form a field. The following operations are supported: addition (<code>+</code>), subtraction (<code>-</code>), multiplication (<code>*</code>), division (<code>/</code>), and exponentiation (<code>**</code>).</p>"},{"location":"ref/BinarySequence/","title":"komm.BinarySequence","text":"<p>General binary sequence. It may be represented either in bit format, denoted by $b[n]$, with elements in the set $\\{ 0, 1 \\}$, or in polar format, denoted by $x[n]$, with elements in the set $\\{ \\pm 1 \\}$. The correspondences $0 \\mapsto +1$ and $1 \\mapsto -1$ from bit format to polar format is assumed.</p> <p>The constructor expects either the bit sequence or the polar sequence.</p> <p>Parameters:</p> <ul> <li> <code>bit_sequence</code> (<code>ArrayLike | None</code>)         \u2013          <p>The binary sequence in bit format, $b[n] \\in \\{ 0, 1 \\}$.</p> </li> <li> <code>polar_sequence</code> (<code>ArrayLike | None</code>)         \u2013          <p>The binary sequence in polar format, $x[n] \\in \\{ \\pm 1 \\}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = komm.BinarySequence(bit_sequence=[0, 1, 1, 0])\n&gt;&gt;&gt; seq.bit_sequence\narray([0, 1, 1, 0])\n&gt;&gt;&gt; seq.polar_sequence\narray([ 1, -1, -1,  1])\n</code></pre> <pre><code>&gt;&gt;&gt; seq = komm.BinarySequence(polar_sequence=[1, -1, -1, 1])\n&gt;&gt;&gt; seq.bit_sequence\narray([0, 1, 1, 0])\n&gt;&gt;&gt; seq.polar_sequence\narray([ 1, -1, -1,  1])\n</code></pre>"},{"location":"ref/BinarySequence/#length","title":"<code>length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The length (or period) $L$ of the binary sequence.</p>"},{"location":"ref/BinarySequence/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>Returns the autocorrelation $R[\\ell]$ of the binary sequence in polar format. See <code>komm.autocorrelation</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>See the corresponding parameter in <code>komm.autocorrelation</code>.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>See the corresponding parameter in <code>komm.autocorrelation</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[floating]</code>         \u2013          <p>The autocorrelation $R[\\ell]$ of the binary sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = komm.BinarySequence(bit_sequence=[0, 1, 1, 0])\n&gt;&gt;&gt; seq.autocorrelation()\narray([ 4, -1, -2,  1])\n</code></pre>"},{"location":"ref/BinarySequence/#cyclic_autocorrelation","title":"<code>cyclic_autocorrelation()</code>","text":"<p>Returns the cyclic autocorrelation $\\tilde{R}[\\ell]$ of the binary sequence in polar format. See <code>komm.cyclic_autocorrelation</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>See the corresponding parameter in <code>komm.cyclic_autocorrelation</code>.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>See the corresponding parameter in <code>komm.cyclic_autocorrelation</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[floating]</code>         \u2013          <p>The cyclic autocorrelation $\\tilde{R}[\\ell]$ of the binary sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = komm.BinarySequence(bit_sequence=[0, 1, 1, 0])\n&gt;&gt;&gt; seq.cyclic_autocorrelation()\narray([ 4,  0, -4,  0])\n</code></pre>"},{"location":"ref/BinarySymmetricChannel/","title":"komm.BinarySymmetricChannel","text":"<p>Binary symmetric channel (BSC). It is a discrete memoryless channel with input and output alphabets $\\mathcal{X} = \\mathcal{Y} = \\{ 0, 1 \\}$. The channel is characterized by a parameter $p$, called the crossover probability. With probability $1 - p$, the output symbol is identical to the input symbol, and with probability $p$, the output symbol is flipped. Equivalently, the channel can be modeled as $$     Y_n = X_n + Z_n, $$ where $Z_n$ are iid Bernoulli random variables with $\\Pr[Z_n = 1] = p$. For more details, see CT06, Sec. 7.1.4.</p> <p>Parameters:</p> <ul> <li> <code>crossover_probability</code> (<code>float</code>)         \u2013          <p>The channel crossover probability $p$. Must satisfy $0 \\leq p \\leq 1$. The default value is <code>0.0</code>, which corresponds to a noiseless channel.</p> </li> </ul>"},{"location":"ref/BinarySymmetricChannel/#input_cardinality","title":"<code>input_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel input cardinality $|\\mathcal{X}|$.</p> <p>For the BSC, it is given by $|\\mathcal{X}| = 2$.</p>"},{"location":"ref/BinarySymmetricChannel/#output_cardinality","title":"<code>output_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel output cardinality $|\\mathcal{Y}|$.</p> <p>For the BSC, it is given by $|\\mathcal{Y}| = 2$.</p>"},{"location":"ref/BinarySymmetricChannel/#transition_matrix","title":"<code>transition_matrix</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The channel transition probability matrix $p_{Y \\mid X}$.</p> <p>For the BSC, it is given by $$     p_{Y \\mid X} = \\begin{bmatrix} 1-p &amp; p \\\\ p &amp; 1-p \\end{bmatrix}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bsc = komm.BinarySymmetricChannel(0.2)\n&gt;&gt;&gt; bsc.transition_matrix\narray([[0.8, 0.2],\n       [0.2, 0.8]])\n</code></pre>"},{"location":"ref/BinarySymmetricChannel/#mutual_information","title":"<code>mutual_information()</code>","text":"<p>Returns the mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$ of the channel.</p> <p>Parameters:</p> <ul> <li> <code>input_pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p_X$ of the channel input $X$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$.</p> </li> </ul> <p>For the BSC, it is given by $$     \\mathrm{I}(X ; Y) = \\Hb(p + \\pi - 2 p \\pi) - \\Hb(p), $$ in bits, where $\\pi = \\Pr[X = 1]$, and $\\Hb$ is the binary entropy function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bsc = komm.BinarySymmetricChannel(0.2)\n&gt;&gt;&gt; bsc.mutual_information([0.45, 0.55])\nnp.float64(0.2754734936803773)\n</code></pre>"},{"location":"ref/BinarySymmetricChannel/#capacity","title":"<code>capacity()</code>","text":"<p>Returns the channel capacity $C$.</p> <p>Parameters:</p> <ul> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The channel capacity $C$.</p> </li> </ul> <p>For the BSC, it is given by $$     C = 1 - \\Hb(p), $$ in bits, where $\\Hb$ is the binary entropy function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bsc = komm.BinarySymmetricChannel(0.2)\n&gt;&gt;&gt; bsc.capacity()\nnp.float64(0.2780719051126377)\n</code></pre>"},{"location":"ref/BinarySymmetricChannel/#transmit","title":"<code>transmit()</code>","text":"<p>Transmits the input sequence through the channel and returns the output sequence.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bsc = komm.BinarySymmetricChannel(0.2)\n&gt;&gt;&gt; bsc.transmit([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\narray([1, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n</code></pre>"},{"location":"ref/BlockCode/","title":"komm.BlockCode","text":"<p>General binary linear block code. It is characterized by its generator matrix $G \\in \\mathbb{B}^{k \\times n}$, and by its check matrix $H \\in \\mathbb{B}^{m \\times n}$, which are related by $G H^\\transpose = 0$. The parameters $n$, $k$, and $m$ are called the code length, dimension, and redundancy, respectively, and are related by $k + m = n$. For more details, see LC04, Ch. 3.</p> <p>The constructor expects either the generator matrix or the check matrix.</p> <p>Parameters:</p> <ul> <li> <code>generator_matrix</code> (<code>ArrayLike | None</code>)         \u2013          <p>The generator matrix $G$ of the code, which is a $k \\times n$ binary matrix.</p> </li> <li> <code>check_matrix</code> (<code>ArrayLike | None</code>)         \u2013          <p>The check matrix $H$ of the code, which is a $m \\times n$ binary matrix.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 2, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(check_matrix=[\n...     [0, 1, 1, 0, 0],\n...     [1, 1, 0, 1, 0],\n...     [1, 0, 0, 0, 1],\n... ])\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 2, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n</code></pre>"},{"location":"ref/BlockCode/#length","title":"<code>length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The length $n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.length\n5\n</code></pre>"},{"location":"ref/BlockCode/#dimension","title":"<code>dimension</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The dimension $k$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.dimension\n2\n</code></pre>"},{"location":"ref/BlockCode/#redundancy","title":"<code>redundancy</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The redundancy $m$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.redundancy\n3\n</code></pre>"},{"location":"ref/BlockCode/#rate","title":"<code>rate</code><code>  float </code>  <code>cached</code> <code>property</code>","text":"<p>The rate $R = k/n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.rate\n0.4\n</code></pre>"},{"location":"ref/BlockCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The generator matrix $G \\in \\mathbb{B}^{k \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(check_matrix=[\n...     [0, 1, 1, 0, 0],\n...     [1, 1, 0, 1, 0],\n...     [1, 0, 0, 0, 1],\n... ])\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0]])\n</code></pre>"},{"location":"ref/BlockCode/#check_matrix","title":"<code>check_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The check matrix $H \\in \\mathbb{B}^{m \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.check_matrix\narray([[0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n</code></pre>"},{"location":"ref/BlockCode/#encode","title":"<code>encode()</code>","text":"<p>Applies the encoding mapping $\\Enc : \\mathbb{B}^k \\to \\mathbb{B}^n$ of the code. This method takes one or more sequences of messages and returns their corresponding codeword sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $k$, or a multidimensional array where the last dimension is a multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension expanded from $bk$ to $bn$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.encode([0, 0])  # Sequence with single message\narray([0, 0, 0, 0, 0])\n&gt;&gt;&gt; code.encode([0, 0, 1, 1])  # Sequence with two messages\narray([0, 0, 0, 0, 0, 1, 1, 1, 0, 1])\n&gt;&gt;&gt; code.encode([[0, 0],  # 2D array of single messages\n...              [1, 1]])\narray([[0, 0, 0, 0, 0],\n       [1, 1, 1, 0, 1]])\n&gt;&gt;&gt; code.encode([[0, 0, 1, 1],  # 2D array of two messages\n...              [1, 1, 1, 0]])\narray([[0, 0, 0, 0, 0, 1, 1, 1, 0, 1],\n       [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]])\n</code></pre>"},{"location":"ref/BlockCode/#inverse_encode","title":"<code>inverse_encode()</code>","text":"<p>Applies the inverse encoding partial mapping $\\Enc^{-1} : \\mathbb{B}^n \\rightharpoonup \\mathbb{B}^k$ of the code. This method takes one or more sequences of codewords and returns their corresponding message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input contains any invalid codewords.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.inverse_encode([0, 0, 0, 0, 0])  # Sequence with single codeword\narray([0, 0])\n&gt;&gt;&gt; code.inverse_encode([0, 0, 0, 0, 0, 1, 1, 1, 0, 1])  # Sequence with two codewords\narray([0, 0, 1, 1])\n&gt;&gt;&gt; code.inverse_encode([[0, 0, 0, 0, 0],  # 2D array of single codewords\n...                      [1, 1, 1, 0, 1]])\narray([[0, 0],\n       [1, 1]])\n&gt;&gt;&gt; code.inverse_encode([[0, 0, 0, 0, 0, 1, 1, 1, 0, 1],  # 2D array of two codewords\n...                      [1, 1, 1, 0, 1, 1, 0, 0, 1, 1]])\narray([[0, 0, 1, 1],\n       [1, 1, 1, 0]])\n</code></pre>"},{"location":"ref/BlockCode/#check","title":"<code>check()</code>","text":"<p>Applies the check mapping $\\mathrm{Chk} : \\mathbb{B}^n \\to \\mathbb{B}^m$ of the code. This method takes one or more sequences of received words and returns their corresponding syndrome sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bm$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.check([1, 1, 1, 0, 1])  # Sequence with single received word\narray([0, 0, 0])\n&gt;&gt;&gt; code.check([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])  # Sequence with two received words\narray([0, 0, 0, 0, 1, 0])\n&gt;&gt;&gt; code.check([[1, 1, 1, 0, 1],  # 2D array of single received words\n...             [1, 1, 1, 1, 1]])\narray([[0, 0, 0],\n       [0, 1, 0]])\n&gt;&gt;&gt; code.check([[1, 1, 1, 0, 1, 1, 1, 1, 1, 1],  # 2D array of two received words\n...             [1, 1, 1, 1, 1, 0, 0, 0, 1, 1]])\narray([[0, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 1, 1]])\n</code></pre>"},{"location":"ref/BlockCode/#codewords","title":"<code>codewords()</code>  <code>cached</code>","text":"<p>Returns the codewords of the code. This is a $2^k \\times n$ matrix whose rows are all the codewords. The codeword in row $i$ corresponds to the message obtained by expressing $i$ in binary with $k$ bits (LSB-first).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.codewords()\narray([[0, 0, 0, 0, 0],\n       [1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0],\n       [1, 1, 1, 0, 1]])\n</code></pre>"},{"location":"ref/BlockCode/#codeword_weight_distribution","title":"<code>codeword_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the codeword weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of codewords of Hamming weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([1, 0, 0, 2, 1, 0])\n</code></pre>"},{"location":"ref/BlockCode/#minimum_distance","title":"<code>minimum_distance()</code>  <code>cached</code>","text":"<p>Returns the minimum distance $d$ of the code. This is equal to the minimum Hamming weight of the non-zero codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre>"},{"location":"ref/BlockCode/#coset_leaders","title":"<code>coset_leaders()</code>  <code>cached</code>","text":"<p>Returns the coset leaders of the code. This is a $2^m \\times n$ matrix whose rows are all the coset leaders. The coset leader in row $i$ corresponds to the syndrome obtained by expressing $i$ in binary with $m$ bits (LSB-first), and whose Hamming weight is minimal. This may be used as a LUT for syndrome-based decoding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.coset_leaders()\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1],\n       [1, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0]])\n</code></pre>"},{"location":"ref/BlockCode/#coset_leader_weight_distribution","title":"<code>coset_leader_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the coset leader weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of coset leaders of weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([1, 5, 2, 0, 0, 0])\n</code></pre>"},{"location":"ref/BlockCode/#packing_radius","title":"<code>packing_radius()</code>  <code>cached</code>","text":"<p>Returns the packing radius of the code. This is also called the error-correcting capability of the code, and is equal to $\\lfloor (d - 1) / 2 \\rfloor$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.packing_radius()\n1\n</code></pre>"},{"location":"ref/BlockCode/#covering_radius","title":"<code>covering_radius()</code>  <code>cached</code>","text":"<p>Returns the covering radius of the code. This is equal to the maximum Hamming weight of the coset leaders.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[\n...     [1, 0, 0, 1, 1],\n...     [0, 1, 1, 1, 0],\n... ])\n&gt;&gt;&gt; code.covering_radius()\n2\n</code></pre>"},{"location":"ref/ComplexSequence/","title":"komm.ComplexSequence","text":"<p>General complex sequence. It is denoted by $x[n]$, with elements in $\\mathbb{C}$. Its length (or period) is denoted by $L$.</p> <p>Parameters:</p> <ul> <li> <code>sequence</code> (<code>ArrayLike</code>)         \u2013          <p>The complex sequence. Must be a 1D-array of length $L$ with elements in $\\mathbb{C}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = ComplexSequence([1, 1j, -1, -1j])\n&gt;&gt;&gt; seq.sequence\narray([ 1.+0.j,  0.+1.j, -1.+0.j, -0.-1.j])\n</code></pre>"},{"location":"ref/ComplexSequence/#length","title":"<code>length</code>  <code>cached</code> <code>property</code>","text":"<p>The length (or period) $L$ of the complex sequence.</p>"},{"location":"ref/ComplexSequence/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>Returns the autocorrelation $R[\\ell]$ of the complex sequence. See <code>komm.autocorrelation</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>See the corresponding parameter in <code>komm.autocorrelation</code>.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>See the corresponding parameter in <code>komm.autocorrelation</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[complexfloating]</code>         \u2013          <p>The autocorrelation $R[\\ell]$ of the complex sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = ComplexSequence([1, 1j, -1, -1j])\n&gt;&gt;&gt; seq.autocorrelation(shifts=[-2, -1, 0, 1, 2])\narray([-2.+0.j,  0.-3.j,  4.+0.j,  0.+3.j, -2.+0.j])\n</code></pre>"},{"location":"ref/ComplexSequence/#cyclic_autocorrelation","title":"<code>cyclic_autocorrelation()</code>","text":"<p>Returns the cyclic autocorrelation $\\tilde{R}[\\ell]$ of the complex sequence. See <code>komm.cyclic_autocorrelation</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>See the corresponding parameter in <code>komm.cyclic_autocorrelation</code>.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>See the corresponding parameter in <code>komm.cyclic_autocorrelation</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[complexfloating]</code>         \u2013          <p>The cyclic autocorrelation $\\tilde{R}[\\ell]$ of the complex sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = ComplexSequence([1, 1j, -1, -1j])\n&gt;&gt;&gt; seq.cyclic_autocorrelation(shifts=[-2, -1, 0, 1, 2])\narray([-4.+0.j,  0.-4.j,  4.+0.j,  0.+4.j, -4.+0.j])\n</code></pre>"},{"location":"ref/Constellation/","title":"komm.Constellation","text":"<p>General real or complex constellation. A constellation of dimension $N$ and order $M$ is defined by an ordered set $\\{ \\mathbf{x}_i : i \\in [0:M) \\}$ of $M$ distinct points in $\\mathbb{R}^N$ or $\\mathbb{C}^N$, called symbols. In this class, the constellation is represented by a matrix $\\mathbf{X} \\in \\mathbb{R}^{M \\times N}$ or $\\mathbf{X} \\in \\mathbb{C}^{M \\times N}$, where the $i$-th row of $\\mathbf{X}$ corresponds to symbol $\\mathbf{x}_i$. For more details, see SA15, Sec. 2.5.1.</p> <p>Parameters:</p> <ul> <li> <code>matrix</code> (<code>ArrayLike</code>)         \u2013          <p>The constellation matrix $\\mathbf{X}$. Must be a 2D-array of shape $(M, N)$ with real or complex entries.</p> </li> </ul> <p>Examples:</p> <p>The real constellation depicted in the figure below has $M = 5$ and $N = 2$.</p> <p></p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n</code></pre>"},{"location":"ref/Constellation/#matrix","title":"<code>matrix</code><code>  Array2D[T] </code>  <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.matrix\narray([[ 0.,  4.],\n       [-2.,  2.],\n       [ 2.,  2.],\n       [ 1.,  1.],\n       [ 0., -2.]])\n</code></pre>"},{"location":"ref/Constellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.order\n5\n</code></pre>"},{"location":"ref/Constellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.dimension\n2\n</code></pre>"},{"location":"ref/Constellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[T]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.mean()\narray([0.2, 1.4])\n</code></pre>"},{"location":"ref/Constellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(7.6)\n</code></pre>"},{"location":"ref/Constellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(1.4142135623730951)\n</code></pre>"},{"location":"ref/Constellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[T]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([1., 1., 0., 4.])\n&gt;&gt;&gt; const.indices_to_symbols([[3, 0], [1, 2]])\narray([[ 1.,  1.,  0.,  4.],\n       [-2.,  2.,  2.,  2.]])\n</code></pre>"},{"location":"ref/Constellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.closest_indices([0.3, 1.8, 0.0, 5.0])\narray([3, 0])\n&gt;&gt;&gt; const.closest_indices([[0.3, 1.8], [0.0, 5.0]])\narray([[3],\n       [0]])\n</code></pre>"},{"location":"ref/Constellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[T]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.closest_symbols([0.3, 1.8, 0.0, 5.0])\narray([1., 1., 0., 4.])\n&gt;&gt;&gt; const.closest_symbols([[0.3, 1.8], [0.0, 5.0]])\narray([[1., 1.], [0., 4.]])\n</code></pre>"},{"location":"ref/Constellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.Constellation([[0, 4], [-2, 2], [2, 2], [1, 1], [0, -2]])\n&gt;&gt;&gt; const.posteriors([0.3, 1.8, 0.0, 5.0], snr=2.0).round(4)\narray([0.1565, 0.1408, 0.2649, 0.4253, 0.0125,\n       0.9092, 0.0387, 0.0387, 0.0135, 0.    ])\n&gt;&gt;&gt; const.posteriors([[0.3, 1.8], [0.0, 5.0]], snr=2.0).round(4)\narray([[0.1565, 0.1408, 0.2649, 0.4253, 0.0125],\n       [0.9092, 0.0387, 0.0387, 0.0135, 0.    ]])\n</code></pre>"},{"location":"ref/ConvolutionalCode/","title":"komm.ConvolutionalCode","text":"<p>Binary convolutional encoder. It is characterized by a matrix of feedforward polynomials $P(D)$, of shape $k \\times n$, and (optionally) by a vector of feedback polynomials $q(D)$, of length $k$. The parameters $k$ and $n$ are the number of input and output bits per block, respectively. In this class, the encoder is realized in controllable canonical form. For more details, see McE98, JZ15, and LC04, Chs. 11, 12.</p> <p>Parameters:</p> <ul> <li> <code>feedforward_polynomials</code> (<code>ArrayLike</code>)         \u2013          <p>The matrix of feedforward polynomials $P(D)$, which is a $k \\times n$ matrix whose entries are either binary polynomials or integers to be converted to the former.</p> </li> <li> <code>feedback_polynomials</code> (<code>ArrayLike | None</code>)         \u2013          <p>The vector of feedback polynomials $q(D)$, which is a $k$-vector whose entries are either binary polynomials or integers to be converted to the former. The default value corresponds to no feedback, that is, $q_i(D) = 1$ for all $i \\in [0 : k)$.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>Consider the encoder with parameters $(n, k, \\sigma) = (3, 2, 7)$ depicted below.</p> <p> </p> <p>Its matrix of feedforward polynomials is given by $$     P(D) =     \\begin{bmatrix}         D^4 + D^3 + 1  &amp;  D^4 + D^2 + D + 1  &amp;  0 \\\\         0  &amp;  D^3 + D  &amp;  D^3 + D^2 + 1     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode(\n...     feedforward_polynomials=[\n...         [0b11001, 0b10111,      0],\n...         [      0,  0b1010, 0b1101],\n...     ],\n... )\n&gt;&gt;&gt; code = komm.ConvolutionalCode(\n...     feedforward_polynomials=[[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]],\n... )\n</code></pre> </li> <li> <p>Consider the feedback encoder with parameters $(n, k, \\sigma) = (2, 1, 4)$ depicted below.</p> <p> </p> <p>Its matrix of feedforward polynomials is given by $$     P(D) =     \\begin{bmatrix}         D^4 + D^2 + D + 1 &amp;&amp; D^4 + D^3 + 1     \\end{bmatrix}, $$ and its vector of feedback polynomials is given by $$     q(D) =     \\begin{bmatrix}         D^4 + D^2 + D + 1     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode(\n...     feedforward_polynomials=[[0b10111, 0b11001]],\n...     feedback_polynomials=[0b10111],\n... )\n&gt;&gt;&gt; code = komm.ConvolutionalCode(\n...     feedforward_polynomials=[[0o27, 0o31]],\n...     feedback_polynomials=[0o27],\n... )\n</code></pre> </li> </ol>"},{"location":"ref/ConvolutionalCode/#num_input_bits","title":"<code>num_input_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of input bits per block, $k$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.num_input_bits\n2\n</code></pre>"},{"location":"ref/ConvolutionalCode/#num_output_bits","title":"<code>num_output_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of output bits per block, $n$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.num_output_bits\n3\n</code></pre>"},{"location":"ref/ConvolutionalCode/#degree","title":"<code>degree</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The degree $\\sigma$ of the encoder. This corresponds to the number of delay elements in the encoder realization.</p> <p>For a convolutional encoder realized in controllable canonical form, the degree $\\sigma$ is equal to the overall constraint length $\\nu$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.degree\n7\n</code></pre>"},{"location":"ref/ConvolutionalCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[object_] </code>  <code>cached</code> <code>property</code>","text":"<p>Returns the (transform-domain) generator matrix (also known as transfer function matrix) $G(D)$ of the encoder. This is a $k \\times n$ array of binary polynomial fractions.</p> <p>For a convolutional code with matrix of feedforward polynomials $$     P(D) =     \\begin{bmatrix}         p_{0,0}(D)   &amp; p_{0,1}(D)   &amp; \\cdots &amp; p_{0,n-1}(D)   \\\\         p_{1,0}(D)   &amp; p_{1,1}(D)   &amp; \\cdots &amp; p_{1,n-1}(D)   \\\\         \\vdots       &amp; \\vdots       &amp; \\ddots &amp; \\vdots         \\\\         p_{k-1,0}(D) &amp; p_{k-1,1}(D) &amp; \\cdots &amp; p_{k-1,n-1}(D)     \\end{bmatrix}, $$ and vector of feedback polynomials $$     q(D) =     \\begin{bmatrix}         q_0(D)     \\\\         q_1(D)     \\\\         \\vdots     \\\\         q_{k-1}(D)     \\end{bmatrix}, $$ the generator matrix is given by $$     G(D) =     \\begin{bmatrix}         p_{0,0}(D)/q_0(D)       &amp; p_{0,1}(D)/q_0(D)       &amp; \\cdots &amp; p_{0,n-1}(D)/q_0(D)       \\\\         p_{1,0}(D)/q_1(D)       &amp; p_{1,1}(D)/q_1(D)       &amp; \\cdots &amp; p_{1,n-1}(D)/q_1(D)       \\\\         \\vdots                  &amp; \\vdots                  &amp; \\ddots &amp; \\vdots                    \\\\         p_{k-1,0}(D)/q_{k-1}(D) &amp; p_{k-1,1}(D)/q_{k-1}(D) &amp; \\cdots &amp; p_{k-1,n-1}(D)/q_{k-1}(D)     \\end{bmatrix}. $$</p> <p>Examples:</p> <p>If matrix of feedforward polynomials is $$     P(D) =     \\begin{bmatrix}         D^4 + D^2 + D + 1 &amp;&amp; D^4 + D^3 + 1     \\end{bmatrix} $$ and vector of feedback polynomials is $$     q(D) =     \\begin{bmatrix}         D^4 + D^2 + D + 1     \\end{bmatrix}, $$ then the generator matrix is given by $$     G(D) =     \\begin{bmatrix}         1 &amp; \\frac{D^4 + D^3 + 1}{D^4 + D^2 + D + 1}     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o27, 0o31]], [0o27])\n&gt;&gt;&gt; for row in code.generator_matrix:\n...     print(\"[\" + \", \".join(str(x) for x in row) + \"]\")\n[0b1/0b1, 0b11001/0b10111]\n</code></pre>"},{"location":"ref/ConvolutionalCode/#constraint_lengths","title":"<code>constraint_lengths</code><code>  Array1D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The constraint lengths $\\nu_i$ of the encoder, for $i \\in [0 : k)$. These are defined by $$     \\nu_i = \\max \\{ \\deg p_{i,0}(D), \\deg p_{i,1}(D), \\ldots, \\deg p_{i,n-1}(D), \\deg q_i(D) \\}, $$ where $p_{i,j}(D) / q_i(D)$ are the entries of the $i$-th row of the generator matrix $G(D)$, satisfying $$     \\gcd(p_{i,0}(D), p_{i,1}(D), \\ldots, p_{i,n-1}(D), q_i(D)) = 1. $$ This is a $k$-array of integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.constraint_lengths\narray([4, 3])\n</code></pre>"},{"location":"ref/ConvolutionalCode/#overall_constraint_length","title":"<code>overall_constraint_length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The overall constraint length $\\nu$ of the encoder, defined by $$     \\nu = \\sum_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.overall_constraint_length\n7\n</code></pre>"},{"location":"ref/ConvolutionalCode/#memory_order","title":"<code>memory_order</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The memory order $\\mu$ of the encoder. It is given by $$     \\mu = \\max_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.memory_order\n4\n</code></pre>"},{"location":"ref/ConvolutionalCode/#state_space_representation","title":"<code>state_space_representation()</code>  <code>cached</code>","text":"<p>Returns the matrices $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ corresponding to the state-space representation of the encoder realization. The state-space representation of the encoder is given by $$ \\begin{aligned}     s_{t+1} &amp; = s_t \\mathbf{A} + u_t \\mathbf{B}, \\\\     v_t &amp; = s_t \\mathbf{C} + u_t \\mathbf{D}, \\end{aligned} $$ where</p> <ul> <li>$u_t \\in \\mathbb{B}^k$ is the input block,</li> <li>$v_t \\in \\mathbb{B}^n$ is the output block,</li> <li>$s_t \\in \\mathbb{B}^\\sigma$ is the state,</li> <li>$\\mathbf{A} \\in \\mathbb{B}^{\\sigma \\times \\sigma}$ is the state matrix,</li> <li>$\\mathbf{B} \\in \\mathbb{B}^{k \\times \\sigma}$ is the control matrix,</li> <li>$\\mathbf{C} \\in \\mathbb{B}^{\\sigma \\times n}$ is the observation matrix,</li> <li>$\\mathbf{D} \\in \\mathbb{B}^{k \\times n}$ is the transition matrix.</li> </ul> <p>Returns:</p> <ul> <li> <code>state_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The state matrix $\\mathbf{A}$ of the encoder.</p> </li> <li> <code>control_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The control matrix $\\mathbf{B}$ of the encoder.</p> </li> <li> <code>observation_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The observation matrix $\\mathbf{C}$ of the encoder.</p> </li> <li> <code>transition_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The transition matrix $\\mathbf{D}$ of the encoder.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; A_mat, B_mat, C_mat, D_mat = code.state_space_representation()\n&gt;&gt;&gt; A_mat\narray([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0]])\n&gt;&gt;&gt; B_mat\narray([[1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0]])\n&gt;&gt;&gt; C_mat\narray([[0, 1, 0],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 1, 0],\n       [0, 1, 0],\n       [0, 0, 1],\n       [0, 1, 1]])\n&gt;&gt;&gt; D_mat\narray([[1, 1, 0],\n       [0, 0, 1]])\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o27, 0o31]], [0o27])\n&gt;&gt;&gt; A_mat, B_mat, C_mat, D_mat = code.state_space_representation()\n&gt;&gt;&gt; A_mat\narray([[1, 1, 0, 0],\n       [1, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0]])\n&gt;&gt;&gt; B_mat\narray([[1, 0, 0, 0]])\n&gt;&gt;&gt; C_mat\narray([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 0]])\n&gt;&gt;&gt; D_mat\narray([[1, 1]])\n</code></pre>"},{"location":"ref/ConvolutionalCode/#finite_state_machine","title":"<code>finite_state_machine()</code>  <code>cached</code>","text":"<p>Returns the finite-state (Mealy) machine of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0b111, 0b101]])\n&gt;&gt;&gt; code.finite_state_machine()\nMealyMachine(transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n             outputs=[[0, 3], [1, 2], [3, 0], [2, 1]])\n</code></pre>"},{"location":"ref/ConvolutionalCode/#is_catastrophic","title":"<code>is_catastrophic()</code>  <code>cached</code>","text":"<p>Returns whether the encoder is catastrophic. A convolutional encoder is catastrophic if there exists an infinite-weight input sequence that generates a finite-weight output sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0b111, 0b101]])\n&gt;&gt;&gt; code.is_catastrophic()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0b11, 0b101]])\n&gt;&gt;&gt; code.is_catastrophic()\nTrue\n</code></pre>"},{"location":"ref/ConvolutionalCode/#free_distance","title":"<code>free_distance()</code>  <code>cached</code>","text":"<p>Returns the free distance $d_\\mathrm{free}$ of the code. This is equal to the minimum Hamming weight among all possible non-zero output sequences.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0o31, 0o27, 0o0], [0o0, 0o12, 0o15]])\n&gt;&gt;&gt; code.free_distance()\n5\n</code></pre>"},{"location":"ref/ConvolutionalCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a given bit sequence, starting from the all-zero state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0b111, 0b101]])\n&gt;&gt;&gt; code.encode([1, 1, 1, 1])\narray([1, 1, 0, 1, 1, 0, 1, 0])\n</code></pre>"},{"location":"ref/ConvolutionalCode/#encode_with_state","title":"<code>encode_with_state()</code>","text":"<p>Encodes a given bit sequence, starting from a given state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> <li> <code>initial_state</code> (<code>ArrayLike</code>)         \u2013          <p>The initial state. Must be a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> <li> <code>final_state</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The final state. It is a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ConvolutionalCode([[0b111, 0b101]])\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1], [0, 0])\n(array([1, 1, 0, 1, 1, 0, 1, 0]), array([1, 1]))\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1], [1, 1])\n(array([1, 0, 1, 0, 1, 0, 1, 0]), array([1, 1]))\n</code></pre>"},{"location":"ref/CordaroWagnerCode/","title":"komm.CordaroWagnerCode","text":"<p>Cordaro\u2013Wagner code. For a given length $n \\geq 2$, it is the linear block code with dimension $k = 2$ which is optimum for the BSC with sufficiently small crossover probability. For more details, see CW67.</p> <ul> <li>Length: $n$</li> <li>Dimension: $k = 2$</li> <li>Redundancy: $m = n - 2$</li> <li>Minimum distance: $d = \\left\\lceil 2n / 3 \\right\\rceil - 1$</li> </ul> <p>Parameters:</p> <ul> <li> <code>n</code> (<code>int</code>)         \u2013          <p>The length $n$ of the code. Must satisfy $n \\geq 2$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CordaroWagnerCode(11)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(11, 2, 9)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n7\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0])\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([  1,  11,  55, 165, 226,  54,   0,   0,   0,   0,   0,   0])\n</code></pre>"},{"location":"ref/CyclicCode/","title":"komm.CyclicCode","text":"<p>General binary cyclic code. A cyclic code is a linear block code such that, if $c$ is a codeword, then every cyclic shift of $c$ is also a codeword. It is characterized by its generator polynomial $g(X)$, of degree $m$ (the redundancy of the code), and by its check polynomial $h(X)$, of degree $k$ (the dimension of the code). Those polynomials are related by $g(X) h(X) = X^n + 1$, where $n = k + m$ is the length of the code.</p> <p>Examples of generator polynomials can be found in the table below.</p> Code $(n, k, d)$ Generator polynomial $g(X)$ Integer representation Hamming $(7,4,3)$ $X^3 + X + 1$ <code>0b1011 = 0o13 = 11</code> Simplex $(7,3,4)$ $X^4 + X^2 + X +   1$ <code>0b10111 = 0o27 = 23</code> BCH $(15,5,7)$ $X^{10} + X^8 + X^5 + X^4 + X^2 + X + 1$ <code>0b10100110111 = 0o2467 = 1335</code> Golay $(23,12,7)$ $X^{11} + X^9 + X^7 + X^6 + X^5 + X + 1$ <code>0b101011100011 = 0o5343 = 2787</code> <p>For more details, see LC04, Ch. 5 and McE04, Ch. 8.</p> <p>The constructor expects either the generator polynomial or the check polynomial.</p> <p>Parameters:</p> <ul> <li> <code>length</code> (<code>int</code>)         \u2013          <p>The length $n$ of the code.</p> </li> <li> <code>generator_polynomial</code> (<code>SupportsInt | None</code>)         \u2013          <p>The generator polynomial $g(X)$ of the code, of degree $m$ (the redundancy of the code), specified either as a binary polynomial or as an integer to be converted to the former.</p> </li> <li> <code>check_polynomial</code> (<code>SupportsInt | None</code>)         \u2013          <p>The check polynomial $h(X)$ of the code, of degree $k$ (the dimension of the code), specified either as a binary polynomial or as an integer to be converted to the former.</p> </li> <li> <code>systematic</code> (<code>bool</code>)         \u2013          <p>Whether the encoder is systematic. Default is <code>True</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(7, 4, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, check_polynomial=0b10111)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(7, 4, 3)\n</code></pre>"},{"location":"ref/CyclicCode/#length","title":"<code>length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The length $n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.length\n7\n</code></pre>"},{"location":"ref/CyclicCode/#dimension","title":"<code>dimension</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The dimension $k$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.dimension\n4\n</code></pre>"},{"location":"ref/CyclicCode/#redundancy","title":"<code>redundancy</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The redundancy $m$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.redundancy\n3\n</code></pre>"},{"location":"ref/CyclicCode/#rate","title":"<code>rate</code><code>  float </code>  <code>cached</code> <code>property</code>","text":"<p>The rate $R = k/n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.rate\n0.5714285714285714\n</code></pre>"},{"location":"ref/CyclicCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The generator matrix $G \\in \\mathbb{B}^{k \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 0, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 0],\n       [1, 1, 1, 0, 0, 1, 0],\n       [1, 0, 1, 0, 0, 0, 1]])\n</code></pre>"},{"location":"ref/CyclicCode/#check_matrix","title":"<code>check_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The check matrix $H \\in \\mathbb{B}^{m \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.check_matrix\narray([[1, 0, 0, 1, 0, 1, 1],\n       [0, 1, 0, 1, 1, 1, 0],\n       [0, 0, 1, 0, 1, 1, 1]])\n</code></pre>"},{"location":"ref/CyclicCode/#encode","title":"<code>encode()</code>","text":"<p>Applies the encoding mapping $\\Enc : \\mathbb{B}^k \\to \\mathbb{B}^n$ of the code. This method takes one or more sequences of messages and returns their corresponding codeword sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $k$, or a multidimensional array where the last dimension is a multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension expanded from $bk$ to $bn$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.encode</code> for examples. </p>"},{"location":"ref/CyclicCode/#inverse_encode","title":"<code>inverse_encode()</code>","text":"<p>Applies the inverse encoding partial mapping $\\Enc^{-1} : \\mathbb{B}^n \\rightharpoonup \\mathbb{B}^k$ of the code. This method takes one or more sequences of codewords and returns their corresponding message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input contains any invalid codewords.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.inverse_encode</code> for examples. </p>"},{"location":"ref/CyclicCode/#check","title":"<code>check()</code>","text":"<p>Applies the check mapping $\\mathrm{Chk} : \\mathbb{B}^n \\to \\mathbb{B}^m$ of the code. This method takes one or more sequences of received words and returns their corresponding syndrome sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bm$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.check</code> for examples. </p>"},{"location":"ref/CyclicCode/#codewords","title":"<code>codewords()</code>  <code>cached</code>","text":"<p>Returns the codewords of the code. This is a $2^k \\times n$ matrix whose rows are all the codewords. The codeword in row $i$ corresponds to the message obtained by expressing $i$ in binary with $k$ bits (LSB-first).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.codewords()\narray([[0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 0, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0, 0],\n       [1, 0, 1, 1, 1, 0, 0],\n       [1, 1, 1, 0, 0, 1, 0],\n       [0, 0, 1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1, 1, 0],\n       [0, 1, 0, 1, 1, 1, 0],\n       [1, 0, 1, 0, 0, 0, 1],\n       [0, 1, 1, 1, 0, 0, 1],\n       [1, 1, 0, 0, 1, 0, 1],\n       [0, 0, 0, 1, 1, 0, 1],\n       [0, 1, 0, 0, 0, 1, 1],\n       [1, 0, 0, 1, 0, 1, 1],\n       [0, 0, 1, 0, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]])\n</code></pre>"},{"location":"ref/CyclicCode/#codeword_weight_distribution","title":"<code>codeword_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the codeword weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of codewords of Hamming weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([1, 0, 0, 7, 7, 0, 0, 1])\n</code></pre>"},{"location":"ref/CyclicCode/#minimum_distance","title":"<code>minimum_distance()</code>  <code>cached</code>","text":"<p>Returns the minimum distance $d$ of the code. This is equal to the minimum Hamming weight of the non-zero codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre>"},{"location":"ref/CyclicCode/#coset_leaders","title":"<code>coset_leaders()</code>  <code>cached</code>","text":"<p>Returns the coset leaders of the code. This is a $2^m \\times n$ matrix whose rows are all the coset leaders. The coset leader in row $i$ corresponds to the syndrome obtained by expressing $i$ in binary with $m$ bits (LSB-first), and whose Hamming weight is minimal. This may be used as a LUT for syndrome-based decoding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.coset_leaders()\narray([[0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0]])\n</code></pre>"},{"location":"ref/CyclicCode/#coset_leader_weight_distribution","title":"<code>coset_leader_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the coset leader weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of coset leaders of weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([1, 7, 0, 0, 0, 0, 0, 0])\n</code></pre>"},{"location":"ref/CyclicCode/#packing_radius","title":"<code>packing_radius()</code>  <code>cached</code>","text":"<p>Returns the packing radius of the code. This is also called the error-correcting capability of the code, and is equal to $\\lfloor (d - 1) / 2 \\rfloor$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.packing_radius()\n1\n</code></pre>"},{"location":"ref/CyclicCode/#covering_radius","title":"<code>covering_radius()</code>  <code>cached</code>","text":"<p>Returns the covering radius of the code. This is equal to the maximum Hamming weight of the coset leaders.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.CyclicCode(length=7, generator_polynomial=0b1011)\n&gt;&gt;&gt; code.covering_radius()\n1\n</code></pre>"},{"location":"ref/CyclicRedundancyCheck/","title":"komm.CyclicRedundancyCheck","text":"<p>Cyclic redundancy check (CRC) [Not implemented yet].</p>"},{"location":"ref/DiscreteMemorylessChannel/","title":"komm.DiscreteMemorylessChannel","text":"<p>General discrete memoryless channel (DMC). It is defined by an input alphabet $\\mathcal{X}$, an output alphabet $\\mathcal{Y}$, and a transition probability matrix $p_{Y \\mid X}$. Here, for simplicity, the input and output alphabets are always taken as $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, respectively. The transition probability matrix $p_{Y \\mid X}$, of size $|\\mathcal{X}|$-by-$|\\mathcal{Y}|$, gives the conditional probability of receiving $Y = y$ given that $X = x$ is transmitted. For more details, see CT06, Ch. 7.</p> <p>Parameters:</p> <ul> <li> <code>transition_matrix</code> (<code>ArrayLike</code>)         \u2013          <p>The channel transition probability matrix $p_{Y \\mid X}$. The element in row $x \\in \\mathcal{X}$ and column $y \\in \\mathcal{Y}$ must be equal to $p_{Y \\mid X}(y \\mid x)$.</p> </li> </ul>"},{"location":"ref/DiscreteMemorylessChannel/#input_cardinality","title":"<code>input_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel input cardinality $|\\mathcal{X}|$.</p>"},{"location":"ref/DiscreteMemorylessChannel/#output_cardinality","title":"<code>output_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel output cardinality $|\\mathcal{Y}|$.</p>"},{"location":"ref/DiscreteMemorylessChannel/#transition_matrix","title":"<code>transition_matrix</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The channel transition probability matrix $p_{Y \\mid X}$.</p>"},{"location":"ref/DiscreteMemorylessChannel/#mutual_information","title":"<code>mutual_information()</code>","text":"<p>Returns the mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$ of the channel.</p> <p>Parameters:</p> <ul> <li> <code>input_pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p_X$ of the channel input $X$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$.</p> </li> </ul> <p>The mutual information is given by $$     \\mathrm{I}(X; Y) = \\mathrm{H}(X) - \\mathrm{H}(X \\mid Y), $$ where $\\mathrm{H}(X)$ is the the entropy of $X$ and $\\mathrm{H}(X \\mid Y)$ is the conditional entropy of $X$ given $Y$. By default, the base of the logarithm is $2$, in which case the mutual information is measured in bits. See CT06, Ch. 2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dmc = komm.DiscreteMemorylessChannel([\n...     [0.6, 0.3, 0.1],\n...     [0.7, 0.1, 0.2],\n...     [0.5, 0.05, 0.45],\n... ])\n&gt;&gt;&gt; dmc.mutual_information([1/3, 1/3, 1/3])\nnp.float64(0.12381109879798724)\n&gt;&gt;&gt; dmc.mutual_information([1/3, 1/3, 1/3], base=3)\nnp.float64(0.07811610605402552)\n&gt;&gt;&gt; dmc.mutual_information([1/3, 1/3, 1/3], base='e')\nnp.float64(0.08581931405385379)\n</code></pre>"},{"location":"ref/DiscreteMemorylessChannel/#capacity","title":"<code>capacity()</code>","text":"<p>Returns the channel capacity $C$.</p> <p>Parameters:</p> <ul> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The channel capacity $C$.</p> </li> </ul> <p>The channel capacity is given by $$     C = \\max_{p_X} \\mathrm{I}(X; Y). $$ This method computes the channel capacity via the Arimoto\u2013Blahut algorithm. See CT06, Sec. 10.8.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dmc = komm.DiscreteMemorylessChannel([\n...     [0.6, 0.3, 0.1],\n...     [0.7, 0.1, 0.2],\n...     [0.5, 0.05, 0.45],\n... ])\n&gt;&gt;&gt; dmc.capacity()\nnp.float64(0.1616318609548566)\n&gt;&gt;&gt; dmc.capacity(base=3)\nnp.float64(0.10197835020154389)\n&gt;&gt;&gt; dmc.capacity(base='e')\nnp.float64(0.11203466870951606)\n</code></pre>"},{"location":"ref/DiscreteMemorylessChannel/#transmit","title":"<code>transmit()</code>","text":"<p>Transmits the input sequence through the channel and returns the output sequence.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dmc = komm.DiscreteMemorylessChannel([\n...     [0.9, 0.05, 0.05],\n...     [0.0, 0.5, 0.5],\n... ])\n&gt;&gt;&gt; dmc.transmit([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\narray([2, 1, 2, 0, 0, 2, 2, 0, 1, 0])\n</code></pre>"},{"location":"ref/DiscreteMemorylessSource/","title":"komm.DiscreteMemorylessSource","text":"<p>Discrete memoryless source. It is defined by a finite source alphabet $\\mathcal{X}$ and a pmf $p$ over $\\mathcal{X}$. The value of $p(x)$ gives the probability of the source emitting symbol $x \\in \\mathcal{X}$, with the probability of emitting a symbol being independent of all previously emitted symbols. Here, for simplicity, the alphabet is taken as $\\mathcal{X} = [0 : |\\mathcal{X}|)$, where $|\\mathcal{X}|$ is called the source cardinality.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike | int</code>)         \u2013          <p>Either a one-dimensional array of floats representing the source pmf $p$, or a positive integer $M$, in which case a uniform pmf over $[0 : M)$ is assumed.</p> </li> </ul> Note <p>The cardinality $|\\mathcal{X}|$ is inferred from the length of the <code>pmf</code> array.</p>"},{"location":"ref/DiscreteMemorylessSource/#pmf","title":"<code>pmf</code><code>  Array1D[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The source pmf $p$ over $\\mathcal{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource([1/2, 1/4, 1/8, 1/8])\n&gt;&gt;&gt; source.pmf\narray([0.5  , 0.25 , 0.125, 0.125])\n</code></pre> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource(4)\n&gt;&gt;&gt; source.pmf\narray([0.25, 0.25, 0.25, 0.25])\n</code></pre>"},{"location":"ref/DiscreteMemorylessSource/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The source cardinality $|\\mathcal{X}|$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource([1/2, 1/4, 1/8, 1/8])\n&gt;&gt;&gt; source.cardinality\n4\n</code></pre> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource(4)\n&gt;&gt;&gt; source.cardinality\n4\n</code></pre>"},{"location":"ref/DiscreteMemorylessSource/#entropy_rate","title":"<code>entropy_rate()</code>","text":"<p>Computes the source entropy rate. For a discrete memoryless source, this is simply the entropy of the pmf $p$.</p> <p>Parameters:</p> <ul> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>See <code>komm.entropy</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>entropy_rate</code>  (<code>floating</code>)          \u2013          <p>The source entropy rate.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource([1/2, 1/4, 1/8, 1/8])\n&gt;&gt;&gt; source.entropy_rate()\nnp.float64(1.75)\n&gt;&gt;&gt; source.entropy_rate(base=4)\nnp.float64(0.875)\n</code></pre> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource(4)\n&gt;&gt;&gt; source.entropy_rate()\nnp.float64(2.0)\n&gt;&gt;&gt; source.entropy_rate(base=4)\nnp.float64(1.0)\n</code></pre>"},{"location":"ref/DiscreteMemorylessSource/#emit","title":"<code>emit()</code>","text":"<p>Returns random symbols from the source.</p> <p>Parameters:</p> <ul> <li> <code>shape</code> (<code>tuple[int, ...] | int | None</code>)         \u2013          <p>The shape of the output array. The default value corresponds to a single symbol.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The emitted symbols from the source. It is an array with elements in $\\mathcal{X}$ and the given shape.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; source = komm.DiscreteMemorylessSource([1/2, 1/4, 1/8, 1/8])\n&gt;&gt;&gt; source.emit()\narray([2])\n&gt;&gt;&gt; source.emit(10)\narray([0, 2, 1, 0, 3, 2, 2, 0, 0, 0])\n&gt;&gt;&gt; source.emit((2, 5))\narray([[3, 1, 2, 0, 0],\n       [1, 0, 2, 1, 2]])\n</code></pre>"},{"location":"ref/ExhaustiveSearchDecoder/","title":"komm.ExhaustiveSearchDecoder","text":"<p>Exhaustive search decoder for general block codes. This decoder implements a brute-force search over all possible codewords to find the one that is closest (in terms of Hamming distance, for hard-decision decoding, or Euclidean distance, for soft-decision decoding) to the received word.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>BlockCode</code>)         \u2013          <p>The block code to be used for decoding.</p> </li> <li> <code>input_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the input. Either <code>'hard'</code> or <code>'soft'</code>. Default is <code>'hard'</code>.</p> </li> </ul> Notes <ul> <li>Input type: <code>hard</code> (bits) or <code>soft</code> (either L-values or channel outputs).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/ExhaustiveSearchDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HammingCode(3)\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ExhaustiveSearchDecoder(code, input_type=\"hard\")\n&gt;&gt;&gt; decoder.decode([[1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0]])\narray([[1, 1, 0, 0],\n       [1, 0, 1, 1]])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ExhaustiveSearchDecoder(code, input_type=\"soft\")\n&gt;&gt;&gt; decoder.decode([-1.3, -0.8, +1.1, -0.8, +1.2, -0.2, -1.4])\narray([1, 1, 0, 0])\n</code></pre>"},{"location":"ref/FanoCode/","title":"komm.FanoCode","text":"<p>Binary Fano code. For a given pmf $p$ over $\\mathcal{X}$, it is a fixed-to-variable length code in which the source words are first sorted in descending order of probability and then are recursively partitioned into two groups of approximately equal total probability, assigning bit $\\mathtt{0}$ to one group and bit $\\mathtt{1}$ to the other, until each source word is assigned a unique codeword. For more details, see Wikipedia: Shannon\u2013Fano coding.</p> Notes <p>Fano codes are always prefix-free (hence uniquely decodable).</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> <li> <code>source_block_size</code> (<code>int</code>)         \u2013          <p>The source block size $k$. The default value is $k = 1$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pmf = [0.8, 0.1, 0.1]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FanoCode(pmf)\n&gt;&gt;&gt; code.enc_mapping\n{(0,): (0,),\n (1,): (1, 0),\n (2,): (1, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(1.2)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FanoCode(pmf, 2)\n&gt;&gt;&gt; code.enc_mapping\n{(0, 0): (0,),\n (0, 1): (1, 0, 0),\n (0, 2): (1, 0, 1),\n (1, 0): (1, 1, 0),\n (1, 1): (1, 1, 1, 1, 0, 0),\n (1, 2): (1, 1, 1, 1, 0, 1),\n (2, 0): (1, 1, 1, 0),\n (2, 1): (1, 1, 1, 1, 1, 0),\n (2, 2): (1, 1, 1, 1, 1, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(0.96)\n</code></pre>"},{"location":"ref/FibonacciCode/","title":"komm.FibonacciCode","text":"<p>Fibonacci code. It is an integer code. For the definition of this code, see Wikipedia: Fibonacci coding.</p>"},{"location":"ref/FibonacciCode/#encode","title":"<code>encode()</code>","text":"<p>Encode the input integer array.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input integer array. It must be a one-dimensional array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of bits corresponding to the input integer array.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FibonacciCode()\n&gt;&gt;&gt; code.encode([4, 1, 3])\narray([1, 0, 1, 1, 1, 1, 0, 0, 1, 1])\n</code></pre>"},{"location":"ref/FibonacciCode/#decode","title":"<code>decode()</code>","text":"<p>Decode the input bit array.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input bit array. It must be a one-dimensional array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of integers corresponding to the input bit array.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FibonacciCode()\n&gt;&gt;&gt; code.decode([1, 0, 1, 1, 1, 1, 0, 0, 1, 1])\narray([4, 1, 3])\n</code></pre>"},{"location":"ref/FiniteBifield/","title":"komm.FiniteBifield","text":"<p>Finite field with binary characteristic. Objects of this class represent a finite field $\\mathrm{GF}(2^k)$ (also known as Galois field), with characteristic $2$ and degree $k$.</p> <p>Parameters:</p> <ul> <li> <code>degree</code> (<code>int</code>)         \u2013          <p>Degree $k$ of the finite field. Must be a positive integer.</p> </li> <li> <code>modulus</code> (<code>BinaryPolynomial | int | None</code>)         \u2013          <p>Modulus $p(X)$ of the field, specified either as a binary polynomial or as an integer to be converted to the former. Must be an irreducible polynomial. If not specified, the modulus is chosen from the list of default primitive polynomials.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; field = komm.FiniteBifield(4)\n&gt;&gt;&gt; field\nFiniteBifield(4)\n&gt;&gt;&gt; (field.characteristic, field.degree, field.order)\n(2, 4, 16)\n&gt;&gt;&gt; field.modulus\nBinaryPolynomial(0b10011)\n</code></pre> <pre><code>&gt;&gt;&gt; field = komm.FiniteBifield(4, modulus=0b11001)\n&gt;&gt;&gt; field\nFiniteBifield(4, modulus=0b11001)\n&gt;&gt;&gt; (field.characteristic, field.degree, field.order)\n(2, 4, 16)\n&gt;&gt;&gt; field.modulus\nBinaryPolynomial(0b11001)\n</code></pre> Construction of elements <p>To construct elements of the finite field, call the finite field object. For example, <code>field(0b1101)</code> will construct the element whose polynomial representation is $X^3 + X^2 + 1$.</p> Algebraic structure <p>The following operations are supported: addition (<code>+</code>), subtraction (<code>-</code>), multiplication (<code>*</code>), division (<code>/</code>), and exponentiation (<code>**</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; field = komm.FiniteBifield(4)\n&gt;&gt;&gt; x = field(0b1011)\n&gt;&gt;&gt; y = field(0b1100)\n&gt;&gt;&gt; x + y\n0b111\n&gt;&gt;&gt; x - y\n0b111\n&gt;&gt;&gt; x * y\n0b1101\n&gt;&gt;&gt; x / y\n0b10\n&gt;&gt;&gt; x**2\n0b1001\n</code></pre> Further methods on elements <p>The following methods are available on elements of the finite field:</p> <ul> <li><code>logarithm(base)</code>: Returns the logarithm of the element, with respect to a given base.</li> <li><code>conjugates()</code>: Returns the conjugates of the element.</li> <li><code>minimal_polynomial()</code>: Returns the minimal polynomial of the element.</li> </ul> <p>For more details, see LC04, Sec. 2.5.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; field = komm.FiniteBifield(4)\n&gt;&gt;&gt; x = field(0b1011)\n&gt;&gt;&gt; base = field(0b10)\n&gt;&gt;&gt; x.logarithm(base)\n7\n&gt;&gt;&gt; x.conjugates()\n[0b1011, 0b1001, 0b1101, 0b1110]\n&gt;&gt;&gt; x.minimal_polynomial()\nBinaryPolynomial(0b11001)\n</code></pre>"},{"location":"ref/FiniteBifield/#characteristic","title":"<code>characteristic</code><code>  int </code>  <code>property</code>","text":"<p>The characteristic $2$ of the finite field.</p>"},{"location":"ref/FiniteBifield/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order (number of elements) of the finite field. It is given by $2^k$.</p>"},{"location":"ref/FixedToVariableCode/","title":"komm.FixedToVariableCode","text":"<p>General fixed-to-variable length code. A fixed-to-variable length code with source alphabet $\\mathcal{X}$, target alphabet $\\mathcal{Y}$, and source block size $k$ is defined by an encoding mapping $\\Enc: \\mathcal{X}^k \\to \\mathcal{Y}^+$, where the domain is the set of all $k$-tuples with entries in $\\mathcal{X}$, and the co-domain is the set of all finite-length, non-empty tuples with entries in $\\mathcal{Y}$. The elements in the image of $\\Enc$ are called codewords.</p> Note <p>Here, for simplicity, we assume that the source alphabet is $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and the target alphabet is $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, where $|\\mathcal{X}| \\geq 2$ and $|\\mathcal{Y}| \\geq 2$ are called the source cardinality and target cardinality, respectively.</p>"},{"location":"ref/FixedToVariableCode/#from_enc_mapping","title":"<code>from_enc_mapping()</code>  <code>classmethod</code>","text":"<p>Constructs a fixed-to-variable length code from the encoding mapping $\\Enc$.</p> <p>Parameters:</p> <ul> <li> <code>enc_mapping</code> (<code>dict[Word, Word]</code>)         \u2013          <p>The encoding mapping $\\Enc$. Must be a dictionary whose keys are all the $k$-tuples of integers in $\\mathcal{X}$ and whose values are non-empty tuples of integers in $\\mathcal{Y}$.</p> </li> </ul> Notes <p>The source block size $k$ is inferred from the domain of the encoding mapping, and the source and target cardinalities $|\\mathcal{X}|$ and $|\\mathcal{Y}|$ are inferred from the maximum values in the domain and co-domain, respectively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_enc_mapping({\n...     (0,): (0,),\n...     (1,): (1, 0),\n...     (2,): (1, 1),\n... })\n&gt;&gt;&gt; code.source_cardinality, code.target_cardinality, code.source_block_size\n(3, 2, 1)\n&gt;&gt;&gt; code.codewords\n[(0,), (1, 0), (1, 1)]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_enc_mapping({\n...     (0, 0): (0,),\n...     (0, 1): (1, 1),\n...     (1, 0): (1, 1, 0),\n...     (1, 1): (1, 0, 1),\n... })\n&gt;&gt;&gt; code.source_cardinality, code.target_cardinality, code.source_block_size\n(2, 2, 2)\n&gt;&gt;&gt; code.codewords\n[(0,), (1, 1), (1, 1, 0), (1, 0, 1)]\n</code></pre>"},{"location":"ref/FixedToVariableCode/#from_codewords","title":"<code>from_codewords()</code>  <code>classmethod</code>","text":"<p>Constructs a fixed-to-variable length code from the source cardinality $|\\mathcal{X}|$ and a list of codewords.</p> <p>Parameters:</p> <ul> <li> <code>source_cardinality</code> (<code>int</code>)         \u2013          <p>The source cardinality $|\\mathcal{X}|$. Must be an integer greater than or equal to $2$.</p> </li> <li> <code>codewords</code> (<code>list[Word]</code>)         \u2013          <p>The codewords of the code. Must be a list of length $|\\mathcal{X}|^k$ containing tuples of integers in $\\mathcal{Y}$. The tuple in position $i$ must be equal to $\\Enc(\\mathbf{x})$, where $\\mathbf{x}$ is the $i$-th element in the lexicographic ordering of $\\mathcal{X}^k$.</p> </li> </ul> Notes <p>The source block size $k$ is inferred from the length of the codewords, and the target cardinality $|\\mathcal{Y}|$ is inferred from the maximum value in the codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(\n...     source_cardinality=3,\n...     codewords=[(0,), (1, 0), (1, 1)],\n... )\n&gt;&gt;&gt; code.source_cardinality, code.target_cardinality, code.source_block_size\n(3, 2, 1)\n&gt;&gt;&gt; code.enc_mapping\n{(0,): (0,),\n (1,): (1, 0),\n (2,): (1, 1)}\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(\n...     source_cardinality=2,\n...     codewords=[(0,), (1, 1), (1, 1, 0), (1, 0, 1)]\n... )\n&gt;&gt;&gt; code.source_cardinality, code.target_cardinality, code.source_block_size\n(2, 2, 2)\n&gt;&gt;&gt; code.enc_mapping\n{(0, 0): (0,),\n (0, 1): (1, 1),\n (1, 0): (1, 1, 0),\n (1, 1): (1, 0, 1)}\n</code></pre>"},{"location":"ref/FixedToVariableCode/#source_cardinality","title":"<code>source_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The source cardinality $|\\mathcal{X}|$ of the code. It is the number of symbols in the source alphabet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.source_cardinality\n3\n</code></pre>"},{"location":"ref/FixedToVariableCode/#target_cardinality","title":"<code>target_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The target cardinality $|\\mathcal{Y}|$ of the code. It is the number of symbols in the target alphabet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.target_cardinality\n2\n</code></pre>"},{"location":"ref/FixedToVariableCode/#source_block_size","title":"<code>source_block_size</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The source block size $k$ of the code. It is the number of symbols in each source word.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.source_block_size\n1\n</code></pre>"},{"location":"ref/FixedToVariableCode/#size","title":"<code>size</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of codewords in the code. It is equal to $|\\mathcal{X}|^k$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.size\n3\n</code></pre>"},{"location":"ref/FixedToVariableCode/#enc_mapping","title":"<code>enc_mapping</code><code>  dict[Word, Word] </code>  <code>cached</code> <code>property</code>","text":"<p>The encoding mapping $\\Enc$ of the code. It is a dictionary of length $|\\mathcal{X}|^k$ whose keys are all the $k$-tuples of integers in $\\mathcal{X}$ and whose values are the corresponding codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.enc_mapping\n{(0,): (0,),\n (1,): (1, 0),\n (2,): (1, 1)}\n</code></pre>"},{"location":"ref/FixedToVariableCode/#codewords","title":"<code>codewords</code><code>  list[Word] </code>  <code>cached</code> <code>property</code>","text":"<p>The codewords of the code. They correspond to the image of the encoding mapping $\\Enc$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_enc_mapping({\n...     (0,): (0,),\n...     (1,): (1, 0),\n...     (2,): (1, 1),\n... })\n&gt;&gt;&gt; code.codewords\n[(0,), (1, 0), (1, 1)]\n</code></pre>"},{"location":"ref/FixedToVariableCode/#is_uniquely_decodable","title":"<code>is_uniquely_decodable()</code>  <code>cached</code>","text":"<p>Returns whether the code is uniquely decodable. A code is uniquely decodable if there is a unique way to parse any concatenation of codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.is_uniquely_decodable()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0, 1), (1, 1)])\n&gt;&gt;&gt; code.is_uniquely_decodable()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0, 1), (1, 0)])\n&gt;&gt;&gt; code.is_uniquely_decodable()  # 010 can be parsed as 0|10 or 01|0\nFalse\n</code></pre>"},{"location":"ref/FixedToVariableCode/#is_prefix_free","title":"<code>is_prefix_free()</code>  <code>cached</code>","text":"<p>Returns whether the code is prefix-free. A code is prefix-free if no codeword is a prefix of any other codeword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0, 1), (1, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0, 1), (1, 0)])\n&gt;&gt;&gt; code.is_prefix_free()\nFalse\n</code></pre>"},{"location":"ref/FixedToVariableCode/#kraft_parameter","title":"<code>kraft_parameter()</code>  <code>cached</code>","text":"<p>Computes the Kraft parameter $K$ of the code. This quantity is given by $$     K = \\sum_{\\mathbf{x} \\in \\mathcal{X}^k} |\\mathcal{Y}|^{-{\\ell(\\mathbf{x})}}, $$ where $\\ell(\\mathbf{x})$ is the length of the codeword $\\Enc(\\mathbf{x})$ associated with the source word $\\mathbf{x} \\in \\mathcal{X}^k$.</p> <p>Returns:</p> <ul> <li> <code>kraft_parameter</code>  (<code>float</code>)          \u2013          <p>The Kraft parameter $K$ of the code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(\n...     source_cardinality=5,\n...     codewords=[(0, 0, 0), (0, 0, 1), (0, 1, 0), (1, 0, 1), (1, 1)],\n... )\n&gt;&gt;&gt; code.kraft_parameter()\nnp.float64(0.75)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(\n...     source_cardinality=4,\n...     codewords=[(0,), (1, 0), (1, 1, 0), (1, 1, 1)],\n... )\n&gt;&gt;&gt; code.kraft_parameter()\nnp.float64(1.0)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(\n...     source_cardinality=4,\n...     codewords=[(0, 0), (1, 1), (0,), (1,)],\n... )\n&gt;&gt;&gt; code.kraft_parameter()\nnp.float64(1.5)\n</code></pre>"},{"location":"ref/FixedToVariableCode/#rate","title":"<code>rate()</code>","text":"<p>Computes the expected rate $R$ of the code, considering a given (first-order) pmf $p$ over $\\mathcal{X}$. This quantity is given by $$     R = \\frac{\\bar{n}}{k}, $$ where $\\bar{n}$ is the expected codeword length, assuming iid source symbols drawn according to $p$. It is measured in $|\\mathcal{Y}|$-ary digits per source symbol.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>rate</code>  (<code>float</code>)          \u2013          <p>The expected rate $R$ of the code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.rate([1/2, 1/4, 1/4])\nnp.float64(1.5)\n</code></pre>"},{"location":"ref/FixedToVariableCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a sequence of source symbols using the code, which must be uniquely decodable.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of symbols to be encoded. Must be a 1D-array with elements in $\\mathcal{X}$ and have a length that is a multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>Array1D[integer]</code>)          \u2013          <p>The sequence of encoded symbols. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_enc_mapping({\n...     (0, 0): (0,),\n...     (0, 1): (1, 1),\n...     (1, 0): (1, 0, 0),\n...     (1, 1): (1, 0, 1),\n... })\n</code></pre> <pre><code>&gt;&gt;&gt; code.encode([0, 1, 0, 0])\narray([1, 1, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; code.encode([0, 1, 0])  # Not a multiple of the source block size\nTraceback (most recent call last):\n...\nValueError: length of input must be a multiple of block size 2 (got 3)\n</code></pre> <pre><code>&gt;&gt;&gt; code.encode([0, 7, 0, 0])  # 07 is not a valid source word\nTraceback (most recent call last):\n...\nValueError: input contains invalid word\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0,1), (1,0)])\n&gt;&gt;&gt; code.encode([0, 1, 0])  # Code is not uniquely decodable\nTraceback (most recent call last):\n...\nValueError: code is not uniquely decodable\n</code></pre>"},{"location":"ref/FixedToVariableCode/#decode","title":"<code>decode()</code>","text":"<p>Decodes a sequence of target symbols using the code, which must be uniquely decodable.</p> Warning <p>Decoding for non-prefix-free codes is not implemented yet.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of symbols to be decoded. Must be a 1D-array with elements in $\\mathcal{Y}$. Also, the sequence must be a concatenation of codewords (i.e., the output of the <code>encode</code> method).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>Array1D[integer]</code>)          \u2013          <p>The sequence of decoded symbols. It is a 1D-array with elements in $\\mathcal{X}$ with a length that is a multiple of $k$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_enc_mapping({\n...     (0, 0): (0,),\n...     (0, 1): (1, 1),\n...     (1, 0): (1, 0, 0),\n...     (1, 1): (1, 0, 1),\n... })\n</code></pre> <pre><code>&gt;&gt;&gt; code.decode([1, 1, 0])\narray([0, 1, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; code.decode([0, 0, 1])  # Not a concatenation of codewords\nTraceback (most recent call last):\n...\nValueError: input contains invalid word\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.FixedToVariableCode.from_codewords(3, [(0,), (0,1), (1,0)])\n&gt;&gt;&gt; code.decode([0, 1, 0])  # Code is not uniquely decodable\nTraceback (most recent call last):\n...\nValueError: code is not uniquely decodable\n</code></pre>"},{"location":"ref/GaussianPulse/","title":"komm.GaussianPulse","text":"<p>Gaussian pulse. It is a pulse with spectrum given by $$     \\hat{p}(f) = \\frac{1}{\\sqrt{2 \\pi} \\bar{B}} \\mathrm{e}^{-\\frac{1}{2} (f / \\bar{B})^2}, $$ where the $\\bar{B} = B / \\sqrt{\\ln 2}$, and $B$ is the half-power bandwidth of the filter.</p> <p>The waveform of the Gaussian pulse is depicted below for $B = 0.5$, and for $B = 1$.</p> <p> </p> <p>Parameters:</p> <ul> <li> <code>half_power_bandwidth</code> (<code>float</code>)         \u2013          <p>The half-power bandwidth $B$ of the pulse. The default value is <code>1.0</code>.</p> </li> </ul>"},{"location":"ref/GaussianPulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the Gaussian pulse, it is given by $$     p(t) = \\mathrm{e}^{-\\frac{1}{2} (2 \\pi \\bar{B} t)^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.169, 0.367, 0.641, 0.895, 1.   , 0.895, 0.641, 0.367, 0.169])\n</code></pre>"},{"location":"ref/GaussianPulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )).round(3)\narray([0.005, 0.059, 0.332, 0.939, 1.329, 0.939, 0.332, 0.059, 0.005])\n</code></pre>"},{"location":"ref/GaussianPulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the Gaussian pulse, it is given by $$     R(\\tau) = \\frac{1}{2 \\sqrt{\\pi} \\bar{B}} \\mathbb{e}^{-(\\pi \\bar{B} \\tau)^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.386, 0.569, 0.752, 0.889, 0.939, 0.889, 0.752, 0.569, 0.386])\n</code></pre>"},{"location":"ref/GaussianPulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the Gaussian pulse, it is given by $$     S(f) = \\frac{1}{2 \\pi \\bar{B}^2} \\mathrm{e}^{-(f / \\bar{B})^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.003, 0.11 , 0.883, 1.765, 0.883, 0.11 , 0.003, 0.   ])\n</code></pre>"},{"location":"ref/GaussianPulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the Gaussian pulse, the support is given by $(-\\infty, \\infty)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; pulse.support\n(-inf, inf)\n</code></pre>"},{"location":"ref/GaussianPulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.GaussianPulse(half_power_bandwidth=0.25)\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1)).round(3)\narray([0.169, 0.367, 0.641, 0.895, 1.   , 0.895, 0.641, 0.367, 0.169])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-16, 16)).shape\n(129,)\n</code></pre>"},{"location":"ref/GolayCode/","title":"komm.GolayCode","text":"<p>Binary Golay code. It is the linear block code with parity submatrix $$ P = \\begin{bmatrix}     1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\     0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\     1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\     1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\     1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\     0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\     0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\     0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\     1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\     1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\     1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix} $$</p> <p>The Golay code has the following parameters:</p> <ul> <li>Length: $23$</li> <li>Dimension: $12$</li> <li>Minimum distance: $7$</li> </ul> Notes <ul> <li>The binary Golay code is a perfect code.</li> </ul> <p>Parameters:</p> <ul> <li> <code>extended</code> (<code>bool</code>)         \u2013          <p>If <code>True</code>, constructs the code in extended version. The default value is <code>False</code>.</p> </li> </ul> <p>This class represents the code in systematic form, with the information set on the left.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.GolayCode()\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(23, 12, 11)\n&gt;&gt;&gt; code.minimum_distance()\n7\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.GolayCode(extended=True)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(24, 12, 12)\n&gt;&gt;&gt; code.minimum_distance()\n8\n</code></pre>"},{"location":"ref/GoldSequence/","title":"komm.GoldSequence","text":"<p>Gold sequence [Not implemented yet].</p>"},{"location":"ref/HammingCode/","title":"komm.HammingCode","text":"<p>Hamming code. For a given parameter $\\mu \\geq 2$, it is the linear block code with check matrix whose columns are all the $2^\\mu - 1$ nonzero binary $\\mu$-tuples. The Hamming code has the following parameters:</p> <ul> <li>Length: $n = 2^\\mu - 1$</li> <li>Dimension: $k = 2^\\mu - \\mu - 1$</li> <li>Redundancy: $m = \\mu$</li> <li>Minimum distance: $d = 3$</li> </ul> <p>In its extended version, the Hamming code has the following parameters:</p> <ul> <li>Length: $n = 2^\\mu$</li> <li>Dimension: $k = 2^\\mu - \\mu - 1$</li> <li>Redundancy: $m = \\mu + 1$</li> <li>Minimum distance: $d = 4$</li> </ul> <p>For more details, see LC04, Sec. 4.1.</p> Notes <ul> <li>For $\\mu = 2$ it reduces to the repetition code of length $3$.</li> <li>Its dual is the simplex code.</li> <li>Hamming codes are perfect codes.</li> </ul> <p>Parameters:</p> <ul> <li> <code>mu</code> (<code>int</code>)         \u2013          <p>The parameter $\\mu$ of the code. Must satisfy $\\mu \\geq 2$.</p> </li> <li> <code>extended</code> (<code>bool</code>)         \u2013          <p>Whether to use the extended version of the Hamming code. Default is <code>False</code>.</p> </li> </ul> <p>This class represents the code in systematic form, with the information set on the left.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HammingCode(3)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(7, 4, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 0, 1, 1, 0],\n       [0, 1, 0, 0, 1, 0, 1],\n       [0, 0, 1, 0, 0, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 0, 1, 1, 0, 0],\n       [1, 0, 1, 1, 0, 1, 0],\n       [0, 1, 1, 1, 0, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.HammingCode(3, extended=True)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(8, 4, 4)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 0, 1, 1, 0, 1],\n       [0, 1, 0, 0, 1, 0, 1, 1],\n       [0, 0, 1, 0, 0, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 0, 1, 1, 0, 0, 0],\n       [1, 0, 1, 1, 0, 1, 0, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0],\n       [1, 1, 1, 0, 0, 0, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n4\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/","title":"komm.HighRateConvolutionalCode","text":"<p>High-rate convolutional encoder. It is an $(n, n-1)$ recursive systematic convolutional encoder defined by a single check row $h(D) \\in \\mathbb{F}_2[D]^n$ and realized in observable canonical form. By convention, the first $n - 1$ positions represent the information bits.</p> <p>Parameters:</p> <ul> <li> <code>h_row</code> (<code>ArrayLike</code>)         \u2013          <p>The check row $h(D)$ of the encoder. Must be an $n$-vector whose entries are either binary polynomials or integers to be converted to the former.</p> </li> </ul> <p>Examples:</p> <p>Consider the high-rate convolutional encoder with $(n, k, \\sigma) = (4, 3, 3)$ depicted below.</p> <p></p> <p>Its check row is given by $$     h(D) =     \\begin{bmatrix}         D^3 + D  &amp;&amp;  D^3 + D^2 + 1  &amp;&amp;  D^3 + D + 1  &amp;&amp;  D^3 + 1     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0b1010, 0b1101, 0b1011, 0b1001])\n&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n</code></pre> <p>Please refer to the table of optimal high-rate convolutional codes.</p>"},{"location":"ref/HighRateConvolutionalCode/#num_input_bits","title":"<code>num_input_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of input bits per block, $k$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.num_input_bits\n3\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#num_output_bits","title":"<code>num_output_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of output bits per block, $n$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.num_output_bits\n4\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#degree","title":"<code>degree</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The degree $\\sigma$ of the encoder. This corresponds to the number of delay elements in the encoder realization.</p> <p>For a high-rate convolutional encoder realized in observable canonical form, the degree $\\sigma$ is the maximum degree of the polynomials in the check row $h(D)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.degree\n3\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[object_] </code>  <code>cached</code> <code>property</code>","text":"<p>Returns the (transform-domain) generator matrix (also known as transfer function matrix) $G(D)$ of the encoder. This is a $k \\times n$ array of binary polynomial fractions.</p> <p>For a high-rate convolutional code with check row $$     h(D) =     \\begin{bmatrix}         h_0(D)  &amp;&amp;  h_1(D)  &amp;&amp;  \\cdots  &amp;&amp;  h_{n-1}(D)     \\end{bmatrix}, $$ the generator matrix is given by $$     G(D) =     \\begin{bmatrix}         1      &amp; 0      &amp; \\cdots &amp; 0      &amp; h_0(D) / h_{n-1}(D)     \\\\[1ex]         0      &amp; 1      &amp; \\cdots &amp; 0      &amp; h_1(D) / h_{n-1}(D)     \\\\[1ex]         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots                  \\\\[1ex]         0      &amp; 0      &amp; \\cdots &amp; 1      &amp; h_{n-2}(D) / h_{n-1}(D)     \\end{bmatrix}. $$</p> <p>Examples:</p> <p>If the check row is $$     h(D) =     \\begin{bmatrix}         D^3 + D  &amp;&amp;  D^3 + D^2 + 1  &amp;&amp;  D^3 + D + 1  &amp;&amp;  D^3 + 1     \\end{bmatrix}, $$ then the generator matrix is $$     G(D) =     \\begin{bmatrix}         1 &amp; 0 &amp; 0 &amp; \\frac{D^3 + D}{D^3 + 1} \\\\[1ex]         0 &amp; 1 &amp; 0 &amp; \\frac{D^3 + D^2 + 1}{D^3 + 1} \\\\[1ex]         0 &amp; 0 &amp; 1 &amp; \\frac{D^3 + D + 1}{D^3 + 1}     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; for row in code.generator_matrix:\n...     print(\"[\" + \", \".join(str(x).ljust(12) for x in row) + \"]\")\n[0b1/0b1     , 0b0/0b1     , 0b0/0b1     , 0b110/0b111 ]\n[0b0/0b1     , 0b1/0b1     , 0b0/0b1     , 0b1101/0b1001]\n[0b0/0b1     , 0b0/0b1     , 0b1/0b1     , 0b1011/0b1001]\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#constraint_lengths","title":"<code>constraint_lengths</code><code>  Array1D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The constraint lengths $\\nu_i$ of the encoder, for $i \\in [0 : k)$. These are defined by $$     \\nu_i = \\max \\{ \\deg p_{i,0}(D), \\deg p_{i,1}(D), \\ldots, \\deg p_{i,n-1}(D), \\deg q_i(D) \\}, $$ where $p_{i,j}(D) / q_i(D)$ are the entries of the $i$-th row of the generator matrix $G(D)$, satisfying $$     \\gcd(p_{i,0}(D), p_{i,1}(D), \\ldots, p_{i,n-1}(D), q_i(D)) = 1. $$ This is a $k$-array of integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.constraint_lengths\narray([3, 3, 3])\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#overall_constraint_length","title":"<code>overall_constraint_length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The overall constraint length $\\nu$ of the encoder, defined by $$     \\nu = \\sum_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.overall_constraint_length\n9\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#memory_order","title":"<code>memory_order</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The memory order $\\mu$ of the encoder. It is given by $$     \\mu = \\max_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.memory_order\n3\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#state_space_representation","title":"<code>state_space_representation()</code>  <code>cached</code>","text":"<p>Returns the matrices $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ corresponding to the state-space representation of the encoder realization. The state-space representation of the encoder is given by $$ \\begin{aligned}     s_{t+1} &amp; = s_t \\mathbf{A} + u_t \\mathbf{B}, \\\\     v_t &amp; = s_t \\mathbf{C} + u_t \\mathbf{D}, \\end{aligned} $$ where</p> <ul> <li>$u_t \\in \\mathbb{B}^k$ is the input block,</li> <li>$v_t \\in \\mathbb{B}^n$ is the output block,</li> <li>$s_t \\in \\mathbb{B}^\\sigma$ is the state,</li> <li>$\\mathbf{A} \\in \\mathbb{B}^{\\sigma \\times \\sigma}$ is the state matrix,</li> <li>$\\mathbf{B} \\in \\mathbb{B}^{k \\times \\sigma}$ is the control matrix,</li> <li>$\\mathbf{C} \\in \\mathbb{B}^{\\sigma \\times n}$ is the observation matrix,</li> <li>$\\mathbf{D} \\in \\mathbb{B}^{k \\times n}$ is the transition matrix.</li> </ul> <p>Returns:</p> <ul> <li> <code>state_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The state matrix $\\mathbf{A}$ of the encoder.</p> </li> <li> <code>control_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The control matrix $\\mathbf{B}$ of the encoder.</p> </li> <li> <code>observation_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The observation matrix $\\mathbf{C}$ of the encoder.</p> </li> <li> <code>transition_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The transition matrix $\\mathbf{D}$ of the encoder.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; A_mat, B_mat, C_mat, D_mat = code.state_space_representation()\n&gt;&gt;&gt; A_mat\narray([[0, 1, 0],\n       [0, 0, 1],\n       [1, 0, 0]])\n&gt;&gt;&gt; B_mat\narray([[1, 0, 1],\n       [0, 1, 0],\n       [0, 0, 1]])\n&gt;&gt;&gt; C_mat\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 1]])\n&gt;&gt;&gt; D_mat\narray([[1, 0, 0, 0],\n       [0, 1, 0, 1],\n       [0, 0, 1, 1]])\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#finite_state_machine","title":"<code>finite_state_machine()</code>  <code>cached</code>","text":"<p>Returns the finite-state (Mealy) machine of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; fsm = code.finite_state_machine()\n&gt;&gt;&gt; (fsm.num_input_symbols, fsm.num_output_symbols, fsm.num_states)\n(8, 16, 8)\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#is_catastrophic","title":"<code>is_catastrophic()</code>  <code>cached</code>","text":"<p>Returns whether the encoder is catastrophic. A convolutional encoder is catastrophic if there exists an infinite-weight input sequence that generates a finite-weight output sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.is_catastrophic()\nFalse\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#free_distance","title":"<code>free_distance()</code>  <code>cached</code>","text":"<p>Returns the free distance $d_\\mathrm{free}$ of the code. This is equal to the minimum Hamming weight among all possible non-zero output sequences.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.free_distance()\n4\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a given bit sequence, starting from the all-zero state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.encode([1, 1, 1, 1, 1, 1])\narray([1, 1, 1, 0, 1, 1, 1, 0])\n</code></pre>"},{"location":"ref/HighRateConvolutionalCode/#encode_with_state","title":"<code>encode_with_state()</code>","text":"<p>Encodes a given bit sequence, starting from a given state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> <li> <code>initial_state</code> (<code>ArrayLike</code>)         \u2013          <p>The initial state. Must be a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> <li> <code>final_state</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The final state. It is a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HighRateConvolutionalCode([0o12, 0o15, 0o13, 0o11])\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1, 1, 1], [0, 0, 0])\n(array([1, 1, 1, 0, 1, 1, 1, 0]), array([1, 0, 1]))\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1, 1, 1], [1, 0, 1])\n(array([1, 1, 1, 1, 1, 1, 1, 0]), array([1, 1, 0]))\n</code></pre>"},{"location":"ref/HuffmanCode/","title":"komm.HuffmanCode","text":"<p>Binary Huffman code. It is an optimal (minimal expected rate) fixed-to-variable length code for a given pmf $p$ over $\\mathcal{X}$. For more details, see Say06, Sec. 3.2.</p> Notes <p>Huffman codes are always prefix-free (hence uniquely decodable).</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> <li> <code>source_block_size</code> (<code>int</code>)         \u2013          <p>The source block size $k$. The default value is $k = 1$.</p> </li> <li> <code>policy</code> (<code>Literal['high', 'low']</code>)         \u2013          <p>The policy to be used when constructing the code. It must be either <code>'high'</code> (move combined symbols as high as possible) or <code>'low'</code> (move combined symbols as low as possible). The default value is <code>'high'</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pmf = [0.8, 0.1, 0.1]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.HuffmanCode(pmf)\n&gt;&gt;&gt; code.enc_mapping\n{(0,): (0,),\n (1,): (1, 0),\n (2,): (1, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(1.2)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.HuffmanCode(pmf, 2)\n&gt;&gt;&gt; code.enc_mapping\n{(0, 0): (0,),\n (0, 1): (1, 0, 1),\n (0, 2): (1, 1, 0),\n (1, 0): (1, 1, 1),\n (1, 1): (1, 0, 0, 1, 0, 0),\n (1, 2): (1, 0, 0, 1, 0, 1),\n (2, 0): (1, 0, 0, 0),\n (2, 1): (1, 0, 0, 1, 1, 0),\n (2, 2): (1, 0, 0, 1, 1, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(0.96)\n</code></pre>"},{"location":"ref/KasamiSequence/","title":"komm.KasamiSequence","text":"<p>Kasami sequence [Not implemented yet].</p>"},{"location":"ref/LFSRSequence/","title":"komm.LFSRSequence","text":"<p>Linear-feedback shift register (LFSR) sequence. It is a binary sequence obtained from the output of a LFSR. The LFSR feedback taps are specified as a binary polynomial $p(X)$ of degree $n$, called the feedback polynomial. More specifically: if bit $i$ of the LFSR is tapped, for $i \\in [1 : n]$, then the coefficient of $X^i$ in $p(X)$ is $1$; otherwise, it is $0$; moreover, the coefficient of $X^0$ in $p(X)$ is always $1$. For example, the feedback polynomial corresponding to the LFSR in the figure below is $p(X) = X^5 + X^2 + 1$, whose integer representation is <code>0b100101</code>.</p> <p></p> <p>The start state of the machine is specified by the so called start state polynomial. More specifically, the coefficient of $X^i$ in the start state polynomial is equal to the initial value of bit $i$ of the LFSR. For more details, see Wikipedia: Linear-feedback shift register and Wikipedia: Maximum-length sequence.</p> <p>The default constructor of this takes the following parameters:</p> <p>Parameters:</p> <ul> <li> <code>feedback_polynomial</code> (<code>BinaryPolynomial | int</code>)         \u2013          <p>The feedback polynomial $p(X)$ of the LFSR, specified either as a binary polynomial or as an integer to be converted to the former.</p> </li> <li> <code>start_state_polynomial</code> (<code>BinaryPolynomial | int</code>)         \u2013          <p>The start state polynomial of the LFSR, specified either as a binary polynomial or as an integer to be converted to the former. The default value is <code>0b1</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lfsr = komm.LFSRSequence(feedback_polynomial=0b100101)\n&gt;&gt;&gt; lfsr.bit_sequence\narray([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1])\n&gt;&gt;&gt; lfsr.cyclic_autocorrelation()\narray([31, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n</code></pre> Maximum-length sequences <p>If the feedback polynomial $p(X)$ is primitive, then the corresponding LFSR sequence will be a maximum-length sequence. Such sequences have the following cyclic autocorrelation: $$     \\tilde{R}[\\ell] =     \\begin{cases}         L, &amp; \\ell = 0, \\, \\pm L, \\, \\pm 2L, \\ldots, \\\\         -1, &amp; \\text{otherwise},     \\end{cases} $$ where $L$ is the length of the sequence. See the class method <code>maximum_length_sequence</code> for a convenient way to construct a maximum-length sequence.</p>"},{"location":"ref/LFSRSequence/#maximum_length_sequence","title":"<code>maximum_length_sequence()</code>  <code>classmethod</code>","text":"<p>Constructs a maximum-length sequences of a given degree. The feedback polynomial $p(X)$ is chosen from the list of default primitive polynomials.</p> <p>Parameters:</p> <ul> <li> <code>degree</code> (<code>int</code>)         \u2013          <p>The degree $n$ of the maximum-length-sequence. Only degrees in the range $[1 : 24]$ are implemented.</p> </li> <li> <code>start_state_polynomial</code> (<code>BinaryPolynomial | int</code>)         \u2013          <p>See the corresponding parameter of the default constructor.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.LFSRSequence.maximum_length_sequence(degree=5)\nLFSRSequence(feedback_polynomial=0b100101, start_state_polynomial=0b1)\n</code></pre>"},{"location":"ref/Labeling/","title":"komm.Labeling","text":"<p>General binary labeling. It is defined by a bijective mapping from $[0:2^m)$ to $\\mathbb{B}^m$ or, equivalently, by an ordered set $\\{ \\mathbf{q}_i : i \\in [0:2^m) \\}$ of $2^m$ distinct binary vectors in  $\\mathbb{B}^m$. In this class, a labeling is represented by a matrix $\\mathbf{Q} \\in \\mathbb{B}^{2^m \\times m}$, where the $i$-th row of $\\mathbf{Q}$ corresponds to the binary vector $\\mathbf{q}_i$. For more details, see SA15, Sec. 2.5.2.</p> <p>Parameters:</p> <ul> <li> <code>matrix</code> (<code>ArrayLike</code>)         \u2013          <p>The labeling matrix $\\mathbf{Q}$. Must be a $2^m \\times m$ binary matrix whose rows are all distinct.</p> </li> </ul>"},{"location":"ref/Labeling/#matrix","title":"<code>matrix</code><code>  NDArray[integer] </code>  <code>property</code>","text":"<p>The labeling matrix $\\mathbf{Q}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.matrix\narray([[1, 0],\n       [1, 1],\n       [0, 1],\n       [0, 0]])\n</code></pre>"},{"location":"ref/Labeling/#num_bits","title":"<code>num_bits</code><code>  int </code>  <code>property</code>","text":"<p>The number $m$ of bits per index of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.num_bits\n2\n</code></pre>"},{"location":"ref/Labeling/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>property</code>","text":"<p>The cardinality $2^m$ of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.cardinality\n4\n</code></pre>"},{"location":"ref/Labeling/#inverse_mapping","title":"<code>inverse_mapping</code><code>  dict[tuple[int, ...], int] </code>  <code>property</code>","text":"<p>The inverse mapping of the labeling. It is a dictionary that maps each binary tuple to the corresponding index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.inverse_mapping\n{(1, 0): 0, (1, 1): 1, (0, 1): 2, (0, 0): 3}\n</code></pre>"},{"location":"ref/Labeling/#indices_to_bits","title":"<code>indices_to_bits()</code>","text":"<p>Returns the binary representation of the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to bits. Must be an array of integers in $[0:2^m)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bits</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The binary representations of the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.indices_to_bits([2, 0])\narray([0, 1, 1, 0])\n&gt;&gt;&gt; labeling.indices_to_bits([[2, 0], [3, 3]])\narray([[0, 1, 1, 0],\n       [0, 0, 0, 0]])\n</code></pre>"},{"location":"ref/Labeling/#bits_to_indices","title":"<code>bits_to_indices()</code>","text":"<p>Returns the indices corresponding to a given sequence of bits.</p> <p>Parameters:</p> <ul> <li> <code>bits</code> (<code>ArrayLike</code>)         \u2013          <p>The bits to be converted to indices. Must be an array with elements in $\\mathbb{B}$ whose last dimension is a multiple $m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices corresponding to the given bits. Has the same shape as <code>bits</code>, but with the last dimension contracted by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.bits_to_indices([0, 1, 1, 0])\narray([2, 0])\n&gt;&gt;&gt; labeling.bits_to_indices([[0, 1, 1, 0], [0, 0, 0, 0]])\narray([[2, 0],\n       [3, 3]])\n</code></pre>"},{"location":"ref/Labeling/#marginalize","title":"<code>marginalize()</code>","text":"<p>Marginalize metrics over the bits of the labeling. The metrics may represent likelihoods or probabilities, for example. The marginalization is done by computing the L-values of the bits, which are defined as $$ L(\\mathtt{b}_i) = \\log \\frac{\\Pr[\\mathtt{b}_i = 0]}{\\Pr[\\mathtt{b}_i = 1]}. $$</p> <p>Parameters:</p> <ul> <li> <code>metrics</code> (<code>ArrayLike</code>)         \u2013          <p>The metrics for each index of the labeling. Must be an array whose last dimension is a multiple of $2^m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lvalues</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The marginalized metrics over the bits of the labeling. Has the same shape as <code>metrics</code>, but with the last dimension changed by a factor of $m / 2^m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]])\n&gt;&gt;&gt; labeling.marginalize([0.1, 0.2, 0.3, 0.4, 0.25, 0.25, 0.25, 0.25])\narray([0.84729786, 0.        , 0.        , 0.        ])\n&gt;&gt;&gt; labeling.marginalize([[0.1, 0.2, 0.3, 0.4], [0.25, 0.25, 0.25, 0.25]])\narray([[0.84729786, 0.        ],\n       [0.        , 0.        ]])\n</code></pre>"},{"location":"ref/LempelZiv77Code/","title":"komm.LempelZiv77Code","text":"<p>Lempel\u2013Ziv 77 (LZ77 or LZ1) code. It is a lossless data compression algorithm which is asymptotically optimal for ergodic sources. Let $\\mathcal{X}$ be the source alphabet, and $\\mathcal{Y}$ be the target alphabet. Also, let $W \\geq 2$ be the size of the sliding window, $S \\in [1 : W)$ be the size of the search buffer, and $L \\in [1 : W)$ be the size of the lookahead buffer, with $S + L = W$. For more details, see Say06, Sec. 5.4.1 and CT06, Sec. 13.4.1.</p> Note <p>Here, for simplicity, we assume that the source alphabet is $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and the target alphabet is $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, where $|\\mathcal{X}| \\geq 2$ and $|\\mathcal{Y}| \\geq 2$ are called the source cardinality and target cardinality, respectively.</p> <p>The token format follows the original LZ77 paper, namely $(p, \\ell, x)$, where $p \\in [0 : S)$ is the pointer for the match, $\\ell \\in [0 : L)$ is the length of the match, and $x \\in \\mathcal{X}$ is the source symbol following the match, but with both $p$ and $\\ell$ being $0$-indexed instead of $1$-indexed. Also following the LZ77 original paper, a token is represented as a fixed-size word in $\\mathcal{Y}^n$, where $$n = \\log S + \\log L + \\log |\\mathcal{X}|$$ and all logs are to base $|\\mathcal{Y}|$.</p> <p>Parameters:</p> <ul> <li> <code>window_size</code> (<code>int</code>)         \u2013          <p>The sliding window size $W$. Must satisfy $W \\geq 2$.</p> </li> <li> <code>lookahead_size</code> (<code>int</code>)         \u2013          <p>The lookahead buffer size $L$. Must satisfy $1 \\leq L &lt; W$.</p> </li> <li> <code>source_cardinality</code> (<code>int</code>)         \u2013          <p>The source cardinality $|\\mathcal{X}|$. Must satisfy $|\\mathcal{X}| \\geq 2$.</p> </li> <li> <code>target_cardinality</code> (<code>int</code>)         \u2013          <p>The target cardinality $|\\mathcal{Y}|$. Must satisfy $|\\mathcal{Y}| \\geq 2$. The default value is $2$ (binary).</p> </li> <li> <code>search_buffer</code> (<code>ArrayLike | None</code>)         \u2013          <p>The initial state of the search buffer. Must be a 1D-array of length $S$ with elements in $\\mathcal{X}$. The default value corresponds to $(0, \\ldots, 0) \\in \\mathcal{X}^S$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=2**13,\n...     lookahead_size=16,\n...     source_cardinality=256,\n...     target_cardinality=2,\n... )\n</code></pre>"},{"location":"ref/LempelZiv77Code/#search_size","title":"<code>search_size</code><code>  int </code>  <code>property</code>","text":"<p>The search buffer size $S$. It is given by $S = W - L$.</p>"},{"location":"ref/LempelZiv77Code/#source_to_tokens","title":"<code>source_to_tokens()</code>","text":"<p>Encodes a given sequence of source symbols to the corresponding list of tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.source_to_tokens([0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2])\n[(8, 2, 1), (7, 3, 2), (6, 7, 2)]\n</code></pre>"},{"location":"ref/LempelZiv77Code/#tokens_to_source","title":"<code>tokens_to_source()</code>","text":"<p>Decodes a given list of tokens to the corresponding sequence of source symbols.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.tokens_to_source([(8, 2, 1), (7, 3, 2), (6, 7, 2)])\narray([0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2])\n</code></pre>"},{"location":"ref/LempelZiv77Code/#tokens_to_target","title":"<code>tokens_to_target()</code>","text":"<p>Returns the target alphabet representation corresponding to a given list of tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.tokens_to_target([(8, 2, 1), (7, 3, 2), (6, 7, 2)])\narray([2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 2])\n</code></pre>"},{"location":"ref/LempelZiv77Code/#target_to_tokens","title":"<code>target_to_tokens()</code>","text":"<p>Returns the list of tokens corresponding to a given target alphabet representation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.target_to_tokens([2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 2])\n[(8, 2, 1), (7, 3, 2), (6, 7, 2)]\n</code></pre>"},{"location":"ref/LempelZiv77Code/#encode","title":"<code>encode()</code>","text":"<p>Encodes a sequence of source symbols to a sequence of target symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of source symbols to be encoded. Must be a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of encoded target symbols. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.encode([0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2])\narray([2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 2])\n</code></pre>"},{"location":"ref/LempelZiv77Code/#decode","title":"<code>decode()</code>","text":"<p>Decodes a sequence of target symbols to a sequence of source symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of target symbols to be decoded. Must be a 1D-array with elements in $\\mathcal{Y}$. Also, the sequence must be a valid output of the <code>encode</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of decoded source symbols. It is a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz77 = komm.LempelZiv77Code(\n...     window_size=18,\n...     lookahead_size=9,\n...     source_cardinality=3,\n...     target_cardinality=3,\n... )\n&gt;&gt;&gt; lz77.decode([2, 2, 0, 2, 1, 2, 1, 1, 0, 2, 2, 0, 2, 1, 2])\narray([0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 2])\n</code></pre>"},{"location":"ref/LempelZiv78Code/","title":"komm.LempelZiv78Code","text":"<p>Lempel\u2013Ziv 78 (LZ78 or LZ2) code. It is a lossless data compression algorithm which is asymptotically optimal for ergodic sources. For more details, see Say06, Sec. 5.4.2 and CT06, Sec. 13.4.2.</p> Note <p>Here, for simplicity, we assume that the source alphabet is $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and the target alphabet is $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, where $|\\mathcal{X}| \\geq 2$ and $|\\mathcal{Y}| \\geq 2$ are called the source cardinality and target cardinality, respectively.</p> <p>The token format is $(p, x)$, where $p \\in \\mathbb{N}$ is the index of the corresponding dictionary entry, and $x \\in \\mathcal{X}$ is the source symbol following the match. The index $p$ is represented as a variable-size word in $\\mathcal{Y}^k$, where $k = \\log_{|\\mathcal{Y}|}(i + 1)$, and $i$ is the size of the dictionary at the moment.</p> <p>Parameters:</p> <ul> <li> <code>source_cardinality</code> (<code>int</code>)         \u2013          <p>The source cardinality $|\\mathcal{X}|$. Must satisfy $|\\mathcal{X}| \\geq 2$.</p> </li> <li> <code>target_cardinality</code> (<code>int</code>)         \u2013          <p>The target cardinality $|\\mathcal{Y}|$. Must satisfy $|\\mathcal{Y}| \\geq 2$. The default value is $2$ (binary).</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)  # Binary source, binary target\n&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(3, 4)  # Ternary source, quaternary target\n</code></pre>"},{"location":"ref/LempelZiv78Code/#source_to_tokens","title":"<code>source_to_tokens()</code>","text":"<p>Encodes a given sequence of source symbols to the corresponding list of tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.source_to_tokens([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n[(0, 1), (0, 0), (1, 1), (2, 1), (4, 0), (2, 0)]\n</code></pre>"},{"location":"ref/LempelZiv78Code/#tokens_to_source","title":"<code>tokens_to_source()</code>","text":"<p>Decodes a given list of tokens to the corresponding sequence of source symbols.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.tokens_to_source([(0, 1), (0, 0), (1, 1), (2, 1), (4, 0), (2, 0)])\narray([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n</code></pre>"},{"location":"ref/LempelZiv78Code/#tokens_to_target","title":"<code>tokens_to_target()</code>","text":"<p>Returns the target alphabet representation corresponding to a given list of tokens.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.tokens_to_target([(0, 1), (0, 0), (1, 1), (2, 1), (4, 0), (2, 0)])\narray([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n</code></pre>"},{"location":"ref/LempelZiv78Code/#target_to_tokens","title":"<code>target_to_tokens()</code>","text":"<p>Returns the list of tokens corresponding to a given target alphabet representation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.target_to_tokens([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n[(0, 1), (0, 0), (1, 1), (2, 1), (4, 0), (2, 0)]\n</code></pre>"},{"location":"ref/LempelZiv78Code/#encode","title":"<code>encode()</code>","text":"<p>Encodes a sequence of source symbols to a sequence of target symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of source symbols to be encoded. Must be a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of encoded target symbols. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.encode([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\narray([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2, 8)\n&gt;&gt;&gt; lz78.encode(np.zeros(15, dtype=int))\narray([0, 1, 0, 2, 0, 3, 0, 4, 0])\n</code></pre>"},{"location":"ref/LempelZiv78Code/#decode","title":"<code>decode()</code>","text":"<p>Decodes a sequence of target symbols to a sequence of source symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of target symbols to be decoded. Must be a 1D-array with elements in $\\mathcal{Y}$. Also, the sequence must be a valid output of the <code>encode</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of decoded source symbols. It is a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2)\n&gt;&gt;&gt; lz78.decode([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0])\narray([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; lz78 = komm.LempelZiv78Code(2, 8)\n&gt;&gt;&gt; lz78.decode([0, 1, 0, 2, 0, 3, 0, 4, 0])\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code></pre>"},{"location":"ref/LempelZivWelchCode/","title":"komm.LempelZivWelchCode","text":"<p>Lempel\u2013Ziv\u2013Welch (LZW) code. It is a lossless data compression algorithm which is variation of the Lempel\u2013Ziv 78 algorithm. For more details, see Say06, Sec. 5.4.2.</p> Note <p>Here, for simplicity, we assume that the source alphabet is $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and the target alphabet is $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, where $|\\mathcal{X}| \\geq 2$ and $|\\mathcal{Y}| \\geq 2$ are called the source cardinality and target cardinality, respectively.</p> <p>Parameters:</p> <ul> <li> <code>source_cardinality</code> (<code>int</code>)         \u2013          <p>The source cardinality $|\\mathcal{X}|$. Must satisfy $|\\mathcal{X}| \\geq 2$.</p> </li> <li> <code>target_cardinality</code> (<code>int</code>)         \u2013          <p>The target cardinality $|\\mathcal{Y}|$. Must satisfy $|\\mathcal{Y}| \\geq 2$. The default value is $2$ (binary).</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(2)  # Binary source, binary target\n&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(3, 4)  # Ternary source, quaternary target\n</code></pre>"},{"location":"ref/LempelZivWelchCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a sequence of source symbols to a sequence of target symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of source symbols to be encoded. Must be a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of encoded target symbols. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(2)\n&gt;&gt;&gt; lzw.encode(np.zeros(15, dtype=int))\narray([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n</code></pre> <pre><code>&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(2, 8)\n&gt;&gt;&gt; lzw.encode(np.zeros(15, dtype=int))\narray([0, 2, 3, 4, 5])\n</code></pre>"},{"location":"ref/LempelZivWelchCode/#decode","title":"<code>decode()</code>","text":"<p>Decodes a sequence of target symbols to a sequence of source symbols.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of target symbols to be decoded. Must be a 1D-array with elements in $\\mathcal{Y}$. Also, the sequence must be a valid output of the <code>encode</code> method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of decoded source symbols. It is a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(2)\n&gt;&gt;&gt; lzw.decode([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1])\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; lzw = komm.LempelZivWelchCode(2, 8)\n&gt;&gt;&gt; lzw.decode([0, 2, 3, 4, 5])\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code></pre>"},{"location":"ref/Lexicode/","title":"komm.Lexicode","text":"<p>Lexicographic code (lexicode). For a given length $n$ and minimum distance $d$, it is the linear block code obtained by starting with the all-zero codeword and adding all binary $n$-tuples (in lexicographic order) that are at least at distance $d$ from all codewords already in the code.</p> <p>Parameters:</p> <ul> <li> <code>n</code> (<code>int</code>)         \u2013          <p>The length $n$ of the code.</p> </li> <li> <code>d</code> (<code>int</code>)         \u2013          <p>The minimum distance $d$ of the code.</p> </li> </ul> <p>For more details, see HP03, Sec. 2.11.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.Lexicode(7, 3)  # Hamming (7, 4)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(7, 4, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[0, 0, 0, 0, 1, 1, 1],\n       [0, 0, 1, 1, 0, 0, 1],\n       [0, 1, 0, 1, 0, 1, 0],\n       [1, 0, 0, 1, 0, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/","title":"komm.LloydMaxQuantizer","text":"<p>Lloyd\u2013Max scalar quantizer. It is a scalar quantizer that minimizes the mean squared error (MSE) between the input signal $X$ and its quantized version. For more details, see Say06, Sec. 9.6.1.</p> <p>Parameters:</p> <ul> <li> <code>input_pdf</code> (<code>Callable[[NDArray[floating]], NDArray[floating]]</code>)         \u2013          <p>The probability density function $f_X(x)$ of the input signal.</p> </li> <li> <code>input_range</code> (<code>tuple[float, float]</code>)         \u2013          <p>The range $(x_\\mathrm{min}, x_\\mathrm{max})$ of the input signal.</p> </li> <li> <code>num_levels</code> (<code>int</code>)         \u2013          <p>The number $L$ of quantization levels. It must be greater than $1$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; uniform_pdf = lambda x: 1/8 * (np.abs(x) &lt;= 4)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=uniform_pdf,\n...     input_range=(-4, 4),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.levels\narray([-3.5, -2.5, -1.5, -0.5,  0.5,  1.5,  2.5,  3.5])\n&gt;&gt;&gt; quantizer.thresholds\narray([-3., -2., -1.,  0.,  1.,  2.,  3.])\n</code></pre> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.levels.round(3)\narray([-2.152, -1.344, -0.756, -0.245,  0.245,  0.756,  1.344,  2.152])\n&gt;&gt;&gt; quantizer.thresholds.round(3)\narray([-1.748, -1.05 , -0.501,  0.   ,  0.501,  1.05 ,  1.748])\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/#levels","title":"<code>levels</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer levels $v_0, v_1, \\ldots, v_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.levels.round(3)\narray([-2.152, -1.344, -0.756, -0.245,  0.245,  0.756,  1.344,  2.152])\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/#thresholds","title":"<code>thresholds</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer finite thresholds $t_1, t_2, \\ldots, t_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.thresholds.round(3)\narray([-1.748, -1.05 , -0.501,  0.   ,  0.501,  1.05 ,  1.748])\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/#mean_squared_error","title":"<code>mean_squared_error()</code>","text":"<p>Computes the mean squared (quantization) error (MSE) of the quantizer for a given input probability density function (pdf). It is defined as $$     \\mse = \\int_{-\\infty}^{\\infty} (y - x)^2 f_X(x) \\, dx $$ where $y$ is the quantized signal and $f_X(x)$ is the pdf of the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input_pdf</code> (<code>Callable[[NDArray[floating]], NDArray[floating]]</code>)         \u2013          <p>The probability density function $f_X(x)$ of the input signal.</p> </li> <li> <code>input_range</code> (<code>tuple[float, float]</code>)         \u2013          <p>The range $(x_\\mathrm{min}, x_\\mathrm{max})$ of the input signal.</p> </li> <li> <code>points_per_interval</code> (<code>int</code>)         \u2013          <p>The number of points per interval for numerical integration (default: 4096).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mse</code>  (<code>float</code>)          \u2013          <p>The mean square quantization error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.mean_squared_error(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n... )\n0.034542475663845607\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/#digitize","title":"<code>digitize()</code>","text":"<p>Returns the quantization indices for the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be digitized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The integer indices of the quantization levels for each input sample.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.digitize([0, 1, 2, 3, 4, 5, 6, 7])\narray([4, 5, 7, 7, 7, 7, 7, 7])\n</code></pre>"},{"location":"ref/LloydMaxQuantizer/#quantize","title":"<code>quantize()</code>","text":"<p>Quantizes the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be quantized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The quantized signal $y$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer = komm.LloydMaxQuantizer(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n...     num_levels=8,\n... )\n&gt;&gt;&gt; quantizer.quantize([0, 1, 2, 3, 4, 5, 6, 7]).round(3)\narray([0.245, 0.756, 2.152, 2.152, 2.152, 2.152, 2.152, 2.152])\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/","title":"komm.LowRateConvolutionalCode","text":"<p>Low-rate convolutional encoder. It is an $(n, 1)$ non-recursive non-systematic convolutional encoder defined by a single generator row $g(D) \\in \\mathbb{F}_2[D]^n$ and realized in controllable canonical form.</p> <p>Parameters:</p> <ul> <li> <code>g_row</code> (<code>ArrayLike</code>)         \u2013          <p>The generator row $g(D)$ of the encoder. Must be an $n$-vector whose entries are either binary polynomials or integers to be converted to the former.</p> </li> </ul> <p>Examples:</p> <p>Consider the low-rate convolutional encoder with $(n, k, \\sigma) = (2, 1, 6)$ depicted below.</p> <p></p> <p>Its generator row is given by $$     g(D) =     \\begin{bmatrix}         D^6 + D^3 + D^2 + D + 1  &amp;&amp;  D^6 + D^5 + D^3 + D^2 + 1     \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0b1001111, 0b1101101])\n&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n</code></pre> <p>Please refer to the table of optimal low-rate convolutional codes.</p>"},{"location":"ref/LowRateConvolutionalCode/#num_input_bits","title":"<code>num_input_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of input bits per block, $k$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.num_input_bits\n1\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#num_output_bits","title":"<code>num_output_bits</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of output bits per block, $n$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.num_output_bits\n2\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#degree","title":"<code>degree</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The degree $\\sigma$ of the encoder. This corresponds to the number of delay elements in the encoder realization.</p> <p>For a low-rate convolutional encoder realized in controllable canonical form, the degree $\\sigma$ is the maximum degree of the polynomials in the generator row $g(D)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.degree\n6\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[object_] </code>  <code>cached</code> <code>property</code>","text":"<p>Returns the (transform-domain) generator matrix (also known as transfer function matrix) $G(D)$ of the encoder. This is a $k \\times n$ array of binary polynomial fractions.</p> <p>For a low-rate convolutional code, it is given by $$     G(D) = \\big[ ~ g(D) ~ \\big]. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; for row in code.generator_matrix:\n...     print(\"[\" + \", \".join(str(x) for x in row) + \"]\")\n[0b1001111/0b1, 0b1101101/0b1]\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#constraint_lengths","title":"<code>constraint_lengths</code><code>  Array1D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The constraint lengths $\\nu_i$ of the encoder, for $i \\in [0 : k)$. These are defined by $$     \\nu_i = \\max \\{ \\deg p_{i,0}(D), \\deg p_{i,1}(D), \\ldots, \\deg p_{i,n-1}(D), \\deg q_i(D) \\}, $$ where $p_{i,j}(D) / q_i(D)$ are the entries of the $i$-th row of the generator matrix $G(D)$, satisfying $$     \\gcd(p_{i,0}(D), p_{i,1}(D), \\ldots, p_{i,n-1}(D), q_i(D)) = 1. $$ This is a $k$-array of integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.constraint_lengths\narray([6])\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#overall_constraint_length","title":"<code>overall_constraint_length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The overall constraint length $\\nu$ of the encoder, defined by $$     \\nu = \\sum_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.overall_constraint_length\n6\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#memory_order","title":"<code>memory_order</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The memory order $\\mu$ of the encoder. It is given by $$     \\mu = \\max_{i \\in [0:k)} \\nu_i, $$ where $\\nu_i$ are the constraint lengths of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.memory_order\n6\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#state_space_representation","title":"<code>state_space_representation()</code>  <code>cached</code>","text":"<p>Returns the matrices $(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ corresponding to the state-space representation of the encoder realization. The state-space representation of the encoder is given by $$ \\begin{aligned}     s_{t+1} &amp; = s_t \\mathbf{A} + u_t \\mathbf{B}, \\\\     v_t &amp; = s_t \\mathbf{C} + u_t \\mathbf{D}, \\end{aligned} $$ where</p> <ul> <li>$u_t \\in \\mathbb{B}^k$ is the input block,</li> <li>$v_t \\in \\mathbb{B}^n$ is the output block,</li> <li>$s_t \\in \\mathbb{B}^\\sigma$ is the state,</li> <li>$\\mathbf{A} \\in \\mathbb{B}^{\\sigma \\times \\sigma}$ is the state matrix,</li> <li>$\\mathbf{B} \\in \\mathbb{B}^{k \\times \\sigma}$ is the control matrix,</li> <li>$\\mathbf{C} \\in \\mathbb{B}^{\\sigma \\times n}$ is the observation matrix,</li> <li>$\\mathbf{D} \\in \\mathbb{B}^{k \\times n}$ is the transition matrix.</li> </ul> <p>Returns:</p> <ul> <li> <code>state_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The state matrix $\\mathbf{A}$ of the encoder.</p> </li> <li> <code>control_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The control matrix $\\mathbf{B}$ of the encoder.</p> </li> <li> <code>observation_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The observation matrix $\\mathbf{C}$ of the encoder.</p> </li> <li> <code>transition_matrix</code>  (<code>Array2D[integer]</code>)          \u2013          <p>The transition matrix $\\mathbf{D}$ of the encoder.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; A_mat, B_mat, C_mat, D_mat = code.state_space_representation()\n&gt;&gt;&gt; A_mat\narray([[0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0]])\n&gt;&gt;&gt; B_mat\narray([[1, 0, 0, 0, 0, 0]])\n&gt;&gt;&gt; C_mat\narray([[1, 0],\n       [1, 1],\n       [1, 1],\n       [0, 0],\n       [0, 1],\n       [1, 1]])\n&gt;&gt;&gt; D_mat\narray([[1, 1]])\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#finite_state_machine","title":"<code>finite_state_machine()</code>  <code>cached</code>","text":"<p>Returns the finite-state (Mealy) machine of the encoder.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; fsm = code.finite_state_machine()\n&gt;&gt;&gt; (fsm.num_input_symbols, fsm.num_output_symbols, fsm.num_states)\n(2, 4, 64)\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#is_catastrophic","title":"<code>is_catastrophic()</code>  <code>cached</code>","text":"<p>Returns whether the encoder is catastrophic. A convolutional encoder is catastrophic if there exists an infinite-weight input sequence that generates a finite-weight output sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.is_catastrophic()\nFalse\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#free_distance","title":"<code>free_distance()</code>  <code>cached</code>","text":"<p>Returns the free distance $d_\\mathrm{free}$ of the code. This is equal to the minimum Hamming weight among all possible non-zero output sequences.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.free_distance()\n10\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a given bit sequence, starting from the all-zero state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.encode([1, 1, 1, 1])\narray([1, 1, 0, 1, 1, 0, 0, 1])\n</code></pre>"},{"location":"ref/LowRateConvolutionalCode/#encode_with_state","title":"<code>encode_with_state()</code>","text":"<p>Encodes a given bit sequence, starting from a given state.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The bit sequence to be encoded. Must be a 1D-array of bits, with length multiple of $k$.</p> </li> <li> <code>initial_state</code> (<code>ArrayLike</code>)         \u2013          <p>The initial state. Must be a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The encoded bit sequence. It is a 1D-array of bits, with length multiple of $n$.</p> </li> <li> <code>final_state</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The final state. It is a 1D-array of length $\\sigma$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.LowRateConvolutionalCode([0o117, 0o155])\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1], [0, 0, 0, 0, 0, 0])\n(array([1, 1, 0, 1, 1, 0, 0, 1]), array([1, 1, 1, 1, 0, 0]))\n&gt;&gt;&gt; code.encode_with_state([1, 1, 1, 1], [1, 1, 1, 1, 0, 0])\n(array([0, 1, 0, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 1, 1]))\n</code></pre>"},{"location":"ref/ManchesterPulse/","title":"komm.ManchesterPulse","text":"<p>Manchester pulse. It is a pulse with waveform given by $$     p(t) =     \\begin{cases}         -1, &amp; 0 \\leq t &lt;  1/2, \\\\         1, &amp; 1/2 \\leq t &lt; 1, \\\\         0, &amp; \\text{otherwise},     \\end{cases} $$</p> <p>The waveform of the Manchester pulse is depicted below.</p> <p></p> <p>Parameters:</p> <p>(No parameters)</p>"},{"location":"ref/ManchesterPulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the Manchester pulse, it is given by $$     p(t) = -\\rect\\left(\\frac{t - 1/4}{1/2}\\right) + \\rect\\left(\\frac{t - 3/4}{1/2}\\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([ 0.,  0.,  0.,  0., -1., -1.,  1.,  1.,  0.])\n</code></pre>"},{"location":"ref/ManchesterPulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>For the Manchester pulse, it is given by $$     \\hat{p}(f) = \\sinc \\left( \\frac{f}{2} \\right) \\, \\sin \\left( 2 \\pi \\frac{f}{4} \\right) \\mathrm{e}^{-\\mathrm{j} 2 \\pi (f/2\\,+\\,1/4)}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )).round(3)\narray([0.637, 0.725, 0.637, 0.373, 0.   , 0.373, 0.637, 0.725, 0.637])\n</code></pre>"},{"location":"ref/ManchesterPulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the Manchester pulse, it is given $$     R(\\tau) = \\tri \\left( \\frac{\\tau}{1/2} \\right) - \\frac{1}{2} \\tri \\left( \\frac{\\tau + 1/2}{1/2} \\right) - \\frac{1}{2} \\tri \\left( \\frac{\\tau - 1/2}{1/2} \\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([ 0.  , -0.25, -0.5 ,  0.25,  1.  ,  0.25, -0.5 , -0.25,  0.  ])\n</code></pre>"},{"location":"ref/ManchesterPulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the Manchester pulse, it is given by $$     S(f) = \\sinc^2 \\left( \\frac{f}{2} \\right) \\, \\sin^2 \\left( 2 \\pi \\frac{f}{4} \\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.405, 0.525, 0.405, 0.139, 0.   , 0.139, 0.405, 0.525, 0.405])\n</code></pre>"},{"location":"ref/ManchesterPulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the Manchester pulse, the support is given by $[0, 1]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; pulse.support\n(0.0, 1.0)\n</code></pre>"},{"location":"ref/ManchesterPulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.ManchesterPulse()\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4)  # Default span is [0, 1]\narray([-1., -1.,  1.,  1.,  0.])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1))\narray([ 0., 0., 0., 0., -1., -1.,  1.,  1.,  0.])\n</code></pre>"},{"location":"ref/MarkovChain/","title":"komm.MarkovChain","text":"<p>Finite-state homogeneous discrete-time Markov chain. It is defined by a finite set of states $\\mathcal{S}$ and a transition matrix $P$ over $\\mathcal{S}$. Here, for simplicity, the set of states is taken as $\\mathcal{S} = [0 : |\\mathcal{S}|)$, where $|\\mathcal{S}|$ denotes the cardinality of the set of states. For more details, see YG14, Ch. MCS and GS97, Ch. 11.</p> <p>Parameters:</p> <ul> <li> <code>transition_matrix</code> (<code>ArrayLike</code>)         \u2013          <p>The transition matrix $P$ over the set of states $\\mathcal{S}$. It must be a $|\\mathcal{S}| \\times |\\mathcal{S}|$-matrix where $P_{i,j}$ is the probability of transitioning from state $i \\in \\mathcal{S}$ to state $j \\in \\mathcal{S}$. Its elements must be non-negative and each row must sum to $1$.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>Consider the finite-state homogeneous discrete-time Markov chain depicted in the figure below.</p> <p> </p> <p>It has set of states $\\mathcal{S} = \\{ 0, 1, 2 \\}$ and transition matrix $$ P = \\begin{bmatrix}     1/2 &amp; 1/4 &amp; 1/4 \\\\     1/2 &amp;  0  &amp; 1/2 \\\\     1/4 &amp; 1/4 &amp; 1/2 \\end{bmatrix}. $$</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n</code></pre> </li> </ol>"},{"location":"ref/MarkovChain/#num_states","title":"<code>num_states</code><code>  int </code>  <code>property</code>","text":"<p>The number of states in the Markov chain, $|\\mathcal{S}|$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.num_states\n3\n</code></pre>"},{"location":"ref/MarkovChain/#accessible_states_from","title":"<code>accessible_states_from()</code>  <code>cached</code>","text":"<p>Computes the subset of states that are accessible from a given state. State $j \\in \\mathcal{S}$ is accessible from state $i \\in \\mathcal{S}$, denoted by $i \\to j$, if there exists $n \\geq 0$ such that $(P^n)_{i,j} &gt; 0$.</p> <p>Parameters:</p> <ul> <li> <code>state</code> (<code>int</code>)         \u2013          <p>A state $i \\in \\mathcal{S}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[int]</code>         \u2013          <p>The subset of all states accessible from state $i$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.accessible_states_from(1)\n{0, 1, 2}\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.accessible_states_from(0)\n{0, 1}\n&gt;&gt;&gt; chain.accessible_states_from(2)\n{2}\n</code></pre>"},{"location":"ref/MarkovChain/#communicating_classes","title":"<code>communicating_classes()</code>  <code>cached</code>","text":"<p>Computes the communicating classes of the Markov chain. A communicating class is a subset of states such that every state in the class is accessible from every other state in the class. In other words, two states $i, j \\in \\mathcal{S}$ are in the same communicating class if and only if $i \\to j$ and $j \\to i$. The set of all communicating classes forms a partition of $\\mathcal{S}$.</p> <p>Returns:</p> <ul> <li> <code>list[set[int]]</code>         \u2013          <p>A list of all communicating classes in the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.communicating_classes()\n[{0, 1, 2}]\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.communicating_classes()\n[{0, 1}, {2}]\n</code></pre>"},{"location":"ref/MarkovChain/#is_irreducible","title":"<code>is_irreducible()</code>  <code>cached</code>","text":"<p>Returns whether the Markov chain is irreducible. A Markov chain is irreducible if $i \\to j$ for all $i, j \\in \\mathcal{S}$. Equivalently, a Markov chain is irreducible if it has only one communicating class.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p><code>True</code> if the Markov chain is irreducible, <code>False</code> otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.is_irreducible()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.is_irreducible()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.is_irreducible()\nTrue\n</code></pre>"},{"location":"ref/MarkovChain/#stationary_distribution","title":"<code>stationary_distribution()</code>  <code>cached</code>","text":"<p>Computes the stationary distribution of an irreducible Markov chain. The stationary distribution $\\pi$ is a pmf over $\\mathcal{S}$ such that $\\pi P = \\pi$.</p> Note <p>This method is only implemented for irreducible chains.</p> <p>Returns:</p> <ul> <li> <code>Array1D[floating]</code>         \u2013          <p>The stationary distribution $\\pi$ of the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.stationary_distribution()\narray([0.4, 0.2, 0.4])\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.stationary_distribution()\narray([0.33333333, 0.33333333, 0.33333333])\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1, 0, 0],\n...     [0, 1, 0],\n...     [0, 0, 1],\n... ])\n&gt;&gt;&gt; chain.stationary_distribution()\nTraceback (most recent call last):\n...\nNotImplementedError: method is only implemented for irreducible chains\n</code></pre>"},{"location":"ref/MarkovChain/#transient_states","title":"<code>transient_states()</code>  <code>cached</code>","text":"<p>Returns the subset $\\mathcal{T} \\subseteq \\mathcal{S}$ of transient states of the Markov chain. A state $i \\in \\mathcal{S}$ is transient if there exists a state $j \\in \\mathcal{S}$ such that $i \\to j$ but $j \\not\\to i$.</p> <p>Returns:</p> <ul> <li> <code>set[int]</code>         \u2013          <p>The subset $\\mathcal{T}$ of transient states in the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.transient_states()\nset()\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2,   0, 1/2],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.transient_states()\n{0, 1}\n</code></pre>"},{"location":"ref/MarkovChain/#recurrent_states","title":"<code>recurrent_states()</code>  <code>cached</code>","text":"<p>Returns the subset $\\mathcal{R} \\subseteq \\mathcal{S}$ of recurrent states of the Markov chain. A state $i \\in \\mathcal{S}$ is recurrent if it is not transient.</p> <p>Returns:</p> <ul> <li> <code>set[int]</code>         \u2013          <p>The subset $\\mathcal{R}$ of recurrent states in the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.recurrent_states()\n{0, 1, 2}\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2,   0, 1/2],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.recurrent_states()\n{2}\n</code></pre>"},{"location":"ref/MarkovChain/#is_regular","title":"<code>is_regular()</code>  <code>cached</code>","text":"<p>Returns whether the Markov chain is regular. A Markov chain is regular if there exists a positive integer $n$ such that all entries of $P^n$ are positive. A finite-state Markov chain is regular if and only if it is irreducible and aperiodic.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p><code>True</code> if the Markov chain is regular, <code>False</code> otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.is_regular()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.is_regular()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.is_regular()\nFalse\n</code></pre>"},{"location":"ref/MarkovChain/#index_of_primitivity","title":"<code>index_of_primitivity()</code>","text":"<p>Computes the index of primitivity of a regular Markov chain. The index of primitivity is the smallest positive integer $n$ such that all entries of $P^n$ are positive.</p> Notes <ul> <li>This method only applies to regular Markov chains.</li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The index of primitivity of the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.index_of_primitivity()\n2\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.index_of_primitivity()\nTraceback (most recent call last):\n...\nValueError: chain is not regular\n</code></pre>"},{"location":"ref/MarkovChain/#absorbing_states","title":"<code>absorbing_states()</code>  <code>cached</code>","text":"<p>Returns the subset $\\mathcal{A} \\subseteq \\mathcal{S}$ of absorbing states of the Markov chain. A state $i \\in \\mathcal{S}$ is absorbing if $P_{i,i} = 1$.</p> <p>Returns:</p> <ul> <li> <code>set[int]</code>         \u2013          <p>The subset $\\mathcal{A}$ of absorbing states in the Markov chain.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.absorbing_states()\nset()\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2,   0, 1/2],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.absorbing_states()\n{2}\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.absorbing_states()\n{2}\n</code></pre>"},{"location":"ref/MarkovChain/#is_absorbing","title":"<code>is_absorbing()</code>  <code>cached</code>","text":"<p>Returns whether the Markov chain is absorbing. A Markov chain is absorbing if it has at least one absorbing state and each absorbing state is accessible from at least one transient state.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p><code>True</code> if the Markov chain is absorbing, <code>False</code> otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.is_absorbing()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2,   0, 1/2],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.is_absorbing()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.is_absorbing()\nFalse\n</code></pre>"},{"location":"ref/MarkovChain/#mean_number_of_visits","title":"<code>mean_number_of_visits()</code>  <code>cached</code>","text":"<p>Computes the mean number of visits from each transient state to each transient state. This is the expected number of times the chain, starting from transient state $i \\in \\mathcal{T}$, visits transient state $j \\in \\mathcal{T}$ before being absorbed.</p> Notes <ul> <li>This method only applies to absorbing Markov chains.</li> <li>This corresponds to the fundamental matrix $N$ of the Markov chain.</li> </ul> <p>Returns:</p> <ul> <li> <code>Array2D[floating]</code>         \u2013          <p>A $|\\mathcal{T}| \\times |\\mathcal{T}|$-matrix $N$ where entry $N_{i,j}$ is the mean number of visits from $i \\in \\mathcal{T}$ to $j \\in \\mathcal{T}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [  1,   0,   0,   0],\n...     [1/2,   0, 1/2,   0],\n...     [  0, 1/2,   0, 1/2],\n...     [  0,   0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.mean_number_of_visits()\narray([[1.33333333, 0.66666667],\n       [0.66666667, 1.33333333]])\n</code></pre>"},{"location":"ref/MarkovChain/#mean_time_to_absorption","title":"<code>mean_time_to_absorption()</code>  <code>cached</code>","text":"<p>Computes the mean time to absorption from each transient state. This is the expected number of steps until the chain, starting from transient state $i \\in \\mathcal{T}$, is absorbed.</p> Notes <ul> <li>This method only applies to absorbing Markov chains.</li> <li>This is obtained by adding all the entries in the $i$-th row of the fundamental matrix $N$.</li> </ul> <p>Returns:</p> <ul> <li> <code>Array1D[floating]</code>         \u2013          <p>A $|\\mathcal{T}|$-vector $\\mathbf{t}$ where $\\mathbf{t}_i$ is the mean time to absorption from $i \\in \\mathcal{T}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [  1,   0,   0,   0],\n...     [1/2,   0, 1/2,   0],\n...     [  0, 1/2,   0, 1/2],\n...     [  0,   0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.mean_time_to_absorption()\narray([2., 2.])\n</code></pre>"},{"location":"ref/MarkovChain/#absorption_probabilities","title":"<code>absorption_probabilities()</code>  <code>cached</code>","text":"<p>Computes the absorption probabilities from each transient state to each absorbing state. This is the probability of, starting from transient state $i \\in \\mathcal{T}$, being absorbed in absorbing state $j \\in \\mathcal{A}$.</p> Notes <ul> <li>This method only applies to absorbing Markov chains.</li> <li>This corresponds to the matrix $B = N R$, where $N$ is the fundamental matrix and $R$ is the submatrix of $P$ row-indexed by $\\mathcal{T}$ and column-indexed by $\\mathcal{A}$.</li> </ul> <p>Returns:</p> <ul> <li> <code>Array2D[floating]</code>         \u2013          <p>A $|\\mathcal{T}| \\times |\\mathcal{A}|$-matrix $B$ where $B_{i,j}$ is the absorption probability from $i \\in \\mathcal{T}$ to $j \\in \\mathcal{A}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [  1,   0,   0,   0],\n...     [1/2,   0, 1/2,   0],\n...     [  0, 1/2,   0, 1/2],\n...     [  0,   0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.absorption_probabilities()\narray([[0.66666667, 0.33333333],\n       [0.33333333, 0.66666667]])\n</code></pre>"},{"location":"ref/MarkovChain/#period","title":"<code>period()</code>  <code>cached</code>","text":"<p>Computes the period of a given state. The period of a state $i \\in \\mathcal{S}$ is the largest integer $d$ such that $(P^n)_{i,i} = 0$ whenever $n$ is not divisible by $d$.</p> <p>Parameters:</p> <ul> <li> <code>state</code> (<code>int</code>)         \u2013          <p>A state $i \\in \\mathcal{S}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The period of the state $i$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.period(1)\n1\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.period(0)\n3\n</code></pre>"},{"location":"ref/MarkovChain/#is_aperiodic","title":"<code>is_aperiodic()</code>  <code>cached</code>","text":"<p>Returns whether the Markov chain is aperiodic. A Markov chain is aperiodic if all states have period $1$.</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p><code>True</code> if the Markov chain is aperiodic, <code>False</code> otherwise.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.is_aperiodic()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/2,   0],\n...     [1/2, 1/2,   0],\n...     [  0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.is_aperiodic()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.is_aperiodic()\nFalse\n</code></pre>"},{"location":"ref/MarkovChain/#simulate","title":"<code>simulate()</code>","text":"<p>Returns random samples from the Markov chain.</p> <p>Parameters:</p> <ul> <li> <code>steps</code> (<code>int</code>)         \u2013          <p>The number of steps to simulate.</p> </li> <li> <code>initial_state</code> (<code>int</code>)         \u2013          <p>The state $i \\in \\mathcal{S}$ from which to start the simulation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>Array1D[integer]</code>)          \u2013          <p>A 1D-array of integers in $\\mathcal{S}$ representing the states visited during the simulation. The length of the array is equal to <code>steps</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [1/2, 1/4, 1/4],\n...     [1/2,   0, 1/2],\n...     [1/4, 1/4, 1/2],\n... ])\n&gt;&gt;&gt; chain.simulate(initial_state=1, steps=10)\narray([1, 2, 1, 2, 2, 0, 2, 2, 2, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [0, 1, 0],\n...     [0, 0, 1],\n...     [1, 0, 0],\n... ])\n&gt;&gt;&gt; chain.simulate(initial_state=2, steps=10)\narray([2, 0, 1, 2, 0, 1, 2, 0, 1, 2])\n</code></pre>"},{"location":"ref/MarkovChain/#simulate_until_absorption","title":"<code>simulate_until_absorption()</code>","text":"<p>Returns random samples from the Markov chain until an absorbing state is reached.</p> Note <p>This method only applies to absorbing Markov chains.</p> <p>Parameters:</p> <ul> <li> <code>initial_state</code> (<code>int</code>)         \u2013          <p>The state $i \\in \\mathcal{S}$ from which to start the simulation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>Array1D[integer]</code>)          \u2013          <p>A 1D-array of integers in $\\mathcal{S}$ representing the states visited during the simulation.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; chain = komm.MarkovChain([\n...     [  1,   0,   0,   0],\n...     [1/2,   0, 1/2,   0],\n...     [  0, 1/2,   0, 1/2],\n...     [  0,   0,   0,   1],\n... ])\n&gt;&gt;&gt; chain.simulate_until_absorption(initial_state=1)\narray([1, 2, 1, 2, 3])\n</code></pre>"},{"location":"ref/MealyMachine/","title":"komm.MealyMachine","text":"<p>Finite-state Mealy machine. It is defined by a set of states $\\mathcal{S}$, an input alphabet $\\mathcal{X}$, an output alphabet $\\mathcal{Y}$, a transition function $T : \\mathcal{S} \\times \\mathcal{X} \\to \\mathcal{S}$, and an output function $G : \\mathcal{S} \\times \\mathcal{X} \\to \\mathcal{Y}$. Here, for simplicity, the set of states, the input alphabet, and the output alphabet are taken as $\\mathcal{S} = [0 : |\\mathcal{S}|)$, $\\mathcal{X} = [0 : |\\mathcal{X}|)$, and $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, respectively. more details, see Wikipedia: Mealy machine.</p> <p>Parameters:</p> <ul> <li> <code>transitions</code> (<code>ArrayLike</code>)         \u2013          <p>The matrix of transitions of the machine, of shape $|\\mathcal{S}| \\times |\\mathcal{X}|$. The element in row $s \\in \\mathcal{S}$ and column $x \\in \\mathcal{X}$ should be $T(s, x) \\in \\mathcal{S}$, that is, the next state of the machine given that the current state is $s$ and the input is $x$.</p> </li> <li> <code>outputs</code> (<code>ArrayLike</code>)         \u2013          <p>The matrix of outputs of the machine, of shape $|\\mathcal{S}| \\times |\\mathcal{X}|$. The element in row $s \\in \\mathcal{S}$ and column $x \\in \\mathcal{X}$ should be $G(s, x) \\in \\mathcal{Y}$, that is, the output of the machine given that the current state is $s$ and the input is $x$.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>Consider the finite-state Mealy machine whose state diagram depicted in the figure below.</p> <p> </p> <p>It has set of states $\\mathcal{S} = \\{ 0, 1, 2, 3 \\}$, input alphabet $\\mathcal{X} = \\{ 0, 1 \\}$, output alphabet $\\mathcal{Y} = \\{ 0, 1, 2, 3 \\}$. The transition function $T$ and output function $G$ are given by the table below.</p> State $s$ Input $x$ Transition $T(s, x)$ Output $G(s, x)$ $0$ $0$ $0$ $0$ $0$ $1$ $1$ $3$ $1$ $0$ $2$ $1$ $1$ $1$ $3$ $2$ $2$ $0$ $0$ $3$ $2$ $1$ $1$ $0$ $3$ $0$ $2$ $2$ $3$ $1$ $3$ $1$ <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n</code></pre> </li> </ol>"},{"location":"ref/MealyMachine/#num_states","title":"<code>num_states</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of states of the machine.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; machine.num_states\n4\n</code></pre>"},{"location":"ref/MealyMachine/#num_input_symbols","title":"<code>num_input_symbols</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The size (cardinality) of the input alphabet $\\mathcal{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; machine.num_input_symbols\n2\n</code></pre>"},{"location":"ref/MealyMachine/#num_output_symbols","title":"<code>num_output_symbols</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The size (cardinality) of the output alphabet $\\mathcal{Y}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; machine.num_output_symbols\n4\n</code></pre>"},{"location":"ref/MealyMachine/#input_edges","title":"<code>input_edges</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The matrix of input edges of the machine. It has shape $|\\mathcal{S}| \\times |\\mathcal{S}|$. If there is an edge from $s_0 \\in \\mathcal{S}$ to $s_1 \\in \\mathcal{S}$, then the element in row $s_0$ and column $s_1$ is the input associated with that edge (an element of $\\mathcal{X}$); if there is no such edge, then the element is $-1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; machine.input_edges\narray([[ 0,  1, -1, -1],\n       [-1, -1,  0,  1],\n       [ 0,  1, -1, -1],\n       [-1, -1,  0,  1]])\n</code></pre>"},{"location":"ref/MealyMachine/#output_edges","title":"<code>output_edges</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The matrix of output edges of the machine. It has shape $|\\mathcal{S}| \\times |\\mathcal{S}|$. If there is an edge from $s_0 \\in \\mathcal{S}$ to $s_1 \\in \\mathcal{S}$, then the element in row $s_0$ and column $s_1$ is the output associated with that edge (an element of $\\mathcal{Y}$); if there is no such edge, then the element is $-1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; machine.output_edges\narray([[ 0,  3, -1, -1],\n       [-1, -1,  1,  2],\n       [ 3,  0, -1, -1],\n       [-1, -1,  2,  1]])\n</code></pre>"},{"location":"ref/MealyMachine/#process","title":"<code>process()</code>","text":"<p>Returns the output sequence corresponding to a given input sequence. It assumes the machine starts at a given initial state $s_\\mathrm{i}$. The input sequence and the output sequence are denoted by $x = (x_0, x_1, \\ldots, x_{L-1}) \\in \\mathcal{X}^L$ and $y = (y_0, y_1, \\ldots, y_{L-1}) \\in \\mathcal{Y}^L$, respectively.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence $x \\in \\mathcal{X}^L$. It should be a 1D-array with elements in $\\mathcal{X}$.</p> </li> <li> <code>initial_state</code> (<code>int</code>)         \u2013          <p>The initial state $s_\\mathrm{i}$ of the machine. Should be an integer in $\\mathcal{S}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence $y \\in \\mathcal{Y}^L$ corresponding to <code>input</code>, assuming the machine starts at the state given by <code>initial_state</code>. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> <li> <code>final_state</code>  (<code>int</code>)          \u2013          <p>The final state $s_\\mathrm{f}$ of the machine. It is an integer in $\\mathcal{S}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MealyMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[[0, 3], [1, 2], [3, 0], [2, 1]],\n... )\n&gt;&gt;&gt; input, initial_state = [1, 1, 0, 1, 0], 0\n&gt;&gt;&gt; output, final_state = machine.process(input, initial_state)\n&gt;&gt;&gt; output\narray([3, 2, 2, 0, 1])\n&gt;&gt;&gt; final_state\n2\n</code></pre>"},{"location":"ref/MealyMachine/#viterbi","title":"<code>viterbi()</code>","text":"<p>Applies the Viterbi algorithm on a given observed sequence. The Viterbi algorithm finds the most probable input sequence $\\hat{x} \\in \\mathcal{X}^L$ ending in state $s$, for all $s \\in \\mathcal{S}$, given an observed sequence $z \\in \\mathcal{Z}^L$. It is assumed uniform input priors. See LC04, Sec. 12.1.</p> <p>Parameters:</p> <ul> <li> <code>observed</code> (<code>ArrayLike</code>)         \u2013          <p>The observed sequence $z \\in \\mathcal{Z}^L$.</p> </li> <li> <code>metric_function</code> (<code>Callable[[int, Any], float]</code>)         \u2013          <p>The metric function $\\mathcal{Y} \\times \\mathcal{Z} \\to \\mathbb{R}$.</p> </li> <li> <code>initial_metrics</code> (<code>ArrayLike | None</code>)         \u2013          <p>The initial metrics for each state. It must be a 1D-array of length $|\\mathcal{S}|$. The default value is <code>0.0</code> for all states.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>input_hat</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The most probable input sequence $\\hat{x} \\in \\mathcal{X}^L$ ending in state $s$, for all $s \\in \\mathcal{S}$. It is a 2D-array of shape $L \\times |\\mathcal{S}|$, in which column $s$ is equal to $\\hat{x}$.</p> </li> <li> <code>final_metrics</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The final metrics for each state. It is a 1D-array of length $|\\mathcal{S}|$.</p> </li> </ul>"},{"location":"ref/MealyMachine/#viterbi_streaming","title":"<code>viterbi_streaming()</code>","text":"<p>Applies the streaming version of the Viterbi algorithm on a given observed sequence. The path memory (or traceback length) is denoted by $\\tau$. It chooses the survivor with best metric and selects the information block on this path. See LC04, Sec. 12.3.</p> <p>Parameters:</p> <ul> <li> <code>observed</code> (<code>ArrayLike</code>)         \u2013          <p>The observed sequence $z \\in \\mathcal{Z}^L$.</p> </li> <li> <code>metric_function</code> (<code>Callable[[int, Any], float]</code>)         \u2013          <p>The metric function $\\mathcal{Y} \\times \\mathcal{Z} \\to \\mathbb{R}$.</p> </li> <li> <code>memory</code> (<code>MetricMemory</code>)         \u2013          <p>The metrics for each state. It must be a dictionary containing two keys: <code>'paths'</code>, a 2D-array of integers of shape $|\\mathcal{S}| \\times (\\tau + 1)$; and <code>'metrics'</code>, a 2D-array of floats of shape $|\\mathcal{S}| \\times (\\tau + 1)$. This dictionary is updated in-place by this method.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>input_hat</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The most probable input sequence $\\hat{x} \\in \\mathcal{X}^L$</p> </li> </ul>"},{"location":"ref/MealyMachine/#forward_backward","title":"<code>forward_backward()</code>","text":"<p>Applies the forward-backward algorithm on a given observed sequence. The forward-backward algorithm computes the posterior pmf of each input $x_0, x_1, \\ldots, x_{L-1} \\in \\mathcal{X}$ given an observed sequence $z = (z_0, z_1, \\ldots, z_{L-1}) \\in \\mathcal{Z}^L$. The prior pmf of each input may also be provided. See LC04, 12.6.</p> <p>Parameters:</p> <ul> <li> <code>observed</code> (<code>ArrayLike</code>)         \u2013          <p>The observed sequence $z \\in \\mathcal{Z}^L$.</p> </li> <li> <code>metric_function</code> (<code>Callable[[int, Any], float]</code>)         \u2013          <p>The metric function $\\mathcal{Y} \\times \\mathcal{Z} \\to \\mathbb{R}$.</p> </li> <li> <code>input_priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior pmf of each input, of shape $L \\times |\\mathcal{X}|$. The element in row $t \\in [0 : L)$ and column $x \\in \\mathcal{X}$ should be $p(x_t = x)$. The default value yields uniform priors.</p> </li> <li> <code>initial_state_distribution</code> (<code>ArrayLike | None</code>)         \u2013          <p>The pmf of the initial state of the machine. It must be a 1D-array of length $|\\mathcal{S}|$. The default value is uniform over all states.</p> </li> <li> <code>final_state_distribution</code> (<code>ArrayLike | None</code>)         \u2013          <p>The pmf of the final state of the machine. It must be a 1D-array of length $|\\mathcal{S}|$. The default value is uniform over all states.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>input_posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior pmf of each input, given the observed sequence, of shape $L \\times |\\mathcal{X}|$. The element in row $t \\in [0 : L)$ and column $x \\in \\mathcal{X}$ is $p(x_t = x \\mid z)$.</p> </li> </ul>"},{"location":"ref/MooreMachine/","title":"komm.MooreMachine","text":"<p>Finite-state Moore machine. It is defined by a set of states $\\mathcal{S}$, an input alphabet $\\mathcal{X}$, an output alphabet $\\mathcal{Y}$, a transition function $T : \\mathcal{S} \\times \\mathcal{X} \\to \\mathcal{S}$, and an output function $G : \\mathcal{S} \\to \\mathcal{Y}$. Here, for simplicity, the set of states, the input alphabet, and the output alphabet are taken as $\\mathcal{S} = [0 : |\\mathcal{S}|)$, $\\mathcal{X} = [0 : |\\mathcal{X}|)$, and $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, respectively. more details, see Wikipedia: Moore machine.</p> <p>Parameters:</p> <ul> <li> <code>transitions</code> (<code>ArrayLike</code>)         \u2013          <p>The matrix of transitions of the machine, of shape $|\\mathcal{S}| \\times |\\mathcal{X}|$. The element in row $s \\in \\mathcal{S}$ and column $x \\in \\mathcal{X}$ should be $T(s, x) \\in \\mathcal{S}$, that is, the next state of the machine given that the current state is $s$ and the input is $x$.</p> </li> <li> <code>outputs</code> (<code>ArrayLike</code>)         \u2013          <p>The vector of outputs of the machine, of shape $|\\mathcal{S}|$. The element in position $s \\in \\mathcal{S}$ should be $G(s) \\in \\mathcal{Y}$, that is, the output of the machine given that the current state is $s$.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>Consider the finite-state Moore machine whose state diagram depicted in the figure below.</p> <p> </p> <p>It has set of states $\\mathcal{S} = \\{ 0, 1, 2, 3 \\}$, input alphabet $\\mathcal{X} = \\{ 0, 1 \\}$, output alphabet $\\mathcal{Y} = \\{ 0, 1 \\}$. The transition function $T$ and output function $G$ are given by the tables below.</p> <p> State $s$ Input $x$ Transition $T(s, x)$ $0$ $0$ $0$ $0$ $1$ $1$ $1$ $0$ $2$ $1$ $1$ $3$ $2$ $0$ $0$ $2$ $1$ $1$ $3$ $0$ $2$ $3$ $1$ $3$ <p> State $s$ Output $G(s)$ $0$ $0$ $1$ $0$ $2$ $1$ $3$ $1$ <p> </p> <pre><code>&gt;&gt;&gt; machine = komm.MooreMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[0, 0, 1, 1],\n... )\n</code></pre>"},{"location":"ref/MooreMachine/#num_states","title":"<code>num_states</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of states of the machine.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MooreMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[0, 0, 1, 1],\n... )\n&gt;&gt;&gt; machine.num_states\n4\n</code></pre>"},{"location":"ref/MooreMachine/#num_input_symbols","title":"<code>num_input_symbols</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The size (cardinality) of the input alphabet $\\mathcal{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MooreMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[0, 0, 1, 1],\n... )\n&gt;&gt;&gt; machine.num_input_symbols\n2\n</code></pre>"},{"location":"ref/MooreMachine/#num_output_symbols","title":"<code>num_output_symbols</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The size (cardinality) of the output alphabet $\\mathcal{Y}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MooreMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[0, 0, 1, 1],\n... )\n&gt;&gt;&gt; machine.num_output_symbols\n2\n</code></pre>"},{"location":"ref/MooreMachine/#process","title":"<code>process()</code>","text":"<p>Returns the output sequence corresponding to a given input sequence. It assumes the machine starts at a given initial state $s_\\mathrm{i}$. The input sequence and the output sequence are denoted by $x = (x_0, x_1, \\ldots, x_{L-1}) \\in \\mathcal{X}^L$ and $y = (y_0, y_1, \\ldots, y_{L-1}) \\in \\mathcal{Y}^{L}$, respectively.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence $x \\in \\mathcal{X}^L$. It should be a 1D-array with elements in $\\mathcal{X}$.</p> </li> <li> <code>initial_state</code> (<code>int</code>)         \u2013          <p>The initial state $s_\\mathrm{i}$ of the machine. Should be an integer in $\\mathcal{S}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence $y \\in \\mathcal{Y}^{L}$ corresponding to <code>input</code>, assuming the machine starts at the state given by <code>initial_state</code>. It is a 1D-array with elements in $\\mathcal{Y}$.</p> </li> <li> <code>final_state</code>  (<code>int</code>)          \u2013          <p>The final state $s_\\mathrm{f}$ of the machine. It is an integer in $\\mathcal{S}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; machine = komm.MooreMachine(\n...     transitions=[[0, 1], [2, 3], [0, 1], [2, 3]],\n...     outputs=[0, 0, 1, 1],\n... )\n&gt;&gt;&gt; input, initial_state = [1, 1, 0, 1, 0], 0\n&gt;&gt;&gt; output, final_state = machine.process(input, initial_state)\n&gt;&gt;&gt; output\narray([0, 1, 1, 0, 1])\n&gt;&gt;&gt; final_state\n2\n</code></pre>"},{"location":"ref/NaturalLabeling/","title":"komm.NaturalLabeling","text":"<p>Natural binary labeling. It is a binary labeling in which integer $i \\in [0 : 2^m)$ is mapped to its base-$2$ representation (MSB-first).</p>"},{"location":"ref/NaturalLabeling/#matrix","title":"<code>matrix</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The labeling matrix $\\mathbf{Q}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.matrix\narray([[0, 0],\n       [0, 1],\n       [1, 0],\n       [1, 1]])\n</code></pre>"},{"location":"ref/NaturalLabeling/#num_bits","title":"<code>num_bits</code><code>  int </code>  <code>property</code>","text":"<p>The number $m$ of bits per index of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.num_bits\n2\n</code></pre>"},{"location":"ref/NaturalLabeling/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>property</code>","text":"<p>The cardinality $2^m$ of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.cardinality\n4\n</code></pre>"},{"location":"ref/NaturalLabeling/#inverse_mapping","title":"<code>inverse_mapping</code><code>  dict[tuple[int, ...], int] </code>  <code>cached</code> <code>property</code>","text":"<p>The inverse mapping of the labeling. It is a dictionary that maps each binary tuple to the corresponding index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.inverse_mapping\n{(0, 0): 0, (0, 1): 1, (1, 0): 2, (1, 1): 3}\n</code></pre>"},{"location":"ref/NaturalLabeling/#indices_to_bits","title":"<code>indices_to_bits()</code>","text":"<p>Returns the binary representation of the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to bits. Must be an array of integers in $[0:2^m)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bits</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The binary representations of the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.indices_to_bits([2, 0])\narray([1, 0, 0, 0])\n&gt;&gt;&gt; labeling.indices_to_bits([[2, 0], [3, 3]])\narray([[1, 0, 0, 0],\n       [1, 1, 1, 1]])\n</code></pre>"},{"location":"ref/NaturalLabeling/#bits_to_indices","title":"<code>bits_to_indices()</code>","text":"<p>Returns the indices corresponding to a given sequence of bits.</p> <p>Parameters:</p> <ul> <li> <code>bits</code> (<code>ArrayLike</code>)         \u2013          <p>The bits to be converted to indices. Must be an array with elements in $\\mathbb{B}$ whose last dimension is a multiple $m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices corresponding to the given bits. Has the same shape as <code>bits</code>, but with the last dimension contracted by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.bits_to_indices([1, 0, 0, 0])\narray([2, 0])\n&gt;&gt;&gt; labeling.bits_to_indices([[1, 0, 0, 0], [1, 1, 1, 1]])\narray([[2, 0],\n       [3, 3]])\n</code></pre>"},{"location":"ref/NaturalLabeling/#marginalize","title":"<code>marginalize()</code>","text":"<p>Marginalize metrics over the bits of the labeling. The metrics may represent likelihoods or probabilities, for example. The marginalization is done by computing the L-values of the bits, which are defined as $$ L(\\mathtt{b}_i) = \\log \\frac{\\Pr[\\mathtt{b}_i = 0]}{\\Pr[\\mathtt{b}_i = 1]}. $$</p> <p>Parameters:</p> <ul> <li> <code>metrics</code> (<code>ArrayLike</code>)         \u2013          <p>The metrics for each index of the labeling. Must be an array whose last dimension is a multiple of $2^m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lvalues</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The marginalized metrics over the bits of the labeling. Has the same shape as <code>metrics</code>, but with the last dimension changed by a factor of $m / 2^m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.NaturalLabeling(2)\n&gt;&gt;&gt; labeling.marginalize([0.1, 0.2, 0.3, 0.4, 0.25, 0.25, 0.25, 0.25])\narray([-0.84729786, -0.40546511,  0.        ,  0.        ])\n&gt;&gt;&gt; labeling.marginalize([[0.1, 0.2, 0.3, 0.4], [0.25, 0.25, 0.25, 0.25]])\narray([[-0.84729786, -0.40546511],\n       [ 0.        ,  0.        ]])\n</code></pre>"},{"location":"ref/PAMConstellation/","title":"komm.PAMConstellation","text":"<p>Pulse-amplitude modulation (PAM) constellation. It is a real one-dimensional constellation in which the symbols are uniformly arranged in the real line and centered about the origin. For more details, see SA15, Sec. 2.5.1.</p> <p>Parameters:</p> <ul> <li> <code>order</code> (<code>int</code>)         \u2013          <p>The order $M$ of the constellation.</p> </li> <li> <code>delta</code> (<code>float</code>)         \u2013          <p>The distance $\\Delta$ between adjacent symbols. The default value is <code>2.0</code>.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>The $4$-PAM constellation with $\\Delta = 2$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n</code></pre> </li> <li> <p>The $7$-PAM constellation with $\\Delta = 5$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(7, delta=5)\n</code></pre> </li> </ol>"},{"location":"ref/PAMConstellation/#matrix","title":"<code>matrix</code><code>  Array2D[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.matrix\narray([[-3.],\n       [-1.],\n       [ 1.],\n       [ 3.]])\n</code></pre>"},{"location":"ref/PAMConstellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.order\n4\n</code></pre>"},{"location":"ref/PAMConstellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>For the PAM constellation, it is given by $N = 1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.dimension\n1\n</code></pre>"},{"location":"ref/PAMConstellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[floating]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean of the PAM constellation is given by $$     \\mathbf{m} = 0. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.mean()\narray([0.])\n</code></pre>"},{"location":"ref/PAMConstellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean energy of the PAM constellation is given by $$     E = \\frac{\\Delta^2}{12}(M^2 - 1). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(5.0)\n</code></pre>"},{"location":"ref/PAMConstellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>For the PAM constellation, the minimum distance is given by $$     d_{\\min} = \\Delta. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(2.0)\n</code></pre>"},{"location":"ref/PAMConstellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([ 3., -3.])\n&gt;&gt;&gt; const.indices_to_symbols([[3, 0], [1, 2]])\narray([[ 3., -3.],\n       [-1.,  1.]])\n</code></pre>"},{"location":"ref/PAMConstellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.closest_indices([-0.8, 2.4])\narray([1, 3])\n&gt;&gt;&gt; const.closest_indices([[-0.8, 2.4], [0.0, 10.0]])\narray([[1, 3],\n       [2, 3]])\n</code></pre>"},{"location":"ref/PAMConstellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.closest_symbols([-0.8, 2.4])\narray([-1.,  3.])\n&gt;&gt;&gt; const.closest_symbols([[-0.8, 2.4], [0.0, 10.0]])\narray([[-1.,  3.],\n       [ 1.,  3.]])\n</code></pre>"},{"location":"ref/PAMConstellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PAMConstellation(4)\n&gt;&gt;&gt; const.posteriors([-0.8, 2.4], snr=2.0).round(3)\narray([0.103, 0.7  , 0.195, 0.002, 0.   , 0.007, 0.343, 0.65 ])\n&gt;&gt;&gt; const.posteriors([[-0.8, 2.4], [0.0, 10.0]], snr=2.0).round(3)\narray([[0.103, 0.7  , 0.195, 0.002, 0.   , 0.007, 0.343, 0.65 ],\n       [0.02 , 0.48 , 0.48 , 0.02 , 0.   , 0.   , 0.   , 1.   ]])\n</code></pre>"},{"location":"ref/PSKConstellation/","title":"komm.PSKConstellation","text":"<p>Phase-shift keying (PSK) constellation. It is a complex one-dimensional constellation in which the symbols are uniformly arranged in a circle. More precisely, the $i$-th symbol is given by $$     x_i = A \\exp \\left( \\mathrm{j} \\frac{2 \\pi i}{M} \\right) \\exp(\\mathrm{j} 2 \\pi \\phi), \\quad i \\in [0 : M), $$ where $M$ is the order, $A$ is the amplitude, and $\\phi$ is the phase offset of the constellation.</p> <p>Parameters:</p> <ul> <li> <code>order</code> (<code>int</code>)         \u2013          <p>The order $M$ of the constellation.</p> </li> <li> <code>amplitude</code> (<code>float</code>)         \u2013          <p>The amplitude $A$ of the constellation. The default value is <code>1.0</code>.</p> </li> <li> <code>phase_offset</code> (<code>float</code>)         \u2013          <p>The phase offset $\\phi$ of the constellation (in turns, not radians). The default value is <code>0.0</code>.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>The $4$-PSK constellation with amplitude $A = 1$ and phase offset $\\phi = 0$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n</code></pre> </li> <li> <p>The $8$-PSK constellation with amplitude $A = 0.5$ and phase offset $\\phi = 1/16$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(8, amplitude=0.5, phase_offset=1 / 16)\n</code></pre> </li> </ol>"},{"location":"ref/PSKConstellation/#matrix","title":"<code>matrix</code><code>  Array2D[complexfloating] </code>  <code>cached</code> <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.matrix\narray([[ 1.+0.j],\n       [ 0.+1.j],\n       [-1.+0.j],\n       [ 0.-1.j]])\n</code></pre>"},{"location":"ref/PSKConstellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.order\n4\n</code></pre>"},{"location":"ref/PSKConstellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>For the PSK constellation, it is given by $N = 1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.dimension\n1\n</code></pre>"},{"location":"ref/PSKConstellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[complexfloating]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean of the PSK constellation is given by $$     \\mathbf{m} = 0. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.mean()\narray([0.+0.j])\n</code></pre>"},{"location":"ref/PSKConstellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean energy of the PSK constellation is given by $$     E = A^2. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(1.0)\n</code></pre>"},{"location":"ref/PSKConstellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>For the PSK constellation, the minimum distance is given by $$     d_\\mathrm{min} = 2A\\sin\\left(\\frac{\\pi}{M}\\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(1.414213562373095)\n</code></pre>"},{"location":"ref/PSKConstellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([0.-1.j, 1.+0.j])\n</code></pre>"},{"location":"ref/PSKConstellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.closest_indices([0.1 - 1.1j, 1.2 + 0.1j])\narray([3, 0])\n</code></pre>"},{"location":"ref/PSKConstellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.closest_symbols([0.1 - 1.1j, 1.2 + 0.1j])\narray([0.-1.j, 1.+0.j])\n</code></pre>"},{"location":"ref/PSKConstellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.PSKConstellation(4)\n&gt;&gt;&gt; const.posteriors([0.1 - 1.1j, 1.2 + 0.1j], snr=2.0).round(3)\narray([0.018, 0.   , 0.008, 0.974, 0.982, 0.012, 0.   , 0.005])\n</code></pre>"},{"location":"ref/PolarCode/","title":"komm.PolarCode","text":"<p>Polar (Ar\u0131kan) code. Let $\\mu \\geq 1$ be an integer, and $\\mathcal{F}$ (called the frozen bit indices) be a subset of $[0 : 2^\\mu)$. Define $\\mathcal{A} = [0 : 2^\\mu) \\setminus \\mathcal{F}$ (called the active bit indices). The polar code with parameters $(\\mu, \\mathcal{F})$ is the linear block code whose generator matrix is obtained by selecting the rows of the order-$2^\\mu$ Walsh-Hadamard matrix, $$ H_{2^\\mu} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\end{bmatrix} ^ {\\otimes \\mu}, $$ corresponding to the active bit indices, where $\\otimes$ denotes the Kronecker product. The resulting code has the following parameters:</p> <ul> <li>Length: $n = 2^{\\mu}$</li> <li>Dimension: $k = |\\mathcal{A}|$</li> <li>Redundancy: $m = |\\mathcal{F}|$</li> </ul> <p>Parameters:</p> <ul> <li> <code>mu</code> (<code>int</code>)         \u2013          <p>The parameter $\\mu$ of the code.</p> </li> <li> <code>frozen</code> (<code>ArrayLike</code>)         \u2013          <p>The frozen bit indices $\\mathcal{F}$ of the code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.PolarCode(4, [0, 1, 2, 3, 4, 8])\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(16, 10, 6)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n       [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n       [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n4\n</code></pre>"},{"location":"ref/ProductLabeling/","title":"komm.ProductLabeling","text":"<p>Cartesian product of labelings.</p> <p>Parameters:</p> <ul> <li> <code>*labelings</code> (<code>Labeling</code>)         \u2013          <p>The labelings to be combined. At least one labeling is required.</p> </li> <li> <code>repeat</code> (<code>int</code>)         \u2013          <p>Number of times to repeat the full sequence of labelings. Must be a positive integer. Has the same semantics as <code>itertools.product</code>. The default value is <code>1</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling1 = komm.Labeling([[0, 0], [1, 1], [1, 0], [0, 1]])\n&gt;&gt;&gt; labeling2 = komm.Labeling([[1], [0]])\n&gt;&gt;&gt; labeling = komm.ProductLabeling(labeling1, labeling2)\n&gt;&gt;&gt; labeling.matrix\narray([[0, 0, 1],\n       [0, 0, 0],\n       [1, 1, 1],\n       [1, 1, 0],\n       [1, 0, 1],\n       [1, 0, 0],\n       [0, 1, 1],\n       [0, 1, 0]])\n</code></pre> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling(\n...    komm.Labeling([[1, 0], [1, 1], [0, 1], [0, 0]]),\n...    repeat=2,\n... )\n&gt;&gt;&gt; labeling.matrix\narray([[1, 0, 1, 0],\n       [1, 0, 1, 1],\n       [1, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 1, 1, 0],\n       [1, 1, 1, 1],\n       [1, 1, 0, 1],\n       [1, 1, 0, 0],\n       [0, 1, 1, 0],\n       [0, 1, 1, 1],\n       [0, 1, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 1],\n       [0, 0, 0, 1],\n       [0, 0, 0, 0]])\n</code></pre>"},{"location":"ref/ProductLabeling/#from_matrices","title":"<code>from_matrices()</code>  <code>classmethod</code>","text":"<p>Constructs a product labeling from labeling matrices.</p> <p>Parameters:</p> <ul> <li> <code>*matrices</code> (<code>ArrayLike</code>)         \u2013          <p>The labeling matrices. At least one matrix is required. See labeling documentation.</p> </li> <li> <code>repeat</code> (<code>int</code>)         \u2013          <p>Number of times to repeat the full sequence of matrices. Must be a positive integer. Has the same semantics as <code>itertools.product</code>. The default value is <code>1</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.matrix\narray([[0, 0, 1],\n       [0, 0, 0],\n       [1, 1, 1],\n       [1, 1, 0],\n       [1, 0, 1],\n       [1, 0, 0],\n       [0, 1, 1],\n       [0, 1, 0]])\n</code></pre> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[1, 0], [1, 1], [0, 1], [0, 0]],\n...     repeat=2,\n... )\n&gt;&gt;&gt; labeling.matrix\narray([[1, 0, 1, 0],\n       [1, 0, 1, 1],\n       [1, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 1, 1, 0],\n       [1, 1, 1, 1],\n       [1, 1, 0, 1],\n       [1, 1, 0, 0],\n       [0, 1, 1, 0],\n       [0, 1, 1, 1],\n       [0, 1, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 1],\n       [0, 0, 0, 1],\n       [0, 0, 0, 0]])\n</code></pre>"},{"location":"ref/ProductLabeling/#matrix","title":"<code>matrix</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The labeling matrix $\\mathbf{Q}$.</p>"},{"location":"ref/ProductLabeling/#num_bits","title":"<code>num_bits</code><code>  int </code>  <code>property</code>","text":"<p>The number $m$ of bits per index of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.num_bits\n3\n</code></pre>"},{"location":"ref/ProductLabeling/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>property</code>","text":"<p>The cardinality $2^m$ of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.cardinality\n8\n</code></pre>"},{"location":"ref/ProductLabeling/#inverse_mapping","title":"<code>inverse_mapping</code><code>  dict[tuple[int, ...], int] </code>  <code>cached</code> <code>property</code>","text":"<p>The inverse mapping of the labeling. It is a dictionary that maps each binary tuple to the corresponding index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.inverse_mapping\n{(0, 0, 1): 0,\n (0, 0, 0): 1,\n (1, 1, 1): 2,\n (1, 1, 0): 3,\n (1, 0, 1): 4,\n (1, 0, 0): 5,\n (0, 1, 1): 6,\n (0, 1, 0): 7}\n</code></pre>"},{"location":"ref/ProductLabeling/#indices_to_bits","title":"<code>indices_to_bits()</code>","text":"<p>Returns the binary representation of the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to bits. Must be an array of integers in $[0:2^m)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bits</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The binary representations of the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.indices_to_bits([2, 0])\narray([1, 1, 1, 0, 0, 1])\n</code></pre>"},{"location":"ref/ProductLabeling/#bits_to_indices","title":"<code>bits_to_indices()</code>","text":"<p>Returns the indices corresponding to a given sequence of bits.</p> <p>Parameters:</p> <ul> <li> <code>bits</code> (<code>ArrayLike</code>)         \u2013          <p>The bits to be converted to indices. Must be an array with elements in $\\mathbb{B}$ whose last dimension is a multiple $m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices corresponding to the given bits. Has the same shape as <code>bits</code>, but with the last dimension contracted by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ProductLabeling.from_matrices(\n...     [[0, 0], [1, 1], [1, 0], [0, 1]],\n...     [[1], [0]],\n... )\n&gt;&gt;&gt; labeling.bits_to_indices([1, 1, 1, 0, 0, 1])\narray([2, 0])\n</code></pre>"},{"location":"ref/ProductLabeling/#marginalize","title":"<code>marginalize()</code>","text":"<p>Marginalize metrics over the bits of the labeling. The metrics may represent likelihoods or probabilities, for example. The marginalization is done by computing the L-values of the bits, which are defined as $$ L(\\mathtt{b}_i) = \\log \\frac{\\Pr[\\mathtt{b}_i = 0]}{\\Pr[\\mathtt{b}_i = 1]}. $$</p> <p>Parameters:</p> <ul> <li> <code>metrics</code> (<code>ArrayLike</code>)         \u2013          <p>The metrics for each index of the labeling. Must be an array whose last dimension is a multiple of $2^m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lvalues</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The marginalized metrics over the bits of the labeling. Has the same shape as <code>metrics</code>, but with the last dimension changed by a factor of $m / 2^m$.</p> </li> </ul>"},{"location":"ref/Pulse/","title":"komm.Pulse","text":"<p>General pulse [Not implemented yet].</p>"},{"location":"ref/QAMConstellation/","title":"komm.QAMConstellation","text":"<p>Quadrature amplitude modulation (QAM) constellation. It is a complex one-dimensional constellation obtained by a Cartesian product of two PAM constellations, namely, the in-phase constellation, and the quadrature constellation. For more details, see SA15, Sec. 2.5.1.</p> <p>Parameters:</p> <ul> <li> <code>orders</code> (<code>tuple[int, int] | int</code>)         \u2013          <p>A tuple $(M_\\mathrm{I}, M_\\mathrm{Q})$ with the orders of the in-phase and quadrature constellations, respectively. If specified as a single integer $M$, then it is assumed that $M_\\mathrm{I} = M_\\mathrm{Q} = \\sqrt{M}$; in this case, $M$ must be a perfect square.</p> </li> <li> <code>deltas</code> (<code>tuple[float, float] | float</code>)         \u2013          <p>A tuple $(\\Delta_\\mathrm{I}, \\Delta_\\mathrm{Q})$ with the distances of the in-phase and quadrature constellations, respectively. If specified as a single float $\\Delta$, then it is assumed that $\\Delta_\\mathrm{I} = \\Delta_\\mathrm{Q} = \\Delta$. The default value is <code>2.0</code>.</p> </li> <li> <code>phase_offset</code> (<code>float</code>)         \u2013          <p>The phase offset $\\phi$ of the constellation (in turns, not radians). The default value is <code>0.0</code>.</p> </li> </ul> <p>Examples:</p> <ol> <li> <p>The square $16$-QAM constellation with $(M_\\mathrm{I}, M_\\mathrm{Q}) = (4, 4)$ and $(\\Delta_\\mathrm{I}, \\Delta_\\mathrm{Q}) = (2, 2)$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n</code></pre> </li> <li> <p>The rectangular $8$-QAM constellation with $(M_\\mathrm{I}, M_\\mathrm{Q}) = (4, 2)$ and $(\\Delta_\\mathrm{I}, \\Delta_\\mathrm{Q}) = (2, 4)$ is depicted below.      </p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(orders=(4, 2), deltas=(2.0, 4.0))\n</code></pre> </li> </ol>"},{"location":"ref/QAMConstellation/#matrix","title":"<code>matrix</code><code>  Array2D[complexfloating] </code>  <code>cached</code> <code>property</code>","text":"<p>The constellation matrix $\\mathbf{X}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.matrix\narray([[-3.-3.j],\n       [-3.-1.j],\n       [-3.+1.j],\n       [-3.+3.j],\n       [-1.-3.j],\n       [-1.-1.j],\n       [-1.+1.j],\n       [-1.+3.j],\n       [ 1.-3.j],\n       [ 1.-1.j],\n       [ 1.+1.j],\n       [ 1.+3.j],\n       [ 3.-3.j],\n       [ 3.-1.j],\n       [ 3.+1.j],\n       [ 3.+3.j]])\n</code></pre>"},{"location":"ref/QAMConstellation/#order","title":"<code>order</code><code>  int </code>  <code>property</code>","text":"<p>The order $M$ of the constellation.</p> <p>For the QAM constellation, it is given by $$     M = M_\\mathrm{I} M_\\mathrm{Q}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.order\n16\n</code></pre>"},{"location":"ref/QAMConstellation/#dimension","title":"<code>dimension</code><code>  int </code>  <code>property</code>","text":"<p>The dimension $N$ of the constellation.</p> <p>For the QAM constellation, it is given by $N = 1$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.dimension\n1\n</code></pre>"},{"location":"ref/QAMConstellation/#mean","title":"<code>mean()</code>","text":"<p>Computes the mean $\\mathbf{m}$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ \\mathbf{m} = \\sum_{i \\in [0:M)} p_i \\mathbf{x}_i. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean</code>  (<code>Array1D[complexfloating]</code>)          \u2013          <p>The mean $\\mathbf{m}$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean of the QAM constellation is given by $$     \\mathbf{m} = 0. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.mean()\narray([0.+0.j])\n</code></pre>"},{"location":"ref/QAMConstellation/#mean_energy","title":"<code>mean_energy()</code>","text":"<p>Computes the mean energy $E$ of the constellation given prior probabilities $p_i$ of the constellation symbols. It is given by $$ E = \\sum_{i \\in [0:M)} p_i \\lVert \\mathbf{x}_i \\rVert^2. $$</p> <p>Parameters:</p> <ul> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the constellation symbols. Must be a 1D-array whose size is equal to the order $M$ of the constellation. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mean_energy</code>  (<code>floating</code>)          \u2013          <p>The mean energy $E$ of the constellation.</p> </li> </ul> <p>For uniform priors, the mean energy of the QAM constellation is given by $$     E = \\frac{\\Delta_\\mathrm{I}^2}{12}(M_\\mathrm{I}^2 - 1) + \\frac{\\Delta_\\mathrm{Q}^2}{12}(M_\\mathrm{Q}^2 - 1). $$ For the special case of a square QAM constellation, this simplifies to $$     E = \\frac{\\Delta^2}{6}(M - 1). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.mean_energy()\nnp.float64(10.0)\n</code></pre>"},{"location":"ref/QAMConstellation/#minimum_distance","title":"<code>minimum_distance()</code>","text":"<p>Computes the minimum Euclidean distance $d_\\mathrm{min}$ of the constellation. It is given by $$ d_\\mathrm{min} = \\min_ { i, j \\in [0:M), ~ i \\neq j } \\lVert \\mathrm{x}_i - \\mathrm{x}_j \\rVert. $$</p> <p>For the QAM constellation, the minimum distance is given by $$     d_{\\min} = \\min(\\Delta_\\mathrm{I}, \\Delta_\\mathrm{Q}). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.minimum_distance()\nnp.float64(2.0)\n</code></pre>"},{"location":"ref/QAMConstellation/#indices_to_symbols","title":"<code>indices_to_symbols()</code>","text":"<p>Returns the constellation symbols corresponding to the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to symbols. Must be an array of integers in $[0:M)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols corresponding to the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.indices_to_symbols([3, 0])\narray([-3.+3.j, -3.-3.j])\n&gt;&gt;&gt; const.indices_to_symbols([[3, 0], [1, 2]])\narray([[-3.+3.j, -3.-3.j],\n       [-3.-1.j, -3.+1.j]])\n</code></pre>"},{"location":"ref/QAMConstellation/#closest_indices","title":"<code>closest_indices()</code>","text":"<p>Returns the indices of the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices of the symbols closest to the received points. Has the same shape as <code>received</code>, but with the last dimension contracted by a factor of $N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.closest_indices([-3.1 + 2.9j, -3 - 3.5j])\narray([3, 0])\n</code></pre>"},{"location":"ref/QAMConstellation/#closest_symbols","title":"<code>closest_symbols()</code>","text":"<p>Returns the constellation symbols closest to the given received points.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>symbols</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The symbols closest to the received points. Has the same shape as <code>received</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.closest_symbols([-3.1 + 2.9j, -3 - 3.5j])\narray([-3.+3.j, -3.-3.j])\n</code></pre>"},{"location":"ref/QAMConstellation/#posteriors","title":"<code>posteriors()</code>","text":"<p>Returns the posterior probabilities of each constellation symbol given received points, the signal-to-noise ratio (SNR), and prior probabilities.</p> <p>Parameters:</p> <ul> <li> <code>received</code> (<code>ArrayLike</code>)         \u2013          <p>The received points. Must be an array whose last dimension is a multiple of $N$.</p> </li> <li> <code>snr</code> (<code>float</code>)         \u2013          <p>The signal-to-noise ratio (SNR) of the channel (linear, not decibel).</p> </li> <li> <code>priors</code> (<code>ArrayLike | None</code>)         \u2013          <p>The prior probabilities of the symbols. Must be a 1D-array whose size is equal to $M$. If not given, uniform priors are assumed.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>posteriors</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The posterior probabilities of each symbol given the received points. Has the same shape as <code>received</code>, but with the last dimension changed by a factor of $M / N$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; const = komm.QAMConstellation(16)\n&gt;&gt;&gt; const.posteriors([-3.1 + 2.9j], snr=2.0).round(3)\narray([0.   , 0.021, 0.219, 0.449, 0.   , 0.009, 0.091, 0.186, 0.   ,\n       0.001, 0.008, 0.016, 0.   , 0.   , 0.   , 0.   ])\n</code></pre>"},{"location":"ref/RaisedCosinePulse/","title":"komm.RaisedCosinePulse","text":"<p>Raised-cosine pulse. For a given roll-off factor $\\alpha$ satisfing $0 \\leq \\alpha \\leq 1$, it is a pulse with spectrum given by $$     \\hat{p}(f) = \\begin{cases}         1, &amp; |f| \\leq f_1, \\\\[1ex]         \\dfrac{1}{2} \\left( 1 + \\cos \\left( \\pi \\dfrac{|f| - f_1}{f_2 - f_1}\\right) \\right), &amp; f_1 \\leq |f| \\leq f_2, \\\\[1ex]         0, &amp; \\text{otherwise}.     \\end{cases} $$ where $f_1 = (1 - \\alpha) / 2$ and $f_2 = (1 + \\alpha) / 2$.</p> <p>The waveform of the raised-cosine pulse is depicted below for $\\alpha = 0.25$, and for $\\alpha = 0.75$.</p> <p> </p> <p>For more details, see Wikipedia: Raised-cosine filter.</p> Notes <ul> <li>For $\\alpha = 0$ it reduces to the sinc pulse.</li> <li>For $\\alpha = 1$ it becomes what is known as the full cosine roll-off pulse.</li> </ul> <p>Parameters:</p> <ul> <li> <code>rolloff</code> (<code>float</code>)         \u2013          <p>The roll-off factor $\\alpha$ of the pulse. Must satisfy $0 \\leq \\alpha \\leq 1$. The default value is <code>1.0</code>.</p> </li> </ul>"},{"location":"ref/RaisedCosinePulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the raised-cosine pulse, it is given by $$     p(t) = \\sinc(t) \\frac{\\cos(\\pi \\alpha t)}{1 - (2 \\alpha t)^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.29 , 0.627, 0.897, 1.   , 0.897, 0.627, 0.29 , 0.   ])\n</code></pre>"},{"location":"ref/RaisedCosinePulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ))\narray([0. , 0. , 0.5, 1. , 1. , 1. , 0.5, 0. , 0. ])\n</code></pre>"},{"location":"ref/RaisedCosinePulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the raised-cosine pulse, it is given by $$     R(\\tau) = \\sinc(\\tau) \\frac{\\cos(\\pi \\alpha \\tau)}{1 - (2 \\alpha \\tau)^2} - \\frac{\\alpha}{4} \\sinc(\\alpha \\tau) \\frac{\\cos(\\pi \\tau)}{1 - (\\alpha \\tau)^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.06 , 0.334, 0.627, 0.853, 0.938, 0.853, 0.627, 0.334, 0.06 ])\n</code></pre>"},{"location":"ref/RaisedCosinePulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the raised-cosine pulse, it is given by $$     S(f) = \\begin{cases}         1, &amp; |f| \\leq f_1, \\\\[1ex]         \\dfrac{1}{4} \\left( 1 + \\cos \\left( \\pi \\dfrac{|f| - f_1}{f_2 - f_1}\\right) \\right)^2, &amp; f_1 \\leq |f| \\leq f_2, \\\\[1ex]         0, &amp; \\text{otherwise}.     \\end{cases} $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0.  , 0.  , 0.25, 1.  , 1.  , 1.  , 0.25, 0.  , 0.  ])\n</code></pre>"},{"location":"ref/RaisedCosinePulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the raised-cosine pulse, the support is given by $(-\\infty, \\infty)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.support\n(-inf, inf)\n</code></pre>"},{"location":"ref/RaisedCosinePulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1)).round(3)\narray([0.   , 0.29 , 0.627, 0.897, 1.   , 0.897, 0.627, 0.29 , 0.   ])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-16, 16)).shape\n(129,)\n</code></pre>"},{"location":"ref/RectangularPulse/","title":"komm.RectangularPulse","text":"<p>Rectangular pulse. It is a pulse with waveform given by $$     p(t) =     \\begin{cases}         1, &amp; 0 \\leq t &lt; w, \\\\         0, &amp; \\text{otherwise},     \\end{cases} $$ where $w$ is the relative width of the pulse, which must satisfy $0 &lt; w \\leq 1$.</p> <p>The waveform of the rectangular pulse is depicted below for $w = 1$, and for $w = 0.5$.</p> <p> </p> Notes <ul> <li>For $w = 1$ it is also called the NRZ pulse.</li> <li>For $w = 0.5$ it is also called the halfway RZ pulse.</li> </ul> <p>Parameters:</p> <ul> <li> <code>width</code> (<code>float</code>)         \u2013          <p>The relative width $w$ of the pulse. Must satisfy $0 &lt; w \\leq 1$. The default value is <code>1.0</code>.</p> </li> </ul>"},{"location":"ref/RectangularPulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the rectangular pulse, it is given by $$     p(t) = \\rect\\left(\\frac{t - w/2}{w}\\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)  # NRZ pulse\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0., 0., 0., 0., 1., 1., 1., 1., 0.])\n</code></pre> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=0.5)  # Halfway RZ pulse\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0., 0., 0., 0., 1., 1., 0., 0., 0.])\n</code></pre>"},{"location":"ref/RectangularPulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>For the rectangular pulse, it is given by $$     \\hat{p}(f) = w \\sinc(w f) \\mathrm{e}^{-\\mathrm{j} 2 \\pi (w/2) f}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)  # NRZ pulse\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )).round(3)\narray([0.   , 0.3  , 0.637, 0.9  , 1.   , 0.9  , 0.637, 0.3  , 0.   ])\n</code></pre> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=0.5)  # Halfway RZ pulse\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )).round(3)\narray([0.318, 0.392, 0.45 , 0.487, 0.5  , 0.487, 0.45 , 0.392, 0.318])\n</code></pre>"},{"location":"ref/RectangularPulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the rectangular pulse, it is given by $$     R(\\tau) = w \\tri\\left(\\frac{\\tau}{w}\\right). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)  # NRZ pulse\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0.  , 0.25, 0.5 , 0.75, 1.  , 0.75, 0.5 , 0.25, 0.  ])\n</code></pre> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=0.5)  # Halfway RZ pulse\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0.  , 0.  , 0.  , 0.25, 0.5 , 0.25, 0.  , 0.  , 0.  ])\n</code></pre>"},{"location":"ref/RectangularPulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the rectangular pulse, it is given by $$     S(f) = w^2 \\sinc^2(w f). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)  # NRZ pulse\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.09 , 0.405, 0.811, 1.   , 0.811, 0.405, 0.09 , 0.   ])\n</code></pre> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=0.5)  # Halfway RZ pulse\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.101, 0.154, 0.203, 0.237, 0.25 , 0.237, 0.203, 0.154, 0.101])\n</code></pre>"},{"location":"ref/RectangularPulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the rectangular pulse, the support is given by $[0, w]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)\n&gt;&gt;&gt; pulse.support\n(0.0, 1.0)\n</code></pre> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=0.5)\n&gt;&gt;&gt; pulse.support\n(0.0, 0.5)\n</code></pre>"},{"location":"ref/RectangularPulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RectangularPulse(width=1.0)\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4)  # Default span is [0, 1]\narray([1., 1., 1., 1., 0.])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1))\narray([0., 0., 0., 0., 1., 1., 1., 1., 0.])\n</code></pre>"},{"location":"ref/ReedDecoder/","title":"komm.ReedDecoder","text":"<p>Reed decoder for Reed-Muller codes. It's a majority-logic decoding algorithm. For more details, see [LC04, Sec 4.3 and 10.9.1] for hard-decision decoding, and [LC04, Sec 10.9.2] for soft-decision decoding.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>ReedMullerCode</code>)         \u2013          <p>The Reed-Muller code to be used for decoding.</p> </li> <li> <code>input_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the input. Either <code>'hard'</code> or <code>'soft'</code>. Default is <code>'hard'</code>.</p> </li> </ul> Notes <ul> <li>Input type: <code>hard</code> (bits) or <code>soft</code> (either L-values or channel outputs).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/ReedDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ReedMullerCode(1, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ReedDecoder(code, input_type=\"hard\")\n&gt;&gt;&gt; decoder.decode([[0, 0, 0, 0, 0, 1, 0, 0], [1, 1, 0, 1, 1, 1, 1, 1]])\narray([[0, 0, 0, 0],\n       [0, 0, 0, 1]])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ReedDecoder(code, input_type=\"soft\")\n&gt;&gt;&gt; decoder.decode([+1.3, +1.0, +0.9, +0.4, -0.8, +0.2, +0.3, +0.8])\narray([0, 0, 0, 0])\n</code></pre>"},{"location":"ref/ReedMullerCode/","title":"komm.ReedMullerCode","text":"<p>Reed\u2013Muller code. Let $\\mu$ and $\\rho$ be two integers such that $0 \\leq \\rho &lt; \\mu$. The Reed\u2013Muller code with parameters $(\\rho, \\mu)$ is the linear block code whose generator matrix rows are $$     \\mathbf{1}, \\, v_1, \\ldots, v_m, \\, v_1 v_2, v_1 v_3, \\ldots, v_{\\mu} v_{\\mu - 1}, \\, \\ldots \\, \\text{(up to products of degree $\\rho$)}, $$ where $\\mathbf{1}$ is the all-one $2^\\mu$-tuple, $v_i$ is the binary $2^\\mu$-tuple composed of $2^{\\mu-i}$ repetitions of alternating blocks of zeros and ones, each block of length $2^i$, for $1 \\leq i \\leq \\mu$, and $v_i v_j$ denotes the component-wise product (logical AND) of $v_i$ and $v_j$. Here we take the rows in reverse order. The resulting code has the following parameters:</p> <ul> <li>Length: $n = 2^{\\mu}$</li> <li>Dimension: $k = 1 + {\\mu \\choose 1} + \\cdots + {\\mu \\choose \\rho}$</li> <li>Redundancy: $m = 1 + {\\mu \\choose 1} + \\cdots + {\\mu \\choose \\mu - \\rho - 1}$</li> <li>Minimum distance: $d = 2^{\\mu - \\rho}$</li> </ul> <p>For more details, see LC04, Sec. 4.3.</p> Notes <ul> <li>For $\\rho = 0$ it reduces to a repetition code.</li> <li>For $\\rho = 1$ it reduces to a lengthened simplex code.</li> <li>For $\\rho = \\mu - 2$ it reduces to an extended Hamming code.</li> <li>For $\\rho = \\mu - 1$ it reduces to a single parity-check code.</li> </ul> <p>Parameters:</p> <ul> <li> <code>rho</code> (<code>int</code>)         \u2013          <p>The parameter $\\rho$ of the code.</p> </li> <li> <code>mu</code> (<code>int</code>)         \u2013          <p>The parameter $\\mu$ of the code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ReedMullerCode(2, 4)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(16, 11, 5)\n&gt;&gt;&gt; code.generator_matrix\narray([[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n       [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n4\n</code></pre>"},{"location":"ref/ReedMullerCode/#reed_partitions","title":"<code>reed_partitions()</code>  <code>cached</code>","text":"<p>The Reed partitions of the code. See LC04, Sec. 4.3.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.ReedMullerCode(2, 4)\n&gt;&gt;&gt; reed_partitions = code.reed_partitions()\n&gt;&gt;&gt; reed_partitions[1]\narray([[ 0,  1,  4,  5],\n       [ 2,  3,  6,  7],\n       [ 8,  9, 12, 13],\n       [10, 11, 14, 15]])\n&gt;&gt;&gt; reed_partitions[8]\narray([[ 0,  4],\n       [ 1,  5],\n       [ 2,  6],\n       [ 3,  7],\n       [ 8, 12],\n       [ 9, 13],\n       [10, 14],\n       [11, 15]])\n</code></pre>"},{"location":"ref/ReflectedLabeling/","title":"komm.ReflectedLabeling","text":"<p>Reflected (Gray) binary labeling. It is a binary labeling in which integer $i \\in [0 : 2^m)$ is mapped to its Gray code representation.</p>"},{"location":"ref/ReflectedLabeling/#matrix","title":"<code>matrix</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The labeling matrix $\\mathbf{Q}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.matrix\narray([[0, 0],\n       [0, 1],\n       [1, 1],\n       [1, 0]])\n</code></pre>"},{"location":"ref/ReflectedLabeling/#num_bits","title":"<code>num_bits</code><code>  int </code>  <code>property</code>","text":"<p>The number $m$ of bits per index of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.num_bits\n2\n</code></pre>"},{"location":"ref/ReflectedLabeling/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>property</code>","text":"<p>The cardinality $2^m$ of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.cardinality\n4\n</code></pre>"},{"location":"ref/ReflectedLabeling/#inverse_mapping","title":"<code>inverse_mapping</code><code>  dict[tuple[int, ...], int] </code>  <code>cached</code> <code>property</code>","text":"<p>The inverse mapping of the labeling. It is a dictionary that maps each binary tuple to the corresponding index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.inverse_mapping\n{(0, 0): 0, (0, 1): 1, (1, 1): 2, (1, 0): 3}\n</code></pre>"},{"location":"ref/ReflectedLabeling/#indices_to_bits","title":"<code>indices_to_bits()</code>","text":"<p>Returns the binary representation of the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to bits. Must be an array of integers in $[0:2^m)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bits</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The binary representations of the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.indices_to_bits([2, 0])\narray([1, 1, 0, 0])\n&gt;&gt;&gt; labeling.indices_to_bits([[2, 0], [3, 3]])\narray([[1, 1, 0, 0],\n       [1, 0, 1, 0]])\n</code></pre>"},{"location":"ref/ReflectedLabeling/#bits_to_indices","title":"<code>bits_to_indices()</code>","text":"<p>Returns the indices corresponding to a given sequence of bits.</p> <p>Parameters:</p> <ul> <li> <code>bits</code> (<code>ArrayLike</code>)         \u2013          <p>The bits to be converted to indices. Must be an array with elements in $\\mathbb{B}$ whose last dimension is a multiple $m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices corresponding to the given bits. Has the same shape as <code>bits</code>, but with the last dimension contracted by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.bits_to_indices([1, 1, 0, 0])\narray([2, 0])\n&gt;&gt;&gt; labeling.bits_to_indices([[1, 1, 0, 0], [1, 0, 1, 0]])\narray([[2, 0],\n       [3, 3]])\n</code></pre>"},{"location":"ref/ReflectedLabeling/#marginalize","title":"<code>marginalize()</code>","text":"<p>Marginalize metrics over the bits of the labeling. The metrics may represent likelihoods or probabilities, for example. The marginalization is done by computing the L-values of the bits, which are defined as $$ L(\\mathtt{b}_i) = \\log \\frac{\\Pr[\\mathtt{b}_i = 0]}{\\Pr[\\mathtt{b}_i = 1]}. $$</p> <p>Parameters:</p> <ul> <li> <code>metrics</code> (<code>ArrayLike</code>)         \u2013          <p>The metrics for each index of the labeling. Must be an array whose last dimension is a multiple of $2^m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lvalues</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The marginalized metrics over the bits of the labeling. Has the same shape as <code>metrics</code>, but with the last dimension changed by a factor of $m / 2^m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedLabeling(2)\n&gt;&gt;&gt; labeling.marginalize([0.1, 0.2, 0.3, 0.4, 0.25, 0.25, 0.25, 0.25])\narray([-0.84729786,  0.        ,  0.        ,  0.        ])\n&gt;&gt;&gt; labeling.marginalize([[0.1, 0.2, 0.3, 0.4], [0.25, 0.25, 0.25, 0.25]])\narray([[-0.84729786,  0.        ],\n       [ 0.        ,  0.        ]])\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/","title":"komm.ReflectedRectangularLabeling","text":"<p>Reflected rectangular binary labeling. It is the Cartesian product of two reflected binary labelings, possibly with distinct number of bits.</p>"},{"location":"ref/ReflectedRectangularLabeling/#matrix","title":"<code>matrix</code><code>  NDArray[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The labeling matrix $\\mathbf{Q}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4)\n&gt;&gt;&gt; labeling.matrix\narray([[0, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 1],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 1],\n       [0, 1, 1, 1],\n       [0, 1, 1, 0],\n       [1, 1, 0, 0],\n       [1, 1, 0, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 1],\n       [1, 0, 1, 1],\n       [1, 0, 1, 0]])\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#num_bits","title":"<code>num_bits</code><code>  int </code>  <code>property</code>","text":"<p>The number $m$ of bits per index of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4)\n&gt;&gt;&gt; labeling.num_bits\n4\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#cardinality","title":"<code>cardinality</code><code>  int </code>  <code>property</code>","text":"<p>The cardinality $2^m$ of the labeling.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4)\n&gt;&gt;&gt; labeling.cardinality\n16\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#inverse_mapping","title":"<code>inverse_mapping</code><code>  dict[tuple[int, ...], int] </code>  <code>property</code>","text":"<p>The inverse mapping of the labeling. It is a dictionary that maps each binary tuple to the corresponding index.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4  )\n&gt;&gt;&gt; labeling.inverse_mapping\n{(0, 0, 0, 0): 0,\n (0, 0, 0, 1): 1,\n (0, 0, 1, 1): 2,\n (0, 0, 1, 0): 3,\n (0, 1, 0, 0): 4,\n (0, 1, 0, 1): 5,\n (0, 1, 1, 1): 6,\n (0, 1, 1, 0): 7,\n (1, 1, 0, 0): 8,\n (1, 1, 0, 1): 9,\n (1, 1, 1, 1): 10,\n (1, 1, 1, 0): 11,\n (1, 0, 0, 0): 12,\n (1, 0, 0, 1): 13,\n (1, 0, 1, 1): 14,\n (1, 0, 1, 0): 15}\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#indices_to_bits","title":"<code>indices_to_bits()</code>","text":"<p>Returns the binary representation of the given indices.</p> <p>Parameters:</p> <ul> <li> <code>indices</code> (<code>ArrayLike</code>)         \u2013          <p>The indices to be converted to bits. Must be an array of integers in $[0:2^m)$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bits</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The binary representations of the given indices. Has the same shape as <code>indices</code>, but with the last dimension expanded by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4)\n&gt;&gt;&gt; labeling.indices_to_bits([8, 13])\narray([1, 1, 0, 0, 1, 0, 0, 1])\n&gt;&gt;&gt; labeling.indices_to_bits([[8, 13], [0, 1]])\narray([[1, 1, 0, 0, 1, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1]])\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#bits_to_indices","title":"<code>bits_to_indices()</code>","text":"<p>Returns the indices corresponding to a given sequence of bits.</p> <p>Parameters:</p> <ul> <li> <code>bits</code> (<code>ArrayLike</code>)         \u2013          <p>The bits to be converted to indices. Must be an array with elements in $\\mathbb{B}$ whose last dimension is a multiple $m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>indices</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The indices corresponding to the given bits. Has the same shape as <code>bits</code>, but with the last dimension contracted by a factor of $m$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labeling = komm.ReflectedRectangularLabeling(4)\n&gt;&gt;&gt; labeling.bits_to_indices([1, 1, 0, 0, 1, 0, 0, 1])\narray([ 8, 13])\n&gt;&gt;&gt; labeling.bits_to_indices([\n...     [1, 1, 0, 0, 1, 0, 0, 1],\n...     [0, 0, 0, 0, 0, 0, 0, 1],\n... ])\narray([[ 8, 13],\n       [ 0,  1]])\n</code></pre>"},{"location":"ref/ReflectedRectangularLabeling/#marginalize","title":"<code>marginalize()</code>","text":"<p>Marginalize metrics over the bits of the labeling. The metrics may represent likelihoods or probabilities, for example. The marginalization is done by computing the L-values of the bits, which are defined as $$ L(\\mathtt{b}_i) = \\log \\frac{\\Pr[\\mathtt{b}_i = 0]}{\\Pr[\\mathtt{b}_i = 1]}. $$</p> <p>Parameters:</p> <ul> <li> <code>metrics</code> (<code>ArrayLike</code>)         \u2013          <p>The metrics for each index of the labeling. Must be an array whose last dimension is a multiple of $2^m$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>lvalues</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The marginalized metrics over the bits of the labeling. Has the same shape as <code>metrics</code>, but with the last dimension changed by a factor of $m / 2^m$.</p> </li> </ul>"},{"location":"ref/RepetitionCode/","title":"komm.RepetitionCode","text":"<p>Repetition code. For a given length $n \\geq 1$, it is the linear block code whose only two codewords are $00 \\cdots 0$ and $11 \\cdots 1$. The repetition code has the following parameters:</p> <ul> <li>Length: $n$</li> <li>Dimension: $k = 1$</li> <li>Redundancy: $m = n - 1$</li> <li>Minimum distance: $d = n$</li> </ul> Notes <ul> <li>Its dual is the single parity-check code.</li> </ul> <p>Parameters:</p> <ul> <li> <code>n</code> (<code>int</code>)         \u2013          <p>The length $n$ of the code. Must be a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.RepetitionCode(5)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 1, 4)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 0, 0, 0],\n       [1, 0, 1, 0, 0],\n       [1, 0, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n5\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.RepetitionCode(16)\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([    1,    16,   120,   560,  1820,  4368,  8008, 11440,  6435,\n           0,     0,     0,     0,     0,     0,     0,     0])\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/","title":"komm.RootRaisedCosinePulse","text":"<p>Root-raised-cosine pulse. It is a pulse whose spectrum is given by the square root of the spectrum of the raised cosine pulse with same roll-off factor.</p> <p>The waveform of the root-raised cosine pulse is depicted below for $\\alpha = 0.25$, and for $\\alpha = 0.75$.</p> <p> </p> <p>For more details, see Wikipedia: Root-raised-cosine filter.</p> <p>Parameters:</p> <ul> <li> <code>rolloff</code> (<code>float</code>)         \u2013          <p>The roll-off factor $\\alpha$ of the pulse. Must satisfy $0 \\leq \\alpha \\leq 1$. The default value is <code>1.0</code>.</p> </li> </ul>"},{"location":"ref/RootRaisedCosinePulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the root-raised-cosine pulse, it is given by $$   p(t) = \\frac{\\sin ( 2 \\pi f_1 t ) + 4 \\alpha t \\cos ( 2 \\pi f_2 t )}{\\pi t ( 1 - (4 \\alpha t)^2 )}, $$ where $\\alpha$ is the roll-off factor, $f_1 = (1 - \\alpha) / 2$, and $f_2 = (1 + \\alpha) / 2$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([-0.064,  0.238,  0.622,  0.943,  1.068,  0.943,  0.622,  0.238,\n       -0.064])\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )).round(3)\narray([0.   , 0.   , 0.707, 1.   , 1.   , 1.   , 0.707, 0.   , 0.   ])\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the root-raised-cosine pulse, it is given by $$     R(\\tau) = \\sinc(\\tau) \\frac{\\cos(\\pi \\alpha \\tau)}{1 - (2 \\alpha \\tau)^2}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.29 , 0.627, 0.897, 1.   , 0.897, 0.627, 0.29 , 0.   ])\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the root-raised-cosine pulse, it is given by $$     S(f) = \\begin{cases}         1, &amp; |f| \\leq f_1, \\\\[1ex]         \\dfrac{1}{2} \\left( 1 + \\cos \\left( \\pi \\dfrac{|f| - f_1}{f_2 - f_1}\\right) \\right), &amp; f_1 \\leq |f| \\leq f_2, \\\\[1ex]         0, &amp; \\text{otherwise}.     \\end{cases} $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0. , 0. , 0.5, 1. , 1. , 1. , 0.5, 0. , 0. ])\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the root-raised-cosine pulse, the support is given by $(-\\infty, \\infty)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.support\n(-inf, inf)\n</code></pre>"},{"location":"ref/RootRaisedCosinePulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.RootRaisedCosinePulse(rolloff=0.25)\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1)).round(3)\narray([-0.064,  0.238,  0.622,  0.943,  1.068,  0.943,  0.622,  0.238,\n       -0.064])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-16, 16)).shape\n(129,)\n</code></pre>"},{"location":"ref/SCDecoder/","title":"komm.SCDecoder","text":"<p>Successive cancellation decoder for Polar codes.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>PolarCode</code>)         \u2013          <p>The Polar code to be used for decoding.</p> </li> <li> <code>output_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the output. Either <code>'hard'</code> or <code>'soft'</code>. Default is <code>'soft'</code>.</p> </li> </ul> Notes <ul> <li>Input type: <code>soft</code> (L-values).</li> <li>Output type: <code>hard</code> (bits) or <code>soft</code> (L-values).</li> </ul>"},{"location":"ref/SCDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.PolarCode(3, [0, 1, 2, 4])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.SCDecoder(code)\n&gt;&gt;&gt; decoder.decode([1, -4, -3, 2, -2, 3, 4, -1])\narray([ -6.84595089,  -5.96379094,  -9.30685282, -20.        ])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.SCDecoder(code, output_type=\"hard\")\n&gt;&gt;&gt; decoder.decode([1, -4, -3, 2, -2, 3, 4, -1])\narray([1, 1, 1, 1])\n</code></pre>"},{"location":"ref/ScalarQuantizer/","title":"komm.ScalarQuantizer","text":"<p>General scalar quantizer. It is defined by a list of levels, $v_0, v_1, \\ldots, v_{L-1}$, and a list of thresholds, $t_0, t_1, \\ldots, t_L$, satisfying $$     -\\infty = t_0 &lt; v_0 &lt; t_1 &lt; v_1 &lt; \\cdots &lt; t_{L - 1} &lt; v_{L - 1} &lt; t_L = +\\infty. $$ Given an input $x \\in \\mathbb{R}$, the output of the quantizer is given by $y = v_i$ if and only if $t_i \\leq x &lt; t_{i+1}$, where $i \\in [0:L)$. For more details, see Say06, Ch. 9.</p> <p>Parameters:</p> <ul> <li> <code>levels</code> (<code>ArrayLike</code>)         \u2013          <p>The quantizer levels $v_0, v_1, \\ldots, v_{L-1}$. It should be a list floats of length $L$.</p> </li> <li> <code>thresholds</code> (<code>ArrayLike</code>)         \u2013          <p>The quantizer finite thresholds $t_1, t_2, \\ldots, t_{L-1}$. It should be a list of floats of length $L - 1$.</p> </li> </ul> <p>Examples:</p> <p>The $5$-level scalar quantizer whose characteristic (input \u00d7 output) curve is depicted in the figure below has levels $$     v_0 = -2, ~ v_1 = -1, ~ v_2 = 0, ~ v_3 = 1, ~ v_4 = 2, $$ and thresholds $$     t_0 = -\\infty, ~ t_1 = -1.5, ~ t_2 = -0.3, ~ t_3 = 0.8, ~ t_4 = 1.4, ~ t_5 = \\infty. $$</p> <p></p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n</code></pre>"},{"location":"ref/ScalarQuantizer/#levels","title":"<code>levels</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer levels $v_0, v_1, \\ldots, v_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n&gt;&gt;&gt; quantizer.levels\narray([-2., -1.,  0.,  1.,  2.])\n</code></pre>"},{"location":"ref/ScalarQuantizer/#thresholds","title":"<code>thresholds</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer finite thresholds $t_1, t_2, \\ldots, t_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n&gt;&gt;&gt; quantizer.thresholds\narray([-1.5, -0.3,  0.8,  1.4])\n</code></pre>"},{"location":"ref/ScalarQuantizer/#mean_squared_error","title":"<code>mean_squared_error()</code>","text":"<p>Computes the mean squared (quantization) error (MSE) of the quantizer for a given input probability density function (pdf). It is defined as $$     \\mse = \\int_{-\\infty}^{\\infty} (y - x)^2 f_X(x) \\, dx $$ where $y$ is the quantized signal and $f_X(x)$ is the pdf of the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input_pdf</code> (<code>Callable[[NDArray[floating]], NDArray[floating]]</code>)         \u2013          <p>The probability density function $f_X(x)$ of the input signal.</p> </li> <li> <code>input_range</code> (<code>tuple[float, float]</code>)         \u2013          <p>The range $(x_\\mathrm{min}, x_\\mathrm{max})$ of the input signal.</p> </li> <li> <code>points_per_interval</code> (<code>int</code>)         \u2013          <p>The number of points per interval for numerical integration (default: 4096).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mse</code>  (<code>float</code>)          \u2013          <p>The mean square quantization error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer.mean_squared_error(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n... )\n0.13598089455499335\n</code></pre>"},{"location":"ref/ScalarQuantizer/#digitize","title":"<code>digitize()</code>","text":"<p>Returns the quantization indices for the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be digitized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The integer indices of the quantization levels for each input sample.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n&gt;&gt;&gt; quantizer.digitize([-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5])\narray([0, 0, 1, 1, 1, 2, 2, 3, 4, 4, 4])\n</code></pre>"},{"location":"ref/ScalarQuantizer/#quantize","title":"<code>quantize()</code>","text":"<p>Quantizes the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be quantized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The quantized signal $y$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.ScalarQuantizer(\n...     levels=[-2.0, -1.0, 0.0, 1.0, 2.0],\n...     thresholds=[-1.5, -0.3, 0.8, 1.4],\n... )\n&gt;&gt;&gt; quantizer.quantize([-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5])\narray([-2., -2., -1., -1., -1.,  0.,  0.,  1.,  2.,  2.,  2.])\n</code></pre>"},{"location":"ref/ShannonCode/","title":"komm.ShannonCode","text":"<p>Binary Shannon code. For a given pmf $p$ over $\\mathcal{X}$, it is a fixed-to-variable length code in which the length of the codeword $\\Enc(\\mathbf{x})$ associated with a source word $\\mathbf{x} \\in \\mathcal{X}^k$ is given by $$     \\ell(\\mathbf{x}) = \\left\\lceil \\log_2 \\frac{1}{p(\\mathbf{x})} \\right\\rceil. $$ This function implements the lexicographic order assignment as described in Wikipedia: Shannon\u2013Fano coding.</p> Notes <p>Shannon codes are always prefix-free (hence uniquely decodable).</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> <li> <code>source_block_size</code> (<code>int</code>)         \u2013          <p>The source block size $k$. The default value is $k = 1$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pmf = [0.8, 0.1, 0.1]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.ShannonCode(pmf)\n&gt;&gt;&gt; code.enc_mapping\n{(0,): (0,),\n (1,): (1, 0, 0, 0),\n (2,): (1, 0, 0, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(1.6)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.ShannonCode(pmf, 2)\n&gt;&gt;&gt; code.enc_mapping\n{(0, 0): (0,),\n (0, 1): (1, 0, 0, 0),\n (0, 2): (1, 0, 0, 1),\n (1, 0): (1, 0, 1, 0),\n (1, 1): (1, 1, 0, 0, 0, 0, 0),\n (1, 2): (1, 1, 0, 0, 0, 0, 1),\n (2, 0): (1, 0, 1, 1),\n (2, 1): (1, 1, 0, 0, 0, 1, 0),\n (2, 2): (1, 1, 0, 0, 0, 1, 1)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(1.1)\n</code></pre>"},{"location":"ref/SimplexCode/","title":"komm.SimplexCode","text":"<p>Simplex (maximum-length) code. For a given parameter $\\kappa \\geq 2$, it is the linear block code with generator matrix whose columns are all the $2^\\kappa - 1$ nonzero binary $\\kappa$-tuples. The simplex code (also known as maximum-length code) has the following parameters:</p> <ul> <li>Length: $n = 2^\\kappa - 1$</li> <li>Dimension: $k = \\kappa$</li> <li>Redundancy: $m = 2^\\kappa - \\kappa - 1$</li> <li>Minimum distance: $d = 2^{\\kappa - 1}$</li> </ul> <p>In its extended version, the simplex code has the following parameters:</p> <ul> <li>Length: $n = 2^\\kappa$</li> <li>Dimension: $k = \\kappa + 1$</li> <li>Redundancy: $m = 2^\\kappa - \\kappa - 1$</li> <li>Minimum distance: $d = 2^{\\kappa - 1}$</li> </ul> Notes <ul> <li>For $\\kappa = 2$ it reduces to the single parity-check code of length $3$.</li> <li>Its dual is the Hamming code.</li> <li>Simplex codes are constant-weight codes.</li> </ul> <p>Parameters:</p> <ul> <li> <code>kappa</code> (<code>int</code>)         \u2013          <p>The parameter $\\kappa$ of the code. Must satisfy $\\kappa \\geq 2$.</p> </li> <li> <code>extended</code> (<code>bool</code>)         \u2013          <p>Whether to use the extended version of the Simplex code. Default is <code>False</code>.</p> </li> </ul> <p>This class represents the code in systematic form, with the information set on the left.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SimplexCode(3)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(7, 3, 4)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1, 0, 1],\n       [0, 1, 0, 1, 0, 1, 1],\n       [0, 0, 1, 0, 1, 1, 1]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 0, 1, 0, 0, 0],\n       [1, 0, 1, 0, 1, 0, 0],\n       [0, 1, 1, 0, 0, 1, 0],\n       [1, 1, 1, 0, 0, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n4\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.SimplexCode(3, extended=True)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(8, 4, 4)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 0, 1, 1, 0, 1],\n       [0, 1, 0, 0, 1, 0, 1, 1],\n       [0, 0, 1, 0, 0, 1, 1, 1],\n       [0, 0, 0, 1, 1, 1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 0, 1, 1, 0, 0, 0],\n       [1, 0, 1, 1, 0, 1, 0, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0],\n       [1, 1, 1, 0, 0, 0, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n4\n</code></pre>"},{"location":"ref/SincPulse/","title":"komm.SincPulse","text":"<p>Sinc pulse. It is a pulse with waveform given by $$     p(t) = \\frac{\\sin(\\pi t)}{\\pi t} = \\sinc(t). $$</p> <p>The waveform of the sinc pulse is depicted below.</p> <p></p> <p>Parameters:</p> <p>(No parameters)</p>"},{"location":"ref/SincPulse/#waveform","title":"<code>waveform()</code>","text":"<p>The waveform $p(t)$ of the pulse.</p> <p>For the sinc pulse, it is given by $$     p(t) = \\sinc(t). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; pulse.waveform(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.3  , 0.637, 0.9  , 1.   , 0.9  , 0.637, 0.3  , 0.   ])\n</code></pre>"},{"location":"ref/SincPulse/#spectrum","title":"<code>spectrum()</code>","text":"<p>The spectrum $\\hat{p}(f)$ of the pulse.</p> <p>For the sinc pulse, it is given by $$     \\hat{p}(f) = \\rect(f). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; np.abs(pulse.spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ))\narray([0., 0., 1., 1., 1., 1., 0., 0., 0.])\n</code></pre>"},{"location":"ref/SincPulse/#autocorrelation","title":"<code>autocorrelation()</code>","text":"<p>The autocorrelation function $R(\\tau)$ of the pulse.</p> <p>For the sinc pulse, it is given by $$     R(\\tau) = \\sinc(\\tau). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; pulse.autocorrelation(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... ).round(3)\narray([0.   , 0.3  , 0.637, 0.9  , 1.   , 0.9  , 0.637, 0.3  , 0.   ])\n</code></pre>"},{"location":"ref/SincPulse/#energy_density_spectrum","title":"<code>energy_density_spectrum()</code>","text":"<p>The energy density spectrum $S(f)$ of the pulse.</p> <p>For the sinc pulse, it is given by $$     S(f) = \\rect(f). $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; pulse.energy_density_spectrum(\n...     [-1.0, -0.75, -0.5, -0.25, 0.0, 0.25, 0.5, 0.75, 1.0],\n... )\narray([0., 0., 1., 1., 1., 1., 0., 0., 0.])\n</code></pre>"},{"location":"ref/SincPulse/#support","title":"<code>support</code><code>  tuple[float, float] </code>  <code>cached</code> <code>property</code>","text":"<p>The support of the pulse waveform $p(t)$, defined as the interval $[a, b]$ where $p(t)$ is non-zero.</p> <p>For the sinc pulse, the support is given by $(-\\infty, \\infty)$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; pulse.support\n(-inf, inf)\n</code></pre>"},{"location":"ref/SincPulse/#taps","title":"<code>taps()</code>","text":"<p>Returns the FIR taps of the pulse.</p> <p>Parameters:</p> <ul> <li> <code>samples_per_symbol</code> (<code>int</code>)         \u2013          <p>The number of samples per symbol.</p> </li> <li> <code>span</code> (<code>tuple[int, int] | None</code>)         \u2013          <p>The time span to consider for the taps. This parameter is optional for pulses with finite support (defaults to $[0, 1]$), but required for pulses with infinite support.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pulse = komm.SincPulse()\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-1, 1)).round(3)\narray([0.   , 0.3  , 0.637, 0.9  , 1.   , 0.9  , 0.637, 0.3  , 0.   ])\n&gt;&gt;&gt; pulse.taps(samples_per_symbol=4, span=(-16, 16)).shape\n(129,)\n</code></pre>"},{"location":"ref/SingleParityCheckCode/","title":"komm.SingleParityCheckCode","text":"<p>Single parity-check code. For a given length $n \\geq 1$, it is the linear block code whose codewords are obtained by extending $n - 1$ information bits with a single parity-check bit. The repetition code has the following parameters:</p> <ul> <li>Length: $n$.</li> <li>Dimension: $k = n - 1$.</li> <li>Redundancy: $m = 1$.</li> <li>Minimum distance: $d = 2$.</li> </ul> Notes <ul> <li>Its dual is the repetition code.</li> </ul> <p>Parameters:</p> <ul> <li> <code>n</code> (<code>int</code>)         \u2013          <p>The length $n$ of the code. Must be a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SingleParityCheckCode(5)\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 4, 1)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 0, 1],\n       [0, 1, 0, 0, 1],\n       [0, 0, 1, 0, 1],\n       [0, 0, 0, 1, 1]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 1, 1, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n2\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.SingleParityCheckCode(16)\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([    1,     0,   120,     0,  1820,     0,  8008,     0, 12870,\n           0,  8008,     0,  1820,     0,   120,     0,     1])\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</code></pre>"},{"location":"ref/SlepianArray/","title":"komm.SlepianArray","text":"<p>Slepian array (standard array) for a linear block code. It is a table with $2^m$ rows and $2^k$ columns, where $m$ is the redundancy, and $k$ is the dimension of the code. Each row corresponds to a coset of the group of codewords, in which:</p> <ul> <li> <p>The first row is the group of codewords itself.</p> </li> <li> <p>The first column contains coset leaders (i.e., elements of minimal weight in its coset).</p> </li> </ul> <p>In this implementation:</p> <ul> <li> <p>A row's index $i$ corresponds to the $m$-bit syndrome obtained by expressing $i$ in binary (LSB-first).</p> </li> <li> <p>A column's index $j$ corresponds to the $k$-bit message obtained by expressing $j$ in binary (LSB-first).</p> </li> </ul> <p>For more details, see LC04, Sec. 3.5.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>BlockCode</code>)         \u2013          <p>The linear block code for which the Slepian array is generated.</p> </li> </ul>"},{"location":"ref/SlepianArray/#entry","title":"<code>entry()</code>","text":"<p>The entry at the $i$-th row and $j$-th column of the Slepian array.</p> <p>Parameters:</p> <ul> <li> <code>i</code> (<code>int</code>)         \u2013          <p>The index of the row.</p> </li> <li> <code>j</code> (<code>int</code>)         \u2013          <p>The index of the column.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[[1, 0, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0]])\n&gt;&gt;&gt; sa = komm.SlepianArray(code)\n&gt;&gt;&gt; binlist2str = lambda binlist: \"\".join(str(bit) for bit in binlist)\n&gt;&gt;&gt; m, k = code.redundancy, code.dimension\n&gt;&gt;&gt; for i in range(2**m):\n...     for j in range(2**k):\n...         print(binlist2str(sa.entry(i, j)), end=\" \")\n...     print()\n000000 100011 010101 110110 001110 101101 011011 111000\n000100 100111 010001 110010 001010 101001 011111 111100\n000010 100001 010111 110100 001100 101111 011001 111010\n001000 101011 011101 111110 000110 100101 010011 110000\n000001 100010 010100 110111 001111 101100 011010 111001\n010000 110011 000101 100110 011110 111101 001011 101000\n100000 000011 110101 010110 101110 001101 111011 011000\n100100 000111 110001 010010 101010 001001 111111 011100\n</code></pre>"},{"location":"ref/SlepianArray/#row","title":"<code>row()</code>","text":"<p>The $i$-th row of the Slepian array.</p> <p>Parameters:</p> <ul> <li> <code>i</code> (<code>int</code>)         \u2013          <p>The index of the row.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[[1, 0, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0]])\n&gt;&gt;&gt; sa = komm.SlepianArray(code)\n&gt;&gt;&gt; sa.row(0)  # The codewords\narray([[0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 1, 1],\n       [0, 1, 0, 1, 0, 1],\n       [1, 1, 0, 1, 1, 0],\n       [0, 0, 1, 1, 1, 0],\n       [1, 0, 1, 1, 0, 1],\n       [0, 1, 1, 0, 1, 1],\n       [1, 1, 1, 0, 0, 0]])\n</code></pre>"},{"location":"ref/SlepianArray/#col","title":"<code>col()</code>","text":"<p>The $j$-th column of the Slepian array.</p> <p>Parameters:</p> <ul> <li> <code>j</code> (<code>int</code>)         \u2013          <p>The index of the column.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.BlockCode(generator_matrix=[[1, 0, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 0]])\n&gt;&gt;&gt; sa = komm.SlepianArray(code)\n&gt;&gt;&gt; sa.col(0)  # The coset leaders\narray([[0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0],\n       [0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 1, 0, 0]])\n</code></pre>"},{"location":"ref/SyndromeTableDecoder/","title":"komm.SyndromeTableDecoder","text":"<p>Syndrome table decoder for general block codes. This decoder implements syndrome-based hard-decision decoding using a precomputed table of coset leaders.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>BlockCode</code>)         \u2013          <p>The block code to be used for decoding.</p> </li> </ul> Notes <ul> <li>Input type: <code>hard</code> (bits).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/SyndromeTableDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.HammingCode(3)\n&gt;&gt;&gt; decoder = komm.SyndromeTableDecoder(code)\n&gt;&gt;&gt; decoder.decode([[1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0]])\narray([[1, 1, 0, 0],\n       [1, 0, 1, 1]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/","title":"komm.SystematicBlockCode","text":"<p>Systematic linear block code. A systematic linear block code is a linear block code in which the information bits can be found in predefined positions in the codeword, called the information set $\\mathcal{K}$, which is a $k$-sublist of $[0 : n)$; the remaining positions are called the parity set $\\mathcal{M}$, which is a $m$-sublist of $[0 : n)$. In this case, the generator matrix then has the property that the columns indexed by $\\mathcal{K}$ are equal to $I_k$, and the columns indexed by $\\mathcal{M}$ are equal to $P$. The check matrix has the property that the columns indexed by $\\mathcal{M}$ are equal to $I_m$, and the columns indexed by $\\mathcal{K}$ are equal to $P^\\transpose$. The matrix $P \\in \\mathbb{B}^{k \\times m}$ is called the parity submatrix of the code.</p> <p>The constructor expects the parity submatrix and the information set.</p> <p>Parameters:</p> <ul> <li> <code>parity_submatrix</code> (<code>ArrayLike</code>)         \u2013          <p>The parity submatrix $P$ the code, which is a $k \\times m$ binary matrix.</p> </li> <li> <code>information_set</code> (<code>Literal['left', 'right'] | ArrayLike</code>)         \u2013          <p>Either an array containing the indices of the information positions, which must be a $k$-sublist of $[0 : n)$, or one of the strings <code>'left'</code> or <code>'right'</code>. The default value is <code>'left'</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 2, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(\n...     parity_submatrix=[[0, 1, 1], [1, 1, 0]],\n...     information_set='right',\n... )\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(5, 2, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[0, 1, 1, 1, 0],\n       [1, 1, 0, 0, 1]])\n&gt;&gt;&gt; code.check_matrix\narray([[1, 0, 0, 0, 1],\n       [0, 1, 0, 1, 1],\n       [0, 0, 1, 1, 0]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#length","title":"<code>length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The length $n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.length\n5\n</code></pre>"},{"location":"ref/SystematicBlockCode/#dimension","title":"<code>dimension</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The dimension $k$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.dimension\n2\n</code></pre>"},{"location":"ref/SystematicBlockCode/#redundancy","title":"<code>redundancy</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The redundancy $m$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.redundancy\n3\n</code></pre>"},{"location":"ref/SystematicBlockCode/#rate","title":"<code>rate</code><code>  float </code>  <code>cached</code> <code>property</code>","text":"<p>The rate $R = k/n$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.rate\n0.4\n</code></pre>"},{"location":"ref/SystematicBlockCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The generator matrix $G \\in \\mathbb{B}^{k \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#check_matrix","title":"<code>check_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The check matrix $H \\in \\mathbb{B}^{m \\times n}$ of the code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.check_matrix\narray([[0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 0, 0, 0, 1]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#encode","title":"<code>encode()</code>","text":"<p>Applies the encoding mapping $\\Enc : \\mathbb{B}^k \\to \\mathbb{B}^n$ of the code. This method takes one or more sequences of messages and returns their corresponding codeword sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $k$, or a multidimensional array where the last dimension is a multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension expanded from $bk$ to $bn$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.encode</code> for examples. </p>"},{"location":"ref/SystematicBlockCode/#inverse_encode","title":"<code>inverse_encode()</code>","text":"<p>Applies the inverse encoding partial mapping $\\Enc^{-1} : \\mathbb{B}^n \\rightharpoonup \\mathbb{B}^k$ of the code. This method takes one or more sequences of codewords and returns their corresponding message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input contains any invalid codewords.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.inverse_encode</code> for examples. </p>"},{"location":"ref/SystematicBlockCode/#check","title":"<code>check()</code>","text":"<p>Applies the check mapping $\\mathrm{Chk} : \\mathbb{B}^n \\to \\mathbb{B}^m$ of the code. This method takes one or more sequences of received words and returns their corresponding syndrome sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bm$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <p> See <code>BlockCode.check</code> for examples. </p>"},{"location":"ref/SystematicBlockCode/#codewords","title":"<code>codewords()</code>  <code>cached</code>","text":"<p>Returns the codewords of the code. This is a $2^k \\times n$ matrix whose rows are all the codewords. The codeword in row $i$ corresponds to the message obtained by expressing $i$ in binary with $k$ bits (LSB-first).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.codewords()\narray([[0, 0, 0, 0, 0],\n       [1, 0, 0, 1, 1],\n       [0, 1, 1, 1, 0],\n       [1, 1, 1, 0, 1]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#codeword_weight_distribution","title":"<code>codeword_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the codeword weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of codewords of Hamming weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.codeword_weight_distribution()\narray([1, 0, 0, 2, 1, 0])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#minimum_distance","title":"<code>minimum_distance()</code>  <code>cached</code>","text":"<p>Returns the minimum distance $d$ of the code. This is equal to the minimum Hamming weight of the non-zero codewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre>"},{"location":"ref/SystematicBlockCode/#coset_leaders","title":"<code>coset_leaders()</code>  <code>cached</code>","text":"<p>Returns the coset leaders of the code. This is a $2^m \\times n$ matrix whose rows are all the coset leaders. The coset leader in row $i$ corresponds to the syndrome obtained by expressing $i$ in binary with $m$ bits (LSB-first), and whose Hamming weight is minimal. This may be used as a LUT for syndrome-based decoding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.coset_leaders()\narray([[0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1],\n       [1, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [1, 0, 1, 0, 0]])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#coset_leader_weight_distribution","title":"<code>coset_leader_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the coset leader weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of coset leaders of weight $w$, for $w \\in [0 : n]$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.coset_leader_weight_distribution()\narray([1, 5, 2, 0, 0, 0])\n</code></pre>"},{"location":"ref/SystematicBlockCode/#packing_radius","title":"<code>packing_radius()</code>  <code>cached</code>","text":"<p>Returns the packing radius of the code. This is also called the error-correcting capability of the code, and is equal to $\\lfloor (d - 1) / 2 \\rfloor$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.packing_radius()\n1\n</code></pre>"},{"location":"ref/SystematicBlockCode/#covering_radius","title":"<code>covering_radius()</code>  <code>cached</code>","text":"<p>Returns the covering radius of the code. This is equal to the maximum Hamming weight of the coset leaders.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SystematicBlockCode(parity_submatrix=[[0, 1, 1], [1, 1, 0]])\n&gt;&gt;&gt; code.covering_radius()\n2\n</code></pre>"},{"location":"ref/TerminatedConvolutionalCode/","title":"komm.TerminatedConvolutionalCode","text":"<p>Terminated convolutional code. It is a linear block code obtained by terminating a $(n_0, k_0)$ convolutional code. A total of $h$ information blocks (each containing $k_0$ information bits) is encoded. The dimension of the resulting block code is thus $k = h k_0$; its length depends on the termination mode employed. There are three possible termination modes:</p> <ul> <li> <p>Direct truncation. The encoder always starts at state $0$, and its output ends immediately after the last information block. The encoder may not necessarily end in state $0$. The resulting block code will have length $n = h n_0$.</p> </li> <li> <p>Zero termination. The encoder always starts and ends at state $0$. To achieve this, a sequence of $k \\mu$ tail bits is appended to the information bits, where $\\mu$ is the memory order of the convolutional code. The resulting block code will have length $n = (h + \\mu) n_0$.</p> </li> <li> <p>Tail-biting. The encoder always starts and ends at the same state. To achieve this, the initial state of the encoder is chosen as a function of the information bits. The resulting block code will have length $n = h n_0$.</p> </li> </ul> <p>For more details, see LC04, Sec. 12.7 and WBR01.</p> <p>Parameters:</p> <ul> <li> <code>convolutional_code</code> (<code>ConvolutionalCode</code>)         \u2013          <p>The convolutional code to be terminated.</p> </li> <li> <code>num_blocks</code> (<code>int</code>)         \u2013          <p>The number $h$ of information blocks.</p> </li> <li> <code>mode</code> (<code>TerminationMode</code>)         \u2013          <p>The termination mode. It must be one of <code>'direct-truncation'</code> | <code>'zero-termination'</code> | <code>'tail-biting'</code>. The default value is <code>'zero-termination'</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.TerminatedConvolutionalCode(\n...     convolutional_code=komm.ConvolutionalCode([[0b1, 0b11]]),\n...     num_blocks=3,\n...     mode='direct-truncation',\n... )\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(6, 3, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 1, 0, 1],\n       [0, 0, 0, 0, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n2\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.TerminatedConvolutionalCode(\n...     convolutional_code=komm.ConvolutionalCode([[0b1, 0b11]]),\n...     num_blocks=3,\n...     mode='zero-termination',\n... )\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(8, 3, 5)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.TerminatedConvolutionalCode(\n...     convolutional_code=komm.ConvolutionalCode([[0b1, 0b11]]),\n...     num_blocks=3,\n...     mode='tail-biting',\n... )\n&gt;&gt;&gt; (code.length, code.dimension, code.redundancy)\n(6, 3, 3)\n&gt;&gt;&gt; code.generator_matrix\narray([[1, 1, 0, 1, 0, 0],\n       [0, 0, 1, 1, 0, 1],\n       [0, 1, 0, 0, 1, 1]])\n&gt;&gt;&gt; code.minimum_distance()\n3\n</code></pre>"},{"location":"ref/TerminatedConvolutionalCode/#length","title":"<code>length</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The length $n$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#dimension","title":"<code>dimension</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The dimension $k$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#redundancy","title":"<code>redundancy</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The redundancy $m$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#rate","title":"<code>rate</code><code>  float </code>  <code>cached</code> <code>property</code>","text":"<p>The rate $R = k/n$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#generator_matrix","title":"<code>generator_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The generator matrix $G \\in \\mathbb{B}^{k \\times n}$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#check_matrix","title":"<code>check_matrix</code><code>  Array2D[integer] </code>  <code>cached</code> <code>property</code>","text":"<p>The check matrix $H \\in \\mathbb{B}^{m \\times n}$ of the code.</p>"},{"location":"ref/TerminatedConvolutionalCode/#encode","title":"<code>encode()</code>","text":"<p>Applies the encoding mapping $\\Enc : \\mathbb{B}^k \\to \\mathbb{B}^n$ of the code. This method takes one or more sequences of messages and returns their corresponding codeword sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $k$, or a multidimensional array where the last dimension is a multiple of $k$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension expanded from $bk$ to $bn$, where $b$ is a positive integer.</p> </li> </ul>"},{"location":"ref/TerminatedConvolutionalCode/#inverse_encode","title":"<code>inverse_encode()</code>","text":"<p>Applies the inverse encoding partial mapping $\\Enc^{-1} : \\mathbb{B}^n \\rightharpoonup \\mathbb{B}^k$ of the code. This method takes one or more sequences of codewords and returns their corresponding message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input contains any invalid codewords.</p> </li> </ul>"},{"location":"ref/TerminatedConvolutionalCode/#check","title":"<code>check()</code>","text":"<p>Applies the check mapping $\\mathrm{Chk} : \\mathbb{B}^n \\to \\mathbb{B}^m$ of the code. This method takes one or more sequences of received words and returns their corresponding syndrome sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bm$, where $b$ is a positive integer.</p> </li> </ul>"},{"location":"ref/TerminatedConvolutionalCode/#codewords","title":"<code>codewords()</code>  <code>cached</code>","text":"<p>Returns the codewords of the code. This is a $2^k \\times n$ matrix whose rows are all the codewords. The codeword in row $i$ corresponds to the message obtained by expressing $i$ in binary with $k$ bits (LSB-first).</p>"},{"location":"ref/TerminatedConvolutionalCode/#codeword_weight_distribution","title":"<code>codeword_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the codeword weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of codewords of Hamming weight $w$, for $w \\in [0 : n]$.</p>"},{"location":"ref/TerminatedConvolutionalCode/#minimum_distance","title":"<code>minimum_distance()</code>  <code>cached</code>","text":"<p>Returns the minimum distance $d$ of the code. This is equal to the minimum Hamming weight of the non-zero codewords.</p>"},{"location":"ref/TerminatedConvolutionalCode/#coset_leaders","title":"<code>coset_leaders()</code>  <code>cached</code>","text":"<p>Returns the coset leaders of the code. This is a $2^m \\times n$ matrix whose rows are all the coset leaders. The coset leader in row $i$ corresponds to the syndrome obtained by expressing $i$ in binary with $m$ bits (LSB-first), and whose Hamming weight is minimal. This may be used as a LUT for syndrome-based decoding.</p>"},{"location":"ref/TerminatedConvolutionalCode/#coset_leader_weight_distribution","title":"<code>coset_leader_weight_distribution()</code>  <code>cached</code>","text":"<p>Returns the coset leader weight distribution of the code. This is an array of shape $(n + 1)$ in which element in position $w$ is equal to the number of coset leaders of weight $w$, for $w \\in [0 : n]$.</p>"},{"location":"ref/TerminatedConvolutionalCode/#packing_radius","title":"<code>packing_radius()</code>  <code>cached</code>","text":"<p>Returns the packing radius of the code. This is also called the error-correcting capability of the code, and is equal to $\\lfloor (d - 1) / 2 \\rfloor$.</p>"},{"location":"ref/TerminatedConvolutionalCode/#covering_radius","title":"<code>covering_radius()</code>  <code>cached</code>","text":"<p>Returns the covering radius of the code. This is equal to the maximum Hamming weight of the coset leaders.</p>"},{"location":"ref/TunstallCode/","title":"komm.TunstallCode","text":"<p>Binary Tunstall code. It is an optimal (minimal expected rate) variable-to-fixed length code for a given pmf $p$ over $\\mathcal{X}$. For more details, see Say06, Sec. 3.7.</p> Notes <p>Tunstall codes are always prefix-free (hence uniquely encodable) and fully covering.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> <li> <code>target_block_size</code> (<code>int | None</code>)         \u2013          <p>The target block size $n$. Must satisfy $2^n \\geq |\\mathcal{X}|$. The default value is $n = \\lceil \\log_2 |\\mathcal{X}| \\rceil$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pmf = [0.8, 0.1, 0.1]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.TunstallCode(pmf)\n&gt;&gt;&gt; code.dec_mapping\n{(0, 0): (0,),\n (0, 1): (1,),\n (1, 0): (2,)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(2.0)\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.TunstallCode(pmf, 3)\n&gt;&gt;&gt; code.dec_mapping\n{(0, 0, 0): (0, 0, 0),\n (0, 0, 1): (0, 0, 1),\n (0, 1, 0): (0, 0, 2),\n (0, 1, 1): (0, 1),\n (1, 0, 0): (0, 2),\n (1, 0, 1): (1,),\n (1, 1, 0): (2,)}\n&gt;&gt;&gt; code.rate(pmf)\nnp.float64(1.2295081967213108)\n</code></pre>"},{"location":"ref/UnaryCode/","title":"komm.UnaryCode","text":"<p>Unary code. It is an integer code. For the definition of this code, see Wikipedia: Unary coding.</p>"},{"location":"ref/UnaryCode/#encode","title":"<code>encode()</code>","text":"<p>Encode the input integer array.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input integer array. It must be a one-dimensional array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of bits corresponding to the input integer array.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.UnaryCode()\n&gt;&gt;&gt; code.encode([4, 1, 3])\narray([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0])\n</code></pre>"},{"location":"ref/UnaryCode/#decode","title":"<code>decode()</code>","text":"<p>Decode the input bit array.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input bit array. It must be a one-dimensional array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The sequence of integers corresponding to the input bit array.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.UnaryCode()\n&gt;&gt;&gt; code.decode([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0])\narray([4, 1, 3])\n</code></pre>"},{"location":"ref/UniformQuantizer/","title":"komm.UniformQuantizer","text":"<p>Uniform scalar quantizer. It is a scalar quantizer in which the separation between levels is a constant $\\Delta$, called the quantization step, and the thresholds are the mid-point between adjacent levels. More precisely, the levels are given by $$     v_i = (i - (L - 1)/2 + \\theta) \\Delta, \\qquad i \\in [0 : L), $$ and the finite thresholds are given by $$     t_i = \\frac{v_{i-1} + v_i}{2}, \\qquad i \\in [1 : L). $$ For more details, see Say06, Sec. 9.4.</p> <p>Parameters:</p> <ul> <li> <code>num_levels</code> (<code>int</code>)         \u2013          <p>The number of quantization levels $L$. It must be greater than $1$.</p> </li> <li> <code>step</code> (<code>float</code>)         \u2013          <p>The quantization step $\\Delta$. It must be a positive number.</p> </li> <li> <code>offset</code> (<code>float</code>)         \u2013          <p>The offset $\\theta$ of the quantizer. The default value is $0$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=4, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-0.75, -0.25,  0.25,  0.75])\n&gt;&gt;&gt; quantizer.thresholds\narray([-0.5,  0. ,  0.5])\n</code></pre> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=5, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-1. , -0.5,  0. ,  0.5,  1. ])\n&gt;&gt;&gt; quantizer.thresholds\narray([-0.75, -0.25,  0.25,  0.75])\n</code></pre>"},{"location":"ref/UniformQuantizer/#mid_riser","title":"<code>mid_riser()</code>  <code>classmethod</code>","text":"<p>Constructs a mid-riser uniform quantizer. In a mid-riser quantizer, $0$ is always a threshold. It is a symmetric quantizer when $L$ is even.</p> <p>Parameters:</p> <ul> <li> <code>num_levels</code> (<code>int</code>)         \u2013          <p>The number of quantization levels $L$. It must be greater than $1$.</p> </li> <li> <code>step</code> (<code>float</code>)         \u2013          <p>The quantization step $\\Delta$. It must be a positive number.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer.mid_riser(num_levels=4, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-0.75, -0.25,  0.25,  0.75])\n&gt;&gt;&gt; quantizer.thresholds\narray([-0.5,  0. ,  0.5])\n</code></pre> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer.mid_riser(num_levels=5, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-1.25, -0.75, -0.25,  0.25,  0.75])\n&gt;&gt;&gt; quantizer.thresholds\narray([-1. , -0.5,  0. ,  0.5])\n</code></pre>"},{"location":"ref/UniformQuantizer/#mid_tread","title":"<code>mid_tread()</code>  <code>classmethod</code>","text":"<p>Constructs a mid-tread uniform quantizer. In a mid-tread quantizer, $0$ is always a level. It is a symmetric quantizer when $L$ is odd.</p> <p>Parameters:</p> <ul> <li> <code>num_levels</code> (<code>int</code>)         \u2013          <p>The number of quantization levels $L$. It must be greater than $1$.</p> </li> <li> <code>step</code> (<code>float</code>)         \u2013          <p>The quantization step $\\Delta$. It must be a positive number.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer.mid_tread(num_levels=4, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-1. , -0.5,  0. ,  0.5])\n&gt;&gt;&gt; quantizer.thresholds\narray([-0.75, -0.25,  0.25])\n</code></pre> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer.mid_tread(num_levels=5, step=0.5)\n&gt;&gt;&gt; quantizer.levels\narray([-1. , -0.5,  0. ,  0.5,  1. ])\n&gt;&gt;&gt; quantizer.thresholds\narray([-0.75, -0.25,  0.25,  0.75])\n</code></pre>"},{"location":"ref/UniformQuantizer/#levels","title":"<code>levels</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer levels $v_0, v_1, \\ldots, v_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=8, step=3.0)\n&gt;&gt;&gt; quantizer.levels\narray([-10.5,  -7.5,  -4.5,  -1.5,   1.5,   4.5,   7.5,  10.5])\n</code></pre> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=5, step=2.0)\n&gt;&gt;&gt; quantizer.levels\narray([-4., -2.,  0.,  2.,  4.])\n</code></pre>"},{"location":"ref/UniformQuantizer/#thresholds","title":"<code>thresholds</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The quantizer finite thresholds $t_1, t_2, \\ldots, t_{L-1}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=8, step=3.0)\n&gt;&gt;&gt; quantizer.thresholds\narray([-9., -6., -3.,  0.,  3.,  6.,  9.])\n</code></pre> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=5, step=2.0)\n&gt;&gt;&gt; quantizer.thresholds\narray([-3., -1.,  1.,  3.])\n</code></pre>"},{"location":"ref/UniformQuantizer/#mean_squared_error","title":"<code>mean_squared_error()</code>","text":"<p>Computes the mean squared (quantization) error (MSE) of the quantizer for a given input probability density function (pdf). It is defined as $$     \\mse = \\int_{-\\infty}^{\\infty} (y - x)^2 f_X(x) \\, dx $$ where $y$ is the quantized signal and $f_X(x)$ is the pdf of the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input_pdf</code> (<code>Callable[[NDArray[floating]], NDArray[floating]]</code>)         \u2013          <p>The probability density function $f_X(x)$ of the input signal.</p> </li> <li> <code>input_range</code> (<code>tuple[float, float]</code>)         \u2013          <p>The range $(x_\\mathrm{min}, x_\\mathrm{max})$ of the input signal.</p> </li> <li> <code>points_per_interval</code> (<code>int</code>)         \u2013          <p>The number of points per interval for numerical integration (default: 4096).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>mse</code>  (<code>float</code>)          \u2013          <p>The mean square quantization error.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=4, step=2.0)\n&gt;&gt;&gt; gaussian_pdf = lambda x: 1/np.sqrt(2*np.pi) * np.exp(-x**2/2)\n&gt;&gt;&gt; quantizer.mean_squared_error(\n...     input_pdf=gaussian_pdf,\n...     input_range=(-5, 5),\n... )\n0.3363025489037716\n&gt;&gt;&gt; uniform_pdf = lambda x: 1/8 * (np.abs(x) &lt;= 4)\n&gt;&gt;&gt; quantizer.mean_squared_error(\n...     input_pdf=uniform_pdf,\n...     input_range=(-4, 4),\n... )\n0.3333333730891729\n&gt;&gt;&gt; quantizer.step**2 / 12\n0.3333333333333333\n</code></pre>"},{"location":"ref/UniformQuantizer/#digitize","title":"<code>digitize()</code>","text":"<p>Returns the quantization indices for the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be digitized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The integer indices of the quantization levels for each input sample.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=4, step=2.0)\n&gt;&gt;&gt; quantizer.digitize([-2.4,  0.8,  3.2])\narray([0, 2, 3])\n</code></pre>"},{"location":"ref/UniformQuantizer/#quantize","title":"<code>quantize()</code>","text":"<p>Quantizes the input signal.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input signal $x$ to be quantized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The quantized signal $y$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quantizer = komm.UniformQuantizer(num_levels=4, step=2.0)\n&gt;&gt;&gt; quantizer.quantize([-2.4,  0.8,  3.2])\narray([-3.,  1.,  3.])\n</code></pre>"},{"location":"ref/VariableToFixedCode/","title":"komm.VariableToFixedCode","text":"<p>General variable-to-fixed length code. A variable-to-fixed length code with target alphabet $\\mathcal{Y}$, source alphabet $\\mathcal{X}$, and target block size $n$ is defined by a (possibly partial) decoding mapping $\\mathrm{Dec}: \\mathcal{Y}^n \\rightharpoonup \\mathcal{X}^+$, where the domain is the set of all $n$-tuples with entries in $\\mathcal{Y}$, and the co-domain is the set of all finite-length, non-empty tuples with entries in $\\mathcal{X}$. The elements in the image of $\\mathrm{Dec}$ are called sourcewords.</p> Note <p>Here, for simplicity, we assume that the source alphabet is $\\mathcal{X} = [0 : |\\mathcal{X}|)$ and the target alphabet is $\\mathcal{Y} = [0 : |\\mathcal{Y}|)$, where $|\\mathcal{X}| \\geq 2$ and $|\\mathcal{Y}| \\geq 2$ are called the source cardinality and target cardinality, respectively.</p>"},{"location":"ref/VariableToFixedCode/#from_dec_mapping","title":"<code>from_dec_mapping()</code>  <code>classmethod</code>","text":"<p>Constructs a variable-to-fixed length code from the decoding mapping $\\Dec$.</p> <p>Parameters:</p> <ul> <li> <code>dec_mapping</code> (<code>dict[Word, Word]</code>)         \u2013          <p>The decoding mapping $\\Dec$. Must be a dictionary of length at most $|\\mathcal{Y}|^n$ whose keys are $n$-tuples of integers in $\\mathcal{Y}$ and whose values are non-empty tuples of integers in $\\mathcal{X}$.</p> </li> </ul> Notes <p>The target block size $n$ is inferred from the domain of the decoding mapping, and the target and source cardinalities $|\\mathcal{Y}|$ and $|\\mathcal{X}|$ are inferred from the maximum values in the domain and co-domain, respectively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_dec_mapping({\n...     (0, 0): (0, 0, 0),\n...     (0, 1): (0, 0, 1),\n...     (1, 0): (0, 1),\n...     (1, 1): (1,),\n... })\n&gt;&gt;&gt; code.target_cardinality, code.source_cardinality, code.target_block_size\n(2, 2, 2)\n&gt;&gt;&gt; code.sourcewords\n[(0, 0, 0), (0, 0, 1), (0, 1), (1,)]\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_dec_mapping({\n...     (0, 0, 0): (1, ),\n...     (0, 0, 1): (2, ),\n...     (0, 1, 0): (0, 1),\n...     (0, 1, 1): (0, 2),\n...     (1, 0, 0): (0, 0, 0),\n...     (1, 0, 1): (0, 0, 1),\n...     (1, 1, 0): (0, 0, 2),\n... })  # Incomplete mapping\n&gt;&gt;&gt; code.target_cardinality, code.source_cardinality, code.target_block_size\n(2, 3, 3)\n&gt;&gt;&gt; code.sourcewords\n[(1,), (2,), (0, 1), (0, 2), (0, 0, 0), (0, 0, 1), (0, 0, 2)]\n</code></pre>"},{"location":"ref/VariableToFixedCode/#from_sourcewords","title":"<code>from_sourcewords()</code>  <code>classmethod</code>","text":"<p>Constructs a variable-to-fixed length code from the target cardinality $|\\mathcal{Y}|$ and a list of sourcewords.</p> <p>Parameters:</p> <ul> <li> <code>target_cardinality</code> (<code>int</code>)         \u2013          <p>The target cardinality $|\\mathcal{Y}|$. Must be an integer greater than or equal to $2$.</p> </li> <li> <code>sourcewords</code> (<code>list[Word]</code>)         \u2013          <p>The sourcewords of the code. Must be a list of length at most $|\\mathcal{Y}|^n$ containing tuples of integers in $\\mathcal{X}$. The tuple in position $i$ must be equal to $\\mathrm{Dec}(\\mathbf{y})$, where $\\mathbf{y}$ is the $i$-th element in the lexicographic ordering of $\\mathcal{Y}^n$.</p> </li> </ul> Note <p>The target block size $n$ is inferred from the length of the sourcewords, and the source cardinality $|\\mathcal{X}|$ is inferred from the maximum value in the sourcewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(\n...     target_cardinality=2,\n...     sourcewords=[(0, 0, 0), (0, 0, 1), (0, 1), (1,)],\n... )\n&gt;&gt;&gt; code.target_cardinality, code.source_cardinality, code.target_block_size\n(2, 2, 2)\n&gt;&gt;&gt; code.dec_mapping\n{(0, 0): (0, 0, 0),\n (0, 1): (0, 0, 1),\n (1, 0): (0, 1),\n (1, 1): (1,)}\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(\n...     target_cardinality=2,\n...     sourcewords=[\n...         (1,), (2,), (0, 1), (0, 2), (0, 0, 0), (0, 0, 1), (0, 0, 2)\n...     ],\n... )\n&gt;&gt;&gt; code.target_cardinality, code.source_cardinality, code.target_block_size\n(2, 3, 3)\n&gt;&gt;&gt; code.dec_mapping\n{(0, 0, 0): (1,),\n (0, 0, 1): (2,),\n (0, 1, 0): (0, 1),\n (0, 1, 1): (0, 2),\n (1, 0, 0): (0, 0, 0),\n (1, 0, 1): (0, 0, 1),\n (1, 1, 0): (0, 0, 2)}\n</code></pre>"},{"location":"ref/VariableToFixedCode/#target_cardinality","title":"<code>target_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The target cardinality $|\\mathcal{Y}|$ of the code. It is the number of symbols in the target alphabet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.target_cardinality\n2\n</code></pre>"},{"location":"ref/VariableToFixedCode/#source_cardinality","title":"<code>source_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The source cardinality $|\\mathcal{X}|$ of the code. It is the number of symbols in the source alphabet.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.source_cardinality\n2\n</code></pre>"},{"location":"ref/VariableToFixedCode/#target_block_size","title":"<code>target_block_size</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The target block size $n$ of the code. It is the number of symbols in each target block.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.target_block_size\n2\n</code></pre>"},{"location":"ref/VariableToFixedCode/#size","title":"<code>size</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The number of sourcewords in the code. It is less than or equal to $|\\mathcal{Y}|^n$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.size\n3\n</code></pre>"},{"location":"ref/VariableToFixedCode/#dec_mapping","title":"<code>dec_mapping</code><code>  dict[Word, Word] </code>  <code>cached</code> <code>property</code>","text":"<p>The decoding mapping $\\mathrm{Dec}$ of the code. It is a dictionary of length at most $|\\mathcal{Y}|^n$ whose keys are $n$-tuples of integers in $\\mathcal{Y}$ and whose values are the corresponding sourcewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.dec_mapping\n{(0, 0): (0,), (0, 1): (1,), (1, 0): (0, 1)}\n</code></pre>"},{"location":"ref/VariableToFixedCode/#sourcewords","title":"<code>sourcewords</code><code>  list[Word] </code>  <code>cached</code> <code>property</code>","text":"<p>The sourcewords of the code. They correspond to the image of the decoding mapping $\\mathrm{Dec}$.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_dec_mapping({\n...     (0, 0): (0,),\n...     (0, 1): (1,),\n...     (1, 0): (0, 1),\n... })\n&gt;&gt;&gt; code.sourcewords\n[(0,), (1,), (0, 1)]\n</code></pre>"},{"location":"ref/VariableToFixedCode/#is_fully_covering","title":"<code>is_fully_covering()</code>  <code>cached</code>","text":"<p>Returns whether the code is fully covering. A code is fully covering if every possible source sequence has a prefix that is a sourceword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.is_fully_covering()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.is_fully_covering()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0, 0), (0, 1)])\n&gt;&gt;&gt; code.is_fully_covering()  # (1,) is not covered\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (0, 1)])\n&gt;&gt;&gt; code.is_fully_covering()  # (1,) is not covered\nFalse\n</code></pre>"},{"location":"ref/VariableToFixedCode/#is_uniquely_encodable","title":"<code>is_uniquely_encodable()</code>  <code>cached</code>","text":"<p>Returns whether the code is uniquely encodable. A code is uniquely encodable if there is a unique way to parse any concatenation of sourcewords.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.is_uniquely_encodable()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.is_uniquely_encodable()  # 01 can be parsed as 0|1 or 01\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0, 0), (0, 1)])\n&gt;&gt;&gt; code.is_uniquely_encodable()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (0, 1)])\n&gt;&gt;&gt; code.is_uniquely_encodable()\nTrue\n</code></pre>"},{"location":"ref/VariableToFixedCode/#is_prefix_free","title":"<code>is_prefix_free()</code>  <code>cached</code>","text":"<p>Returns whether the code is prefix-free. A code is prefix-free if no sourceword is a prefix of any other sourceword.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1, 0), (1, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0, 0), (0, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (0, 1)])\n&gt;&gt;&gt; code.is_prefix_free()\nFalse\n</code></pre>"},{"location":"ref/VariableToFixedCode/#rate","title":"<code>rate()</code>","text":"<p>Computes the expected rate $R$ of the code, considering a given (first-order) pmf $p$ over $\\mathcal{X}$. This quantity is given by $$     R = \\frac{n}{\\bar{k}}, $$ and $\\bar{k}$ is the expected sourceword length, assuming iid source symbols drawn from $p$. It is measured in $|\\mathcal{Y}|$-ary digits per source symbol.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The pmf $p$ to be considered. It must be a one-dimensional array of floats of size $|\\mathcal{X}|$. The elements must be non-negative and sum to $1$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>rate</code>  (<code>floating</code>)          \u2013          <p>The expected rate $R$ of the code.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.rate([2/3, 1/3])\nnp.float64(1.3846153846153846)\n</code></pre>"},{"location":"ref/VariableToFixedCode/#encode","title":"<code>encode()</code>","text":"<p>Encodes a sequence of source symbols using the code, which must be fully covering and uniquely encodable. When the input sequence ends with symbols that form only a partial match with any sourceword, the encoder will complete this last block using any valid sourceword that starts with these remaining symbols.</p> Warning <p>Encoding for non-prefix-free codes is not implemented yet.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of symbols to be encoded. Must be a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>Array1D[integer]</code>)          \u2013          <p>The sequence of encoded symbols. It is a 1D-array with elements in $\\mathcal{Y}$ with a length that is a multiple of $n$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_dec_mapping({\n...     (0, 0): (0, 0, 0),\n...     (0, 1): (0, 0, 1),\n...     (1, 0): (0, 1),\n...     (1, 1): (1,),\n... })\n</code></pre> <pre><code>&gt;&gt;&gt; code.encode([1, 0, 0, 0])  # Parsed as 1|000\narray([1, 1, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; code.encode([1, 0, 0])  # Incomplete input, completed as 1|000\narray([1, 1, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0, 0), (0, 1)])\n&gt;&gt;&gt; code.encode([1, 0, 0, 0])  # Code is not fully covering\nTraceback (most recent call last):\n...\nValueError: code is not fully covering\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.encode([1, 0, 0, 0])  # Code is not uniquely encodable\nTraceback (most recent call last):\n...\nValueError: code is not uniquely encodable\n</code></pre>"},{"location":"ref/VariableToFixedCode/#decode","title":"<code>decode()</code>","text":"<p>Decodes a sequence of target symbols using the code, which must be fully covering and uniquely encodable.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The sequence of symbols to be decoded. Must be a 1D-array with elements in $\\mathcal{Y}$ and have a length that is a multiple of $n$. Also, the sequence must be a concatenation of target words (i.e., the output of the <code>encode</code> method).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array1D[integer]</code>         \u2013          <p>The sequence of decoded symbols. It is a 1D-array with elements in $\\mathcal{X}$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_dec_mapping({\n...     (0, 0): (0,),\n...     (0, 1): (1,),\n...     (1, 0): (2,),\n... })\n&gt;&gt;&gt; code.decode([0, 0, 1, 0])\narray([0, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; code.decode([1, 1, 0, 0, 1])  # Not a multiple of target block size\nTraceback (most recent call last):\n...\nValueError: length of input must be a multiple of block size 2 (got 5)\n</code></pre> <pre><code>&gt;&gt;&gt; code.decode([0, 0, 1, 1])  # 11 is not a valid target word\nTraceback (most recent call last):\n...\nValueError: input contains invalid word\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0, 0), (0, 1)])\n&gt;&gt;&gt; code.decode([0, 0, 0, 1])  # Code is not fully covering\nTraceback (most recent call last):\n...\nValueError: code is not fully covering\n</code></pre> <pre><code>&gt;&gt;&gt; code = komm.VariableToFixedCode.from_sourcewords(2, [(0,), (1,), (0, 1)])\n&gt;&gt;&gt; code.decode([0, 0, 0, 1])  # Code is not uniquely encodable\nTraceback (most recent call last):\n...\nValueError: code is not uniquely encodable\n</code></pre>"},{"location":"ref/ViterbiDecoder/","title":"komm.ViterbiDecoder","text":"<p>Viterbi decoder for terminated convolutional codes. For more details, see LC04, Sec. 12.1.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>TerminatedConvolutionalCode</code>)         \u2013          <p>The terminated convolutional code to be used for decoding.</p> </li> <li> <code>input_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the input. Either <code>'hard'</code> or <code>'soft'</code>. Default is <code>'hard'</code>.</p> </li> </ul> Notes <ul> <li>Input type: <code>hard</code> (bits) or <code>soft</code> (either L-values or channel outputs).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/ViterbiDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.TerminatedConvolutionalCode(\n...     convolutional_code=komm.ConvolutionalCode([[0b111, 0b101]]),\n...     num_blocks=4,\n...     mode=\"direct-truncation\",\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ViterbiDecoder(code, input_type=\"hard\")\n&gt;&gt;&gt; decoder.decode([1, 1, 1, 1, 1, 0, 0, 0])\narray([1, 0, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; decoder = komm.ViterbiDecoder(code, input_type=\"soft\")\n&gt;&gt;&gt; decoder.decode([-0.7, -0.5, -0.8, -0.6, -1.1, +0.4, +0.9, +0.8])\narray([1, 0, 0, 0])\n</code></pre>"},{"location":"ref/ViterbiStreamDecoder/","title":"komm.ViterbiStreamDecoder","text":"<p>Convolutional stream decoder using Viterbi algorithm. Decode a (hard or soft) bit stream given a convolutional code, assuming a traceback length (path memory) of $\\tau$. At time $t$, the decoder chooses the path survivor with best metric at time $t - \\tau$ and outputs the corresponding information bits. The output stream has a delay equal to $k \\tau$, where $k$ is the number of input bits of the convolutional code. As a rule of thumb, the traceback length is chosen as $\\tau = 5\\mu$, where $\\mu$ is the memory order of the convolutional code.</p> <p>Parameters:</p> <ul> <li> <code>convolutional_code</code> (<code>ConvolutionalCode</code>)         \u2013          <p>The convolutional code.</p> </li> <li> <code>traceback_length</code> (<code>int</code>)         \u2013          <p>The traceback length (path memory) $\\tau$ of the decoder.</p> </li> <li> <code>state</code> (<code>int</code>)         \u2013          <p>The current state of the decoder. The default value is <code>0</code>.</p> </li> <li> <code>input_type</code> (<code>Literal['hard', 'soft']</code>)         \u2013          <p>The type of the input sequence, either <code>hard</code> or <code>soft</code>. The default value is <code>hard</code>.</p> </li> </ul>"},{"location":"ref/ViterbiStreamDecoder/#decode","title":"<code>decode()</code>","text":"<p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The (hard or soft) bit sequence to be decoded.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The decoded bit sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decoder = komm.ViterbiStreamDecoder(\n...     convolutional_code=komm.ConvolutionalCode([[0b111, 0b101]]),\n...     traceback_length=10,\n... )\n&gt;&gt;&gt; decoder.decode([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\narray([0, 0, 0, 0, 0, 0, 0, 0])\n&gt;&gt;&gt; decoder.decode([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1])\narray([0, 0, 0, 0, 0, 0, 0, 0])\n&gt;&gt;&gt; decoder.decode([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\narray([0, 0, 1, 0, 1, 1, 1, 0])\n</code></pre>"},{"location":"ref/WagnerDecoder/","title":"komm.WagnerDecoder","text":"<p>Wagner decoder for single parity-check codes. For more details, see CF07, Sec. III.C.</p> <p>Parameters:</p> <ul> <li> <code>code</code> (<code>SingleParityCheckCode</code>)         \u2013          <p>The single parity-check code to be used for decoding.</p> </li> </ul> Notes <ul> <li>Input type: <code>soft</code> (L-values or channel outputs).</li> <li>Output type: <code>hard</code> (bits).</li> </ul>"},{"location":"ref/WagnerDecoder/#decode","title":"<code>decode()</code>","text":"<p>Decode received words. This method takes one or more sequences of received words and returns their corresponding estimated message sequences.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence(s). Can be either a single sequence whose length is a multiple of $n$, or a multidimensional array where the last dimension is a multiple of $n$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer | floating]</code>)          \u2013          <p>The output sequence(s). Has the same shape as the input, with the last dimension contracted from $bn$ to $bk$, where $b$ is a positive integer.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; code = komm.SingleParityCheckCode(4)\n&gt;&gt;&gt; decoder = komm.WagnerDecoder(code)\n&gt;&gt;&gt; decoder.decode([[1.52, -0.36, 1.56, 0.82], [-0.75,  1.20, -2.11, 1.73]])\narray([[0, 0, 0],\n       [1, 0, 1]])\n</code></pre>"},{"location":"ref/WalshHadamardSequence/","title":"komm.WalshHadamardSequence","text":"<p>Walsh\u2013Hadamard sequence. Consider the following recursive matrix construction: $$     H_1 =     \\begin{bmatrix}         +1     \\end{bmatrix}, \\qquad     H_{2^n} =     \\begin{bmatrix}         H_{2^{n-1}} &amp; H_{2^{n-1}} \\\\         H_{2^{n-1}} &amp; -H_{2^{n-1}}     \\end{bmatrix}, $$ for $n = 1, 2, \\ldots$. For example, for $n = 3$, $$     H_8 =     \\begin{bmatrix}         +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 \\\\         +1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; -1 \\\\         +1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 \\\\         +1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 \\\\         +1 &amp; +1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 \\\\         +1 &amp; -1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 \\\\         +1 &amp; +1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 \\\\         +1 &amp; -1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 \\\\     \\end{bmatrix} $$ The above matrix is said to be in natural ordering. If the rows of the matrix are rearranged by first applying the bit-reversal permutation and then the Gray-code permutation, the following matrix is obtained: $$     H_8^{\\mathrm{s}} =     \\begin{bmatrix}         +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 &amp; +1 \\\\         +1 &amp; +1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 \\\\         +1 &amp; +1 &amp; -1 &amp; -1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 \\\\         +1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 \\\\         +1 &amp; -1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 \\\\         +1 &amp; -1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; +1 &amp; -1 \\\\         +1 &amp; -1 &amp; +1 &amp; -1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 \\\\         +1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; -1 &amp; +1 &amp; -1 \\\\     \\end{bmatrix} $$ The above matrix is said to be in sequency ordering. It has the property that row $i$ has exactly $i$ sign changes.</p> <p>The Walsh\u2013Hadamard sequence of length $L$ and index $i \\in [0 : L)$ is a binary sequence whose polar format is the $i$-th row of $H_L$, if assuming natural ordering, or $H_L^{\\mathrm{s}}$, if assuming sequency ordering. Fore more details, see Wikipedia: Hadamard matrix and Wikipedia: Walsh matrix.</p> <p>Parameters:</p> <ul> <li> <code>length</code> (<code>int</code>)         \u2013          <p>Length $L$ of the Walsh\u2013Hadamard sequence. Must be a power of two.</p> </li> <li> <code>ordering</code> (<code>Literal['natural', 'sequency', 'dyadic']</code>)         \u2013          <p>Ordering to be assumed. Should be one of <code>'natural'</code>, <code>'sequency'</code>, or <code>'dyadic'</code>. The default value is <code>'natural'</code>.</p> </li> <li> <code>index</code> (<code>int</code>)         \u2013          <p>Index of the Walsh\u2013Hadamard sequence, with respect to the ordering assumed. Must be in the set $[0 : L)$. The default value is <code>0</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; walsh_hadamard = komm.WalshHadamardSequence(length=8, ordering='natural', index=5)\n&gt;&gt;&gt; walsh_hadamard.polar_sequence\narray([ 1, -1,  1, -1, -1,  1, -1,  1])\n</code></pre> <pre><code>&gt;&gt;&gt; walsh_hadamard = komm.WalshHadamardSequence(length=8, ordering='sequency', index=5)\n&gt;&gt;&gt; walsh_hadamard.polar_sequence\narray([ 1, -1, -1,  1, -1,  1,  1, -1])\n</code></pre> <pre><code>&gt;&gt;&gt; walsh_hadamard = komm.WalshHadamardSequence(length=8, ordering='dyadic', index=5)\nTraceback (most recent call last):\n...\nNotImplementedError\n</code></pre>"},{"location":"ref/ZChannel/","title":"komm.ZChannel","text":"<p>Z-channel. It is a discrete memoryless channel with input and output alphabets $\\mathcal{X} = \\mathcal{Y} = \\{ 0, 1 \\}$. The channel is characterized by a parameter $p$, called the decay probability. Bit $0$ is always received correctly, but bit $1$ turns into $0$ with probability $p$. Equivalently, the channel can be modeled as $$     Y_n = A_n X_n, $$ where $A_n$ are iid Bernoulli random variables with $\\Pr[A_n = 0] = p$.</p> <p>Parameters:</p> <ul> <li> <code>decay_probability</code> (<code>float</code>)         \u2013          <p>The channel decay probability $p$. Must satisfy $0 \\leq p \\leq 1$. The default value is <code>0.0</code>, which corresponds to a noiseless channel.</p> </li> </ul>"},{"location":"ref/ZChannel/#input_cardinality","title":"<code>input_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel input cardinality $|\\mathcal{X}|$.</p> <p>For the Z-channel, it is given by $|\\mathcal{X}| = 2$.</p>"},{"location":"ref/ZChannel/#output_cardinality","title":"<code>output_cardinality</code><code>  int </code>  <code>cached</code> <code>property</code>","text":"<p>The channel output cardinality $|\\mathcal{Y}|$.</p> <p>For the Z-channel, it is given by $|\\mathcal{Y}| = 2$.</p>"},{"location":"ref/ZChannel/#transition_matrix","title":"<code>transition_matrix</code><code>  NDArray[floating] </code>  <code>cached</code> <code>property</code>","text":"<p>The channel transition probability matrix $p_{Y \\mid X}$.</p> <p>For the Z-channel, it is given by $$     p_{Y \\mid X} = \\begin{bmatrix} 1 &amp; 0 \\\\ p &amp; 1-p \\end{bmatrix}. $$</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zc = komm.ZChannel(0.2)\n&gt;&gt;&gt; zc.transition_matrix\narray([[1. , 0. ],\n       [0.2, 0.8]])\n</code></pre>"},{"location":"ref/ZChannel/#mutual_information","title":"<code>mutual_information()</code>","text":"<p>Returns the mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$ of the channel.</p> <p>Parameters:</p> <ul> <li> <code>input_pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p_X$ of the channel input $X$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The mutual information $\\mathrm{I}(X ; Y)$ between the input $X$ and the output $Y$.</p> </li> </ul> <p>For the Z-channel, it is given by $$     \\mathrm{I}(X ; Y) = \\Hb ( \\pi (1-p) ) - \\pi \\Hb(p), $$ in bits, where $\\pi = \\Pr[X = 1]$, and $\\Hb$ is the binary entropy function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zc = komm.ZChannel(0.2)\n&gt;&gt;&gt; zc.mutual_information([0.5, 0.5])\nnp.float64(0.6099865470109874)\n</code></pre>"},{"location":"ref/ZChannel/#capacity","title":"<code>capacity()</code>","text":"<p>Returns the channel capacity $C$.</p> <p>Parameters:</p> <ul> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The channel capacity $C$.</p> </li> </ul> <p>For the Z-channel, it is given by $$     C = \\log_2 ( 1 + (1-p) p^{p / (1-p)} ), $$ in bits.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zc = komm.ZChannel(0.2)\n&gt;&gt;&gt; zc.capacity()\nnp.float64(0.6182313659549211)\n</code></pre>"},{"location":"ref/ZChannel/#transmit","title":"<code>transmit()</code>","text":"<p>Transmits the input sequence through the channel and returns the output sequence.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input sequence.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The output sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zc = komm.ZChannel(0.2)\n&gt;&gt;&gt; zc.transmit([1, 1, 1, 0, 0, 0, 1, 0, 1, 0])\narray([1, 1, 1, 0, 0, 0, 1, 0, 0, 0])\n</code></pre>"},{"location":"ref/ZadoffChuSequence/","title":"komm.ZadoffChuSequence","text":"<p>Zadoff\u2013Chu sequence. It is a periodic, complex sequence given by $$     z_{L,q}[n] = \\mathrm{e}^{-\\mathrm{j} \\pi q n (n + 1) / L}, $$ where $L$ is the length (and period) of the sequence (which must be an odd integer) and $q \\in [1:L)$ is called the root index of the sequence.</p> <p>Zadoff\u2013Chu sequences have the following properties:</p> <ol> <li> <p>Constant amplitude: The magnitude of the sequence satisfies $$     |z_{L,q}[n]| = 1, \\quad \\forall n. $$</p> </li> <li> <p>Zero autocorrelation: If $q$ is coprime to $L$, then the cyclic autocorrelation of $z_{L,q}$ satisfies $$     \\tilde{R}_{z_{L,q}}[\\ell] = 0, \\quad \\forall \\ell \\neq 0 \\mod L. $$</p> </li> <li> <p>Constant cross-correlation: If $|q' - q|$ is coprime to $L$, then the magnitude of the cyclic cross-correlation of $z_{L,q}$ and $z_{L,q'}$ satisfies $$     |\\tilde{R}_{z_{L,q}, z_{L,q'}}[\\ell]| = \\sqrt{L}, \\quad \\forall \\ell. $$</p> </li> </ol> <p>For more details, see And22.</p> Notes <ul> <li>Theses sequences are also called Frank\u2013Zadoff\u2013Chu sequences.</li> </ul> <p>Parameters:</p> <ul> <li> <code>length</code> (<code>int</code>)         \u2013          <p>The length $L$ of the Zadoff\u2013Chu sequence. Must be an odd integer.</p> </li> <li> <code>root_index</code> (<code>int</code>)         \u2013          <p>The root index $q$ of the Zadoff\u2013Chu sequence. Must be in $[1:L)$. The default value is $1$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; zadoff_chu = komm.ZadoffChuSequence(5, root_index=1)\n&gt;&gt;&gt; zadoff_chu.sequence.round(6)\narray([ 1.      +0.j      ,  0.309017-0.951057j, -0.809017+0.587785j,  0.309017-0.951057j,  1.      +0.j      ])\n&gt;&gt;&gt; zadoff_chu.cyclic_autocorrelation(normalized=True).round(15) + 0.0\narray([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n</code></pre>"},{"location":"ref/autocorrelation/","title":"komm.autocorrelation","text":"<p>Computes the autocorrelation $R[\\ell]$ of a real or complex sequence $x[n]$. This is defined as $$     R[\\ell] = \\sum_{n \\in \\mathbb{Z}} x[n] x^*_\\ell[n], $$ where $x^*_\\ell[n] = x^*[n - \\ell]$ is the complex conjugate of $x[n]$ shifted by $\\ell$ positions. The autocorrelation $R[\\ell]$ is even symmetric and satisfies $R[\\ell] = 0$ for $|\\ell| \\geq L$, where $L$ is the length of the sequence.</p> <p>Parameters:</p> <ul> <li> <code>sequence</code> (<code>ArrayLike</code>)         \u2013          <p>A 1D-array containing the sequence $x[n]$, of length $L$.</p> </li> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>A 1D-array containing the values of $\\ell$ for which the autocorrelation will be computed. The default value is <code>range(len(sequence))</code>, that is, $[0 : L)$.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>If <code>True</code>, returns the autocorrelation divided by the sequence energy, so that $R[0] = 1$. The default value is <code>False</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array1D[DType]</code>         \u2013          <p>The autocorrelation $R[\\ell]$ of the sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.autocorrelation([1.0, 2.0, 3.0, 4.0], shifts=[-2, -1, 0, 1, 2])\narray([11., 20., 30., 20., 11.])\n</code></pre>"},{"location":"ref/binary_entropy/","title":"komm.binary_entropy","text":"<p>Computes the binary entropy function. For a given probability $p$, it is defined as $$     \\Hb(p) = p \\log_2 \\frac{1}{p} + (1 - p) \\log_2 \\frac{1}{1 - p}, $$ and corresponds to the entropy of a Bernoulli random variable with parameter $p$.</p> <p>Parameters:</p> <ul> <li> <code>p</code> (<code>float</code>)         \u2013          <p>A probability value. It must satisfy $0 \\leq p \\leq 1$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The value of the binary entropy function $\\Hb(p)$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; [komm.binary_entropy(p) for p in [0.0, 0.25, 0.5, 0.75, 1.0]]\n[0.0, 0.8112781244591328, 1.0, 0.8112781244591328, 0.0]\n</code></pre>"},{"location":"ref/binary_entropy_inv/","title":"komm.binary_entropy_inv","text":"<p>Computes the inverse of the binary entropy function. More precisely, it computes the value of $p \\in [0, 1/2]$ such that $\\Hb(p) = h$.</p> <p>Parameters:</p> <ul> <li> <code>h</code> (<code>float</code>)         \u2013          <p>A value in the interval $[0, 1]$.</p> </li> <li> <code>tol</code> (<code>float</code>)         \u2013          <p>The tolerance for the binary search. The default value is <code>1e-12</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The value of $p \\in [0, 1/2]$ such that $\\Hb(p) = h$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; [komm.binary_entropy_inv(h) for h in [0.0, 0.25, 0.5, 0.75, 1.0]]\n[0.0, 0.04169269027397604, 0.1100278644385071, 0.2145017448597173, 0.5]\n</code></pre>"},{"location":"ref/bits_to_int/","title":"komm.bits_to_int","text":"<p>Converts a bit array to its integer representation.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input bit array. Must be an array with elements in the set $\\{ 0, 1 \\}$, with the bit sequences in the last axis.</p> </li> <li> <code>bit_order</code> (<code>Literal['LSB-first', 'MSB-first']</code>)         \u2013          <p>Bit order convention. Must be either <code>\"LSB-first\"</code> (least significant bit in the first position) or <code>\"MSB-first\"</code> (most significant bit in the first position). The default value is <code>\"LSB-first\"</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>int | NDArray[integer]</code>)          \u2013          <p>The integer representation of the input bit array. Has the same shape as the input, but with the last dimension removed.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.bits_to_int([0, 0, 0, 0, 1, 0], bit_order=\"LSB-first\")\n16\n</code></pre> <pre><code>&gt;&gt;&gt; komm.bits_to_int([0, 0, 0, 0, 1, 0], bit_order=\"MSB-first\")\n2\n</code></pre> <pre><code>&gt;&gt;&gt; komm.bits_to_int([[0, 0], [1, 0], [0, 1], [1, 1]])\narray([0, 1, 2, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; komm.bits_to_int([[0, 0], [1, 0], [0, 1], [1, 1]], bit_order=\"MSB-first\")\narray([0, 2, 1, 3])\n</code></pre>"},{"location":"ref/boxplus/","title":"komm.boxplus","text":"<p>Computes the box-plus operation. It is defined by $$     a \\boxplus b = 2 \\operatorname{atanh} \\left( \\tanh\\left(\\frac{a}{2}\\right) \\tanh\\left(\\frac{b}{2}\\right) \\right). $$ If $X, Y$ are independent binary random variables, then $L(X \\oplus Y) = L(X) \\boxplus L(Y)$, where $L$ denotes the L-value and $\\oplus$ denotes the modulo-$2$ sum.</p> <p>Parameters:</p> <ul> <li> <code>a</code> (<code>ArrayLike</code>)         \u2013          <p>A float or array of floats containing L-values.</p> </li> <li> <code>b</code> (<code>ArrayLike</code>)         \u2013          <p>A float or array of floats containing L-values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NDArray[floating]</code>         \u2013          <p>The result of the boxplus operation. It has the same shape as the inputs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.boxplus(1, 2)\nnp.float64(0.735325664055519)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.boxplus([0, 1, 2], [1, -1, -1])\narray([ 0.        , -0.43378083, -0.73532566])\n</code></pre>"},{"location":"ref/cyclic_autocorrelation/","title":"komm.cyclic_autocorrelation","text":"<p>Computes the cyclic autocorrelation $\\tilde{R}[\\ell]$ of a real or complex sequence $x[n]$. This is defined as $$     \\tilde{R}[\\ell] = \\sum_{n \\in [0:L)} x[n] \\tilde{x}^*_\\ell[n], $$ where $\\tilde{x}^*_\\ell[n]$ is the complex conjugate of $x[n]$ cyclic-shifted by $\\ell$ positions, and $L$ is the period of the sequence. The cyclic autocorrelation $\\tilde{R}[\\ell]$ is even symmetric and periodic with period $L$.</p> <p>Parameters:</p> <ul> <li> <code>sequence</code> (<code>ArrayLike</code>)         \u2013          <p>A 1D-array containing the sequence $x[n]$, of length $L$.</p> </li> <li> <code>shifts</code> (<code>ArrayLike | None</code>)         \u2013          <p>A 1D-array containing the values of $\\ell$ for which the cyclic autocorrelation will be computed. The default value is <code>range(len(sequence))</code>, that is, $[0 : L)$.</p> </li> <li> <code>normalized</code> (<code>bool</code>)         \u2013          <p>If <code>True</code>, returns the cyclic autocorrelation divided by the sequence energy, so that $R[0] = 1$. The default value is <code>False</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array1D[DType]</code>         \u2013          <p>The cyclic autocorrelation $\\tilde{R}[\\ell]$ of the sequence.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.cyclic_autocorrelation([1.0, 2.0, 3.0, 4.0], shifts=[-2, -1, 0, 1, 2])\narray([22., 24., 30., 24., 22.])\n</code></pre>"},{"location":"ref/entropy/","title":"komm.entropy","text":"<p>Computes the entropy of a random variable with a given pmf. Let $X$ be a random variable with pmf $p_X$ and alphabet $\\mathcal{X}$. Its entropy is given by $$     \\mathrm{H}(X) = \\sum_{x \\in \\mathcal{X}} p_X(x) \\log \\frac{1}{p_X(x)}. $$ By default, the base of the logarithm is $2$, in which case the entropy is measured in bits. For more details, see CT06, Sec. 2.1.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p_X$ of the random variable. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>floating</code>         \u2013          <p>The entropy $\\mathrm{H}(X)$ of the random variable.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.entropy([1/4, 1/4, 1/4, 1/4])\nnp.float64(2.0)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.entropy(pmf=[1/3, 1/3, 1/3], base=3.0)\nnp.float64(1.0)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.entropy([0.5, 0.5], base='e')\nnp.float64(0.6931471805599453)\n</code></pre>"},{"location":"ref/fourier_transform/","title":"komm.fourier_transform","text":"<p>Computes the Fourier transform. This function applies a shift to the spectrum (so that the zero frequency component is at the center) and scales the output by a given time step. Both the spectrum and the corresponding frequency bins are returned.</p> Note <p>This is a simple wrapper around <code>numpy.fft</code> functions.</p> <p>Parameters:</p> <ul> <li> <code>waveform</code> (<code>ArrayLike</code>)         \u2013          <p>The input array representing the waveform to be transformed.</p> </li> <li> <code>time_step</code> (<code>float</code>)         \u2013          <p>The time step between samples in the waveform.</p> </li> <li> <code>nfft</code> (<code>int | None</code>)         \u2013          <p>The number of points in the FFT. If <code>None</code>, it defaults to the size of the input along the specified axis.</p> </li> <li> <code>axis</code> (<code>int</code>)         \u2013          <p>The axis along which to compute the Fourier transform. Default is the last axis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>spectrum</code>  (<code>NDArray[complexfloating]</code>)          \u2013          <p>The spectrum correponding to the input waveform.</p> </li> <li> <code>frequencies</code>  (<code>NDArray[floating]</code>)          \u2013          <p>The corresponding frequency bins.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spectrum, frequencies = komm.fourier_transform([1, 2, 3, 4], time_step=0.1)\n&gt;&gt;&gt; spectrum\narray([-0.2+0.j , -0.2-0.2j,  1. +0.j , -0.2+0.2j])\n&gt;&gt;&gt; frequencies\narray([-5. , -2.5,  0. ,  2.5])\n</code></pre>"},{"location":"ref/gaussian_q/","title":"komm.gaussian_q","text":"<p>Computes the Gaussian Q-function. It is given by $$     \\mathrm{Q}(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_x^\\infty \\mathrm{e}^{-u^2/2} \\, \\mathrm{d}u. $$ This corresponds to the complementary cumulative distribution function of the standard gaussian distribution. For more details, see Wikipedia: Q-function.</p> <p>Parameters:</p> <ul> <li> <code>x</code> (<code>ArrayLike</code>)         \u2013          <p>The input to the function. Should be a float or array of floats.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code>  (<code>NDArray[floating] | floating</code>)          \u2013          <p>The value $y = \\mathrm{Q}(x)$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.gaussian_q(0.0)\nnp.float64(0.5)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.gaussian_q([[-1.0], [0.0], [1.0]])\narray([[0.84134475],\n       [0.5       ],\n       [0.15865525]])\n</code></pre>"},{"location":"ref/gaussian_q_inv/","title":"komm.gaussian_q_inv","text":"<p>Computes the inverse Gaussian Q-function.</p> <p>Parameters:</p> <ul> <li> <code>y</code> (<code>ArrayLike</code>)         \u2013          <p>The input to the function. Should be a float or array of floats in the real interval $[0, 1]$.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>x</code>  (<code>NDArray[floating] | floating</code>)          \u2013          <p>The value $x = \\mathrm{Q^{-1}}(y)$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.gaussian_q_inv(0.5)\nnp.float64(0.0)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.gaussian_q_inv([[0.841344746], [0.5], [0.158655254]])\narray([[-1.],\n       [ 0.],\n       [ 1.]])\n</code></pre>"},{"location":"ref/int_to_bits/","title":"komm.int_to_bits","text":"<p>Converts an integer, or array of integers, to their bit representations.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input integer, or array of integers.</p> </li> <li> <code>width</code> (<code>int</code>)         \u2013          <p>The width of the bit representation.</p> </li> <li> <code>bit_order</code> (<code>Literal['LSB-first', 'MSB-first']</code>)         \u2013          <p>Bit order convention. Must be either <code>\"LSB-first\"</code> (least significant bit in the first position) or <code>\"MSB-first\"</code> (most significant bit in the first position). The default value is <code>\"LSB-first\"</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[integer]</code>)          \u2013          <p>The bit representation of the input, with the bit sequences in the last axis. Has the same shape as the input, but with a new last dimension of size <code>width</code> appended.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.int_to_bits(2, width=6, bit_order=\"LSB-first\")\narray([0, 1, 0, 0, 0, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; komm.int_to_bits(2, width=6, bit_order=\"MSB-first\")\narray([0, 0, 0, 0, 1, 0])\n</code></pre> <pre><code>&gt;&gt;&gt; komm.int_to_bits([0, 1, 2, 3], width=4)\narray([[0, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 1, 0, 0]])\n</code></pre> <pre><code>&gt;&gt;&gt; komm.int_to_bits([0, 1, 2, 3], width=4, bit_order=\"MSB-first\")\narray([[0, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 1, 1]])\n</code></pre>"},{"location":"ref/marcum_q/","title":"komm.marcum_q","text":"<p>Computes the Marcum Q-function. It is given by $$     \\mathrm{Q}_m(a; x) = \\int_x^\\infty u \\left( \\frac{u}{a} \\right)^{m-1}  I_{m-1}(a x) \\exp \\left( -\\frac{u^2 + a^2}{2} \\right) \\mathrm{d}u, $$ where $I_{m-1}$ is the modified Bessel function of the first kind. This corresponds to the complementary cumulative distribution function of the non-central chi distribution with $2m$ degrees of freedom and non-centrality parameter $a$. For more details, see Wikipedia: Marcum Q-function.</p> <p>Parameters:</p> <ul> <li> <code>m</code> (<code>int</code>)         \u2013          <p>The order of the Marcum Q-function. Should be a positive integer.</p> </li> <li> <code>a</code> (<code>ArrayLike</code>)         \u2013          <p>The value of $a$. Should be a float or array of floats.</p> </li> <li> <code>x</code> (<code>ArrayLike</code>)         \u2013          <p>The input to the function. Should be a float or array of floats.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>y</code>  (<code>NDArray[floating] | floating</code>)          \u2013          <p>The value $y = \\mathrm{Q}_m(a; x)$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.marcum_q(1, 1, 1)\nnp.float64(0.7328798037968204)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.marcum_q(2, 0.5, [1.2, 1.4, 1.6])\narray([0.85225816, 0.76472056, 0.66139663])\n</code></pre>"},{"location":"ref/relative_entropy/","title":"komm.relative_entropy","text":"<p>Computes the relative entropy (Kullback\u2013Leibler divergence) between two pmfs. Let $p$ and $q$ be two pmfs over the same alphabet $\\mathcal{X}$. The relative entropy of $p$ with respect to $q$ is defined as $$     \\mathrm{D}(p \\parallel q) = \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)}. $$ Note that, in general, $\\mathrm{D}(p \\parallel q) \\neq \\mathrm{D}(q \\parallel p)$. For more details, see CT06, Sec. 2.3.</p> <p>Parameters:</p> <ul> <li> <code>pmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $p$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>qmf</code> (<code>ArrayLike</code>)         \u2013          <p>The probability mass function $q$. It must be a valid pmf, that is, all of its values must be non-negative and sum up to $1$.</p> </li> <li> <code>base</code> (<code>float | Literal['e']</code>)         \u2013          <p>The base of the logarithm to be used. It must be a positive float or the string <code>'e'</code>. The default value is <code>2.0</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The relative entropy $\\mathrm{D}(p \\parallel q)$ between the two pmfs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.relative_entropy([1/2, 1/2], [1/2, 1/2])\nnp.float64(0.0)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.relative_entropy([1/2, 1/2], [3/4, 1/4])\nnp.float64(0.20751874963942185)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.relative_entropy([3/4, 1/4], [1/2, 1/2])\nnp.float64(0.18872187554086717)\n</code></pre> <pre><code>&gt;&gt;&gt; komm.relative_entropy([1/2, 1/2], [0, 1])\nnp.float64(inf)\n</code></pre>"},{"location":"ref/sampling_rate_compress/","title":"komm.sampling_rate_compress","text":"<p>Performs sampling rate compression (downsampling). For a given input $x[n]$, the output is $$ y[n] = x[n M + \\Delta], $$ where $M$ is the compression factor and $\\Delta \\in [0:M)$ is the offset for the first selected element. In words, the compressor extracts every $M$-th element starting from the offset $\\Delta$ along the specified axis. For more details, see OS99, Sec. 4.6.1.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input array $x[n]$ to be compressed.</p> </li> <li> <code>factor</code> (<code>int</code>)         \u2013          <p>The compression factor $M$.</p> </li> <li> <code>offset</code> (<code>int</code>)         \u2013          <p>The offset $\\Delta$. Must satisfy $\\Delta \\in [0:M)$.</p> </li> <li> <code>axis</code> (<code>int</code>)         \u2013          <p>The axis along which to extract elements. Default is the last axis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[Any]</code>)          \u2013          <p>The compressed array $y[n]$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.sampling_rate_compress(\n...    [[11, 12, 13, 14, 15],\n...     [16, 17, 18, 19, 20]],\n...    factor=3,\n... )\narray([[11, 14],\n       [16, 19]])\n&gt;&gt;&gt; komm.sampling_rate_compress(\n...    [[11, 12, 13, 14, 15],\n...     [16, 17, 18, 19, 20]],\n...    factor=3,\n...    offset=1,\n... )\narray([[12, 15],\n       [17, 20]])\n&gt;&gt;&gt; komm.sampling_rate_compress(\n...     [[11, 12],\n...      [13, 14],\n...      [15, 16],\n...      [17, 18],\n...      [19, 20]],\n...     factor=3,\n...     axis=0,\n... )\narray([[11, 12],\n       [17, 18]])\n</code></pre>"},{"location":"ref/sampling_rate_expand/","title":"komm.sampling_rate_expand","text":"<p>Performs sampling rate expansion (upsampling). For a given input $x[n]$, the output is $$ y[n] = \\begin{cases} x[n \\operatorname{div} L] &amp; \\text{if } n \\bmod L = \\Delta, \\\\ 0,       &amp; \\text{otherwise} \\end{cases} $$ where $L$ is the expansion factor and $\\Delta \\in [0:L)$ is the offset for the first output element. In words, the expander inserts $L-1$ zeros between each element of the input array along the specified axis, starting from the offset $\\Delta$. For more details, see OS99, Sec. 4.6.2.</p> <p>Parameters:</p> <ul> <li> <code>input</code> (<code>ArrayLike</code>)         \u2013          <p>The input array $x[n]$ to be expanded.</p> </li> <li> <code>factor</code> (<code>int</code>)         \u2013          <p>The expansion factor $L$.</p> </li> <li> <code>offset</code> (<code>int</code>)         \u2013          <p>The offset $\\Delta$. Must satisfy $\\Delta \\in [0:L)$.</p> </li> <li> <code>axis</code> (<code>int</code>)         \u2013          <p>The axis along which to insert zeros. Default is the last axis.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code>  (<code>NDArray[Any]</code>)          \u2013          <p>The expanded array $y[n]$.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; komm.sampling_rate_expand([[1, 2], [3, 4]], factor=3)\narray([[1, 0, 0, 2, 0, 0],\n       [3, 0, 0, 4, 0, 0]])\n&gt;&gt;&gt; komm.sampling_rate_expand([[1, 2], [3, 4]], factor=3, offset=1)\narray([[0, 1, 0, 0, 2, 0],\n       [0, 3, 0, 0, 4, 0]])\n&gt;&gt;&gt; komm.sampling_rate_expand([[1, 2], [3, 4]], factor=3, axis=0)\narray([[1, 2],\n       [0, 0],\n       [0, 0],\n       [3, 4],\n       [0, 0],\n       [0, 0]])\n</code></pre>"},{"location":"res/convolutional-codes/","title":"Tables of convolutional codes","text":""},{"location":"res/convolutional-codes/#low-rate","title":"Optimal low-rate convolutional codes","text":"<p>The table below lists optimal low-rate convolutional codes, for $n \\in \\{ 2, 3, 4 \\}$, and small values of degree $\\sigma$.</p> <p>Source: LC04, Tables 12.1 (a)\u2013(c), p. 539\u2013540.</p> $n$ $\\sigma$ $g(D) = [g_0(D) ~ \\cdots ~ g_{n-1}(D)]$ $d_\\mathrm{free}$ $2$ $1$ <code>[0o3, 0o1]</code> $3$ $2$ $2$ <code>[0o5, 0o7]</code> $5$ $2$ $3$ <code>[0o13, 0o17]</code> $6$ $2$ $4$ <code>[0o27, 0o31]</code> $7$ $2$ $5$ <code>[0o53, 0o75]</code> $8$ $2$ $6$ <code>[0o117, 0o155]</code> $10$ $2$ $7$ <code>[0o247, 0o371]</code> $10$ $2$ $8$ <code>[0o561, 0o753]</code> $12$ $2$ $9$ <code>[0o1131, 0o1537]</code> $12$ $2$ $10$ <code>[0o2473, 0o3217]</code> $14$ $2$ $11$ <code>[0o4325, 0o6747]</code> $15$ $2$ $12$ <code>[0o10627, 0o16765]</code> $16$ $2$ $13$ <code>[0o27251, 0o37363]</code> $16$ $3$ $1$ <code>[0o1, 0o3, 0o3]</code> $5$ $3$ $2$ <code>[0o5, 0o7, 0o7]</code> $8$ $3$ $3$ <code>[0o13, 0o15, 0o17]</code> $10$ $3$ $4$ <code>[0o25, 0o33, 0o37]</code> $12$ $3$ $5$ <code>[0o47, 0o53, 0o75]</code> $13$ $3$ $6$ <code>[0o117, 0o127, 0o155]</code> $15$ $3$ $7$ <code>[0o225, 0o331, 0o367]</code> $16$ $3$ $8$ <code>[0o575, 0o623, 0o727]</code> $18$ $3$ $9$ <code>[0o1167, 0o1375, 0o1545]</code> $20$ $3$ $10$ <code>[0o2325, 0o2731, 0o3747]</code> $22$ $3$ $11$ <code>[0o5745, 0o6471, 0o7553]</code> $24$ $3$ $12$ <code>[0o2371, 0o13725, 0o14733]</code> $24$ $4$ $1$ <code>[0o1, 0o1, 0o3, 0o3]</code> $6$ $4$ $2$ <code>[0o5, 0o5, 0o7, 0o7]</code> $10$ $4$ $3$ <code>[0o13, 0o13, 0o15, 0o17]</code> $13$ $4$ $4$ <code>[0o25, 0o27, 0o33, 0o37]</code> $16$ $4$ $5$ <code>[0o45, 0o53, 0o67, 0o77]</code> $18$ $4$ $6$ <code>[0o117, 0o127, 0o155, 0o171]</code> $20$ $4$ $7$ <code>[0o257, 0o311, 0o337, 0o355]</code> $22$ $4$ $8$ <code>[0o533, 0o575, 0o647, 0o711]</code> $24$ $4$ $9$ <code>[0o1173, 0o1325, 0o1467, 0o1751]</code> $27$"},{"location":"res/convolutional-codes/#high-rate","title":"Optimal high-rate convolutional codes","text":"<p>The table below lists optimal high-rate convolutional codes, for $n \\in \\{ 3, 4 \\}$, and small values of degree $\\sigma$.</p> <p>Source: LC04, Tables 12.1 (d) and (e), p. 540.</p> $n$ $\\sigma$ $h(D) = [h_0(D) ~ \\cdots ~ h_{n-1}(D)]$ $d_\\mathrm{free}$ $3$ $2$ <code>[0o7, 0o5, 0o3]</code> $3$ $3$ $3$ <code>[0o13, 0o15, 0o17]</code> $4$ $3$ $4$ <code>[0o27, 0o31, 0o23]</code> $5$ $3$ $5$ <code>[0o73, 0o57, 0o71]</code> $6$ $3$ $6$ <code>[0o121, 0o147, 0o123]</code> $7$ $3$ $7$ <code>[0o241, 0o227, 0o313]</code> $8$ $3$ $8$ <code>[0o477, 0o631, 0o555]</code> $8$ $3$ $9$ <code>[0o1327, 0o1423, 0o1051]</code> $9$ $3$ $10$ <code>[0o3013, 0o2137, 0o2621]</code> $10$ $4$ $2$ <code>[0o6, 0o7, 0o5, 0o1]</code> $3$ $4$ $3$ <code>[0o12, 0o15, 0o13, 0o11]</code> $4$ $4$ $4$ <code>[0o31, 0o37, 0o25, 0o33]</code> $4$ $4$ $5$ <code>[0o75, 0o57, 0o73, 0o47]</code> $5$ $4$ $6$ <code>[0o141, 0o133, 0o135, 0o107]</code> $6$ $4$ $7$ <code>[0o267, 0o315, 0o341, 0o211]</code> $6$ $4$ $8$ <code>[0o661, 0o733, 0o757, 0o535]</code> $7$ $4$ $9$ <code>[0o1371, 0o1157, 0o1723, 0o1475]</code> $8$"},{"location":"res/primitive-polynomials/","title":"Default primitive polynomials","text":"<p>The table below lists the default primitive polynomials of degree $k$ over $\\mathbb{F}_2$ for $k \\in [1 : 24]$. The polynomial $p(X)$ is represented as a binary number, where the leftmost bit stands for the highest degree term. For example, the polynomial $p(X) = X^3 + X + 1$ is represented as <code>0b1011</code>.</p> <p>Source: LC04, Table 2.7, p. 42.</p> Degree $k$ Primitive polynomial $p(X)$ Degree $k$ Primitive polynomial $p(X)$ $1$ <code>0b11</code> $13$ <code>0b10000000011011</code> $2$ <code>0b111</code> $14$ <code>0b100010001000011</code> $3$ <code>0b1011</code> $15$ <code>0b1000000000000011</code> $4$ <code>0b10011</code> $16$ <code>0b11010000000010001</code> $5$ <code>0b100101</code> $17$ <code>0b100000000000001001</code> $6$ <code>0b1000011</code> $18$ <code>0b1000000000010000001</code> $7$ <code>0b10001001</code> $19$ <code>0b10000000000000100111</code> $8$ <code>0b100011101</code> $20$ <code>0b100000000000000001001</code> $9$ <code>0b1000010001</code> $21$ <code>0b1000000000000000000101</code> $10$ <code>0b10000001001</code> $22$ <code>0b10000000000000000000011</code> $11$ <code>0b100000000101</code> $23$ <code>0b100000000000000000100001</code> $12$ <code>0b1000001010011</code> $24$ <code>0b1000000000000000010000111</code>"}]}